apiVersion: v1
kind: Service
metadata:
  name: gpu-runtime-wxg-service
spec:
  clusterIP: None
  selector:
    job-name: gpu-runtime-wxg-job

---
apiVersion: batch/v1
kind: Job
metadata:
  name: gpu-runtime-wxg-job
spec:
  completions: 2
  parallelism: 2
  completionMode: Indexed
  template:
    metadata:
      labels:
        k8s-app: gpu-runtime-wxg
      annotations:
        networking.gke.io/default-interface: 'eth0'
        networking.gke.io/interfaces: |
          [
            {"interfaceName":"eth0","network":"default"},
            {"interfaceName":"eth2","network":"rdma-0"},
            {"interfaceName":"eth3","network":"rdma-1"},
            {"interfaceName":"eth4","network":"rdma-2"},
            {"interfaceName":"eth5","network":"rdma-3"},
            {"interfaceName":"eth6","network":"rdma-4"},
            {"interfaceName":"eth7","network":"rdma-5"},
            {"interfaceName":"eth8","network":"rdma-6"},
            {"interfaceName":"eth9","network":"rdma-7"}
          ]
        kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
        gke-gcsfuse/volumes: "true"
        gke-gcsfuse/cpu-limit: "0"
        gke-gcsfuse/memory-limit: "0"
        gke-gcsfuse/ephemeral-storage-limit: "0"
    spec:
      restartPolicy: Never
      subdomain: gpu-runtime-wxg-service
      dnsPolicy: ClusterFirstWithHostNet
      hostNetwork: true
      hostPID: true
      serviceAccount: workload-identity-k8s-sa
      nodeSelector:
        cloud.google.com/gke-nodepool: a4-highgpu-ubuntu-02
      volumes:
        - name: local-ssd
          hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd
        - name: library-dir-host
          hostPath:
            path: /home/kubernetes/bin/nvidia
        - name: gib
          hostPath:
            path: /home/kubernetes/bin/gib
        - name: shared-memory
          emptyDir:
            medium: "Memory"
            sizeLimit: 250Gi
        # - name: data-volume
        #   persistentVolumeClaim:
        #     claimName: gcs-pvc
        # - name: cfs-volume
        #   persistentVolumeClaim:
        #     claimName: cfs-pvc
      containers:
        # - image: nvcr.io/nvidia/pytorch:25.01-py3
        # - image: nvcr.io/nvidia/pytorch:25.06-py3
        - image: us-central1-docker.pkg.dev/gpu-launchpad-playground/hzchen-poc/wxg-poc:0.2
        # - image: docker.io/pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel
          name: ngc-25-04
          resources:
            requests:
              # cpu: 32
              nvidia.com/gpu: "8"
            limits:
              # cpu: 32
              nvidia.com/gpu: "8"
          securityContext:
           privileged: true
          volumeMounts:
            - name: local-ssd
              mountPath: /scratch-data
            - name: library-dir-host
              mountPath: /usr/local/nvidia
            - name: gib
              mountPath: /usr/local/gib
            - name: shared-memory
              mountPath: /dev/shm
            # - name: data-volume
            #   mountPath: /data
            # - name: cfs-volume
            #   mountPath: /cfs
          env:
            - name: RDMAV_DRIVERS
              value: /usr/local/nvidia/lib64/libibverbs
            - name: LD_LIBRARY_PATH
              value: /usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/gib/lib64
            - name: PYTHONPATH
              value: /usr/local/nvidia/deepep
            - name: N_NODES
              value: "2"
            - name: NCCL_NET
              value: gIB
            - name: NCCL_CROSS_NIC
              value: "0"
            - name: NCCL_NET_GDR_LEVEL
              value: PIX
            - name: NCCL_P2P_NET_CHUNKSIZE
              value: "131072"
            - name: NCCL_NVLS_CHUNKSIZE
              value: "524288"
            - name: NCCL_IB_ADAPTIVE_ROUTING
              value: "1"
            - name: NCCL_IB_QPS_PER_CONNECTION
              value: "4"
            - name: NCCL_IB_TC
              value: "52"
            - name: NCCL_IB_FIFO_TC
              value: "84"
            - name: NCCL_TUNER_CONFIG_PATH
              value: /usr/local/gib/configs/tuner_config_a4.txtpb
            - name: NVSHMEM_IBGDA_SUPPORT
              value: "1"
            - name: NVSHMEM_USE_GDRCOPY
              value: "1"
            - name: GDRCOPY_HOME
              value: /usr/local/nvidia
            - name: NVSHMEM_HOME
              value: /usr/local/nvidia
            - name: USE_NVPEERMEM
              value: "1"
            - name: CUDA_HOME
              value: /usr/local/cuda
            - name: TORCH_CUDA_ARCH_LIST_B200
              value: "10.0"
          command: ["/bin/bash", "-c"]
          args:
            - |
              cp -R /dev/shm/scripts /
              cp -R /dev/shm/diagnostic /
              cp -R /dev/shm/third_party /

              # Fix RDMA driver for rdmav57 - copy to system default path
              cp /usr/local/nvidia/lib64/libmlx5.so.1.25.56.0 /usr/lib/x86_64-linux-gnu/libibverbs/libmlx5-rdmav57.so
              chmod +x /usr/lib/x86_64-linux-gnu/libibverbs/libmlx5-rdmav57.so
              
              # Also create in nvidia lib64 path for consistency
              mkdir -p /usr/local/nvidia/lib64/libibverbs
              cp /usr/local/nvidia/lib64/libmlx5.so.1.25.56.0 /usr/local/nvidia/lib64/libibverbs/libmlx5-rdmav57.so
              chmod +x /usr/local/nvidia/lib64/libibverbs/libmlx5-rdmav57.so
              
              # Unset TORCH_NCCL_USE_COMM_NONBLOCKING for GIB compatibility
              unset TORCH_NCCL_USE_COMM_NONBLOCKING

              export N_NODES=${N_NODES}

              # Job-specific environment variables
              export RANK=${JOB_COMPLETION_INDEX}
              export WORLD_SIZE=2
              export MASTER_ADDR=gpu-runtime-wxg-job-0.gpu-runtime-wxg-service

              # Load all the cuda libs
              /sbin/ldconfig

              # Install ping
              apt update -y
              apt install -y iputils-ping openssh-server iproute2 pciutils curl

              # Install gcloud sdk
              echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | \
                tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
                curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | \
                gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg && \
                apt-get update -y && \
                apt-get install google-cloud-cli -y

              # Enable persistence
              for i in $(seq 0 7)
              do
                nvidia-smi -i $i -pm ENABLED
              done

              # Setup variables
              echo "unset NCCL_NVLS_ENABLE TORCH_NCCL_USE_COMM_NONBLOCKING" | tee -a /root/.bashrc > /dev/null
              echo "export LD_LIBRARY_PATH=/usr/local/gib/lib64:${LD_LIBRARY_PATH}" | tee -a /root/.bashrc > /dev/null
              echo "source /usr/local/gib/scripts/set_nccl_env.sh" | tee -a /root/.bashrc > /dev/null

              echo "Pod ${RANK} started with RANK=${RANK}, WORLD_SIZE=${WORLD_SIZE}, MASTER_ADDR=${MASTER_ADDR}"
              
              # Run DeepEP test for verification
              if [ ! -d "/tmp/deepep_build" ]; then
                git clone https://github.com/deepseek-ai/DeepEP.git /tmp/deepep_build
              fi
              cd /tmp/deepep_build
              
              echo "Starting DeepEP internode test..."
              python3 tests/test_internode.py
              
              echo "Test completed successfully!"
              sleep infinity
      initContainers:
        - name: nccl-plugin-installer
          image: us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib-diagnostic:v1.1.0
          imagePullPolicy: Always
          args:
          - |
            set -ex
            /scripts/container_entry.sh install --install-nccl
            cp -R /var/lib/gib/lib64/. /target/usr/local/gib/lib64
            cp -R /var/lib/gib/. /target/usr/local/gib
            cp -R /scripts /target/dev/shm
            cp -R /diagnostic /target/dev/shm
            cp -R /third_party /target/dev/shm
          command:
          - /bin/sh
          - -c
          volumeMounts:
          - mountPath: /target/usr/local/gib
            name: gib
          - mountPath: /target/dev/shm
            name: shared-memory