
# Wan æ¨¡å‹ TPU è¿ç§»ä¸ä¼˜åŒ–å®Œå…¨æŒ‡å—

> **ç‰ˆæœ¬**: 4.1 | **æ›´æ–°æ—¥æœŸ**: 2024å¹´12æœˆ
>
> æœ¬æ–‡æ¡£æ˜¯ Wan 2.1/2.2 æ¨¡å‹åœ¨ Google Cloud TPU v6e ä¸Šè¿ç§»ä¸ä¼˜åŒ–çš„**æƒå¨æŠ€æœ¯å‚è€ƒ**ã€‚
>
> **æ–‡æ¡£æ¥æº**ï¼šæœ¬æ–‡æ±‡é›†ä»¥ä¸‹ä¸‰ä»½æ ¸å¿ƒæŠ€æœ¯æ–‡æ¡£çš„å…¨éƒ¨ç²¾åï¼š
> - ğŸ“Š **FLOPs Utilization Analysis**: æ·±åº¦æ€§èƒ½åˆ†æä¸ Roofline å»ºæ¨¡ï¼ˆ20+ å¼  Profiler æˆªå›¾ï¼‰
> - ğŸ”§ **Model Optimization Report**: å®Œæ•´ä¼˜åŒ–è·¯å¾„ä¸ä»£ç å®ç° (428s â†’ 124.9s)
> - ğŸ¬ **I2V Optimization Report**: Image-to-Video ä¸“é¡¹ä¼˜åŒ– (94.5s on v6e-16)

---

## æ ¸å¿ƒå›¾è¡¨é€Ÿè§ˆ

æœ¬èŠ‚å±•ç¤ºæºæ–‡æ¡£ä¸­çš„å…³é”®å¯è§†åŒ–èµ„æ–™ï¼Œå¸®åŠ©å¿«é€Ÿç†è§£ä¼˜åŒ–å…¨è²Œã€‚

### Self-Attention æ€§èƒ½ç“¶é¢ˆåˆ†æ

![Self-Attention å»¶è¿Ÿåˆ†æ](images/profiler_self_attention_latency.png)

*å›¾ï¼šXprof æ˜¾ç¤º Self-Attention å•æ¬¡æ‰§è¡Œå»¶è¿Ÿä¸º 43.93msï¼Œå æ® DiT æ€»æ—¶é—´çš„ 66.8%*

### Kernel å†…éƒ¨æ—¶é—´åˆ†è§£

![Kernel æ—¶é—´åˆ†è§£](images/profiler_kernel_breakdown.png)

*å›¾ï¼šé€šè¿‡ named_scope åˆ†æ Splash Attention Kernel å†…éƒ¨å„æ“ä½œçš„æ—¶é—´å æ¯”ï¼ŒSoftmax å  ~33%*

### æ“ä½œç±»å‹æ—¶é—´åˆ†å¸ƒ

![æ“ä½œæ—¶é—´åˆ†å¸ƒ](images/profiler_time_distribution.png)

*å›¾ï¼šXprof é¥¼å›¾æ˜¾ç¤º custom-call (Splash Attention) å  66.8%ï¼Œconvolution fusion å  14.3%*

### æ•´ä½“ MFU è¡¨ç°

![æ•´ä½“ MFU](images/profiler_overall_mfu.png)

*å›¾ï¼šä¼˜åŒ–åæ•´ä½“ MFU è¾¾åˆ° 34%ï¼Œç›¸æ¯”åŸºçº¿ 12% æå‡æ˜¾è‘—*

### ä¼˜åŒ–æ—¶é—´çº¿

![ä¼˜åŒ–æ—¶é—´çº¿](images/optimization_timeline.png)

*å›¾ï¼šä»åŸºçº¿ 428s åˆ°æœ€ç»ˆ 124.9s çš„å®Œæ•´ä¼˜åŒ–è·¯å¾„ï¼Œæ¯ä¸ªé˜¶æ®µçš„è´¡çŒ®æ¸…æ™°å¯è§*

### DiT åˆ†ç‰‡ç­–ç•¥å›¾

![DiT åˆ†ç‰‡ç­–ç•¥](images/dit_sharding_diagram.png)

*å›¾ï¼šDiT Transformer block çš„ FSDP + CP + SP + DP æ··åˆåˆ†ç‰‡ç­–ç•¥å¯è§†åŒ–*

### VAE Spatial Partitioning

![VAE Spatial Partitioning](images/vae_spatial_partitioning.png)

*å›¾ï¼šVAE åœ¨ Width ç»´åº¦çš„ Spatial Partitioningï¼Œæ¯ä¸ª TPU chip å¤„ç†è§†é¢‘çš„ä¸€ä¸ªå‚ç›´æ¡å¸¦*

---

## ç›®å½•

- [æ ¸å¿ƒå›¾è¡¨é€Ÿè§ˆ](#æ ¸å¿ƒå›¾è¡¨é€Ÿè§ˆ)
- [ç¬¬ä¸€ç« ï¼šTPU v6e ç¡¬ä»¶æ¶æ„ä¸æ€§èƒ½ç‰¹æ€§](#ç¬¬ä¸€ç« tpu-v6e-ç¡¬ä»¶æ¶æ„ä¸æ€§èƒ½ç‰¹æ€§)
- [ç¬¬äºŒç« ï¼šWan æ¨¡å‹æ¶æ„æ·±åº¦è§£æ](#ç¬¬äºŒç« wan-æ¨¡å‹æ¶æ„æ·±åº¦è§£æ)
- [ç¬¬ä¸‰ç« ï¼šåˆ†ç‰‡ç­–ç•¥è¯¦è§£](#ç¬¬ä¸‰ç« åˆ†ç‰‡ç­–ç•¥è¯¦è§£)
- [ç¬¬å››ç« ï¼šSplash Attention å†…æ ¸ä¼˜åŒ–](#ç¬¬å››ç« splash-attention-å†…æ ¸ä¼˜åŒ–)
  - [4.1 ä» Profiler åˆ°ä¼˜åŒ–ç‚¹ï¼šæ€§èƒ½ç“¶é¢ˆåˆ†æ](#41-ä»-profiler-åˆ°ä¼˜åŒ–ç‚¹æ€§èƒ½ç“¶é¢ˆåˆ†æ)
  - [4.2 Pallas Kernel é€è¡Œæ·±åº¦è§£æ](#42-pallas-kernel-é€è¡Œæ·±åº¦è§£æ)
  - [4.3 exp2 æ•°å­¦æ¨å¯¼ä¸å®ç°](#43-exp2-æ•°å­¦æ¨å¯¼ä¸å®ç°)
  - [4.4 QK Transpose ä¼˜åŒ–åŸç†](#44-qk-transpose-ä¼˜åŒ–åŸç†)
  - [4.5 LP LLO Scheduler è°ƒåº¦æœºåˆ¶](#45-lp-llo-scheduler-è°ƒåº¦æœºåˆ¶)
  - [4.6 Block Size é…ç½®åŸç†](#46-block-size-é…ç½®åŸç†)
- [ç¬¬äº”ç« ï¼šVAE åœ¨ Torchax ä¸Šçš„å·¥ä½œåŸç†ä¸å¹¶è¡Œè®¾è®¡](#ç¬¬äº”ç« vae-åœ¨-torchax-ä¸Šçš„å·¥ä½œåŸç†ä¸å¹¶è¡Œè®¾è®¡)
- [ç¬¬å…­ç« ï¼šæ€§èƒ½åˆ†ææ–¹æ³•è®º](#ç¬¬å…­ç« æ€§èƒ½åˆ†ææ–¹æ³•è®º)
- [ç¬¬ä¸ƒç« ï¼šTorchax æ¡¥æ¥ä¸ä»£ç å®ç°](#ç¬¬ä¸ƒç« torchax-æ¡¥æ¥ä¸ä»£ç å®ç°)
- [ç¬¬å…«ç« ï¼šå®Œæ•´ä»£ç ç¤ºä¾‹ä¸å®æˆ˜](#ç¬¬å…«ç« å®Œæ•´ä»£ç ç¤ºä¾‹ä¸å®æˆ˜)
- [ç¬¬ä¹ç« ï¼šImage-to-Video ä¸“é¡¹ä¼˜åŒ–](#ç¬¬ä¹ç« image-to-video-ä¸“é¡¹ä¼˜åŒ–)
- [ç¬¬åç« ï¼šè°ƒè¯•ä¸æ•…éšœæ’é™¤](#ç¬¬åç« è°ƒè¯•ä¸æ•…éšœæ’é™¤)
- [é™„å½•](#é™„å½•)

---

## ç¬¬ä¸€ç« ï¼šTPU v6e ç¡¬ä»¶æ¶æ„ä¸æ€§èƒ½ç‰¹æ€§

### 1.1 TPU v6e æ ¸å¿ƒè§„æ ¼

TPU v6e (ä»£å· Trillium) æ˜¯ Google Cloud æœ€æ–°ä¸€ä»£å¼ é‡å¤„ç†å•å…ƒï¼Œä¸“ä¸ºå¤§è§„æ¨¡æœºå™¨å­¦ä¹ è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–ã€‚

| è§„æ ¼é¡¹ | TPU v6e å•èŠ¯ç‰‡ | TPU v6e-8 | TPU v6e-16 |
|--------|---------------|-----------|------------|
| **å³°å€¼ bf16 TFLOPs** | 918 | 7,344 | 14,688 |
| **HBM å®¹é‡** | 32 GB | 256 GB | 512 GB |
| **HBM å¸¦å®½** | 1,638 GB/s | 13,104 GB/s | 26,208 GB/s |
| **èŠ¯ç‰‡é—´äº’è”** | ICI 3.0 | ç¯å½¢æ‹“æ‰‘ | 2D Torus |
| **MXU è§„æ ¼** | 256Ã—256 | - | - |

```mermaid
graph TB
    subgraph "TPU v6e èŠ¯ç‰‡æ¶æ„"
        HBM[HBM3 32GB<br/>1638 GB/s]
        
        subgraph "TensorCore"
            MXU[MXU 256x256<br/>çŸ©é˜µä¹˜æ³•å•å…ƒ]
            VPU[VPU<br/>å‘é‡å¤„ç†å•å…ƒ]
            SFU[SFU<br/>ç‰¹æ®Šå‡½æ•°å•å…ƒ]
        end
        
        subgraph "å†…å­˜å±‚çº§"
            VMEM[VMEM<br/>å‘é‡å†…å­˜]
            SMEM[SMEM<br/>æ ‡é‡å†…å­˜]
            REG[å¯„å­˜å™¨æ–‡ä»¶]
        end
        
        ICI[ICI 3.0<br/>èŠ¯ç‰‡é—´äº’è”]
    end
    
    HBM --> VMEM
    VMEM --> MXU
    VMEM --> VPU
    VPU --> SFU
    MXU --> REG
    VPU --> REG
    
    style MXU fill:#e1f5fe
    style HBM fill:#fff3e0
    style VPU fill:#f3e5f5
```

### 1.2 è®¡ç®—å•å…ƒæ¶æ„

#### MXU (Matrix Multiply Unit)
- **è§„æ ¼**: 256Ã—256 è„‰åŠ¨é˜µåˆ—
- **æ•°æ®ç±»å‹**: bf16, int8
- **å³°å€¼æ€§èƒ½**: 918 TFLOPs (bf16)
- **å…³é”®é™åˆ¶**: å½“ K ç»´åº¦ < 256 æ—¶ï¼ŒMXU åˆ©ç”¨ç‡ä¸‹é™

```python
# MXU åˆ©ç”¨ç‡è®¡ç®—ç¤ºä¾‹
mxu_size = 256
head_dim = 128  # Wan æ¨¡å‹çš„ head dimension

# K ç»´åº¦ = head_dim = 128ï¼Œåªå ç”¨ MXU ä¸€åŠ
mxu_utilization = head_dim / mxu_size  # = 0.5 = 50%
```

#### VPU (Vector Processing Unit)
- **åŠŸèƒ½**: å‘é‡è¿ç®—ï¼ˆsoftmaxã€layernormã€æ¿€æ´»å‡½æ•°ç­‰ï¼‰
- **ç‰¹ç‚¹**:
  - `exp2` æ¯” `exp` æ›´é«˜æ•ˆï¼ˆåŸç”Ÿç¡¬ä»¶æŒ‡ä»¤ï¼‰
  - æ˜¯ attention softmax çš„ä¸»è¦æ‰§è¡Œå•å…ƒ

### 1.3 å†…å­˜å±‚çº§ä¸å¸¦å®½

```mermaid
graph LR
    subgraph "å†…å­˜å±‚çº§é‡‘å­—å¡”"
        REG[å¯„å­˜å™¨<br/>æœ€å¿«/æœ€å°]
        VMEM[VMEM<br/>å‘é‡å†…å­˜]
        SMEM[SMEM<br/>æ ‡é‡å†…å­˜]
        HBM[HBM<br/>32GB/1638GB/s]
    end
    
    REG --> VMEM --> SMEM --> HBM
    
    style REG fill:#4caf50
    style VMEM fill:#8bc34a
    style HBM fill:#ffeb3b
```

**å…³é”®å¸¦å®½æ•°æ®**:
- HBM å¸¦å®½: 1,638 GB/s
- ç®—æœ¯å¼ºåº¦é˜ˆå€¼: 918 TFLOPs Ã· 1,638 GB/s â‰ˆ **560 FLOPs/Byte**

### 1.4 Roofline æ€§èƒ½æ¨¡å‹

```
æ€§èƒ½ = min(å³°å€¼ç®—åŠ›, ç®—æœ¯å¼ºåº¦ Ã— å†…å­˜å¸¦å®½)
```

**Self-Attention çš„ Roofline åˆ†æ**:

```python
# å¯¹äº Wan 720P: S = 75,600
arithmetic_intensity = 75600 / 2  # = 37,800 FLOPs/Byte

# è¿œå¤§äº 560 çš„é˜ˆå€¼ï¼Œç†è®ºä¸Šåº”è¯¥æ˜¯ compute-bound
# ä½†å®é™… MFU åªæœ‰ 37%ï¼ŒåŸå› åœ¨äº MXU åˆ©ç”¨ç‡ (head_dim=128 < 256)
```

---

## ç¬¬äºŒç« ï¼šWan æ¨¡å‹æ¶æ„æ·±åº¦è§£æ

### 2.1 Wan 2.1 T2V 14B æ¨¡å‹ç»“æ„

```mermaid
graph TB
    subgraph "Wan 2.1 Pipeline"
        TEXT[Text Prompt]
        T5[UMT5 Text Encoder<br/>4096 dims]
        
        NOISE[Random Noise<br/>Latent Space]
        DIT[DiT Transformer<br/>14B Parameters<br/>40 Blocks]
        
        LATENT[Denoised Latents<br/>16 channels]
        VAE[WanVAE Decoder<br/>3D Causal Conv]
        VIDEO[Output Video<br/>720P 81 frames]
    end
    
    TEXT --> T5
    T5 --> DIT
    NOISE --> DIT
    DIT --> LATENT
    LATENT --> VAE
    VAE --> VIDEO
    
    style DIT fill:#e3f2fd
    style VAE fill:#f3e5f5
```

**æ¨¡å‹è§„æ ¼**:

| ç»„ä»¶ | è§„æ ¼ |
|------|------|
| Text Encoder | UMT5-XXL, 4096 hidden dims |
| DiT Blocks | 40 layers |
| Hidden Dimension | 5120 |
| Attention Heads | 40 (128 dims each) |
| FFN Dimension | 13824 (SwiGLU) |
| VAE Latent Channels | 16 |
| Temporal Compression | 4x |
| Spatial Compression | 8x |

### 2.2 DiT Transformer æ¶æ„

```mermaid
graph TB
    subgraph "DiT Block x 40"
        INPUT[Hidden States<br/>B, S, 5120]
        
        subgraph "Self-Attention"
            NORM1[RMSNorm]
            QKV[Q, K, V Projection<br/>5120 â†’ 5120 x 3]
            ATTN[Multi-Head Attention<br/>40 heads, 128 dims]
            OUT1[Output Projection<br/>5120 â†’ 5120]
        end
        
        subgraph "Cross-Attention"
            NORM2[RMSNorm]
            Q2[Q Projection]
            KV2[K, V from Text<br/>4096 â†’ 5120]
            XATTN[Cross Attention]
            OUT2[Output Projection]
        end
        
        subgraph "FFN"
            NORM3[RMSNorm]
            FFN1[Gate + Up<br/>5120 â†’ 27648]
            SILU[SiLU Activation]
            FFN2[Down<br/>13824 â†’ 5120]
        end
        
        OUTPUT[Output]
    end
    
    INPUT --> NORM1 --> QKV --> ATTN --> OUT1
    OUT1 --> NORM2 --> Q2 --> XATTN --> OUT2
    NORM2 --> KV2 --> XATTN
    OUT2 --> NORM3 --> FFN1 --> SILU --> FFN2 --> OUTPUT
```

**Self-Attention åºåˆ—é•¿åº¦è®¡ç®—**:

```python
# 720P 81å¸§è§†é¢‘çš„ latent åºåˆ—é•¿åº¦
height, width, frames = 720, 1280, 81

# VAE å‹ç¼©å
latent_h = height // 8   # = 90
latent_w = width // 8    # = 160
latent_t = (frames - 1) // 4 + 1  # = 21

# Transformer çš„ patch å¤§å°ä¸º 2
patch_h = latent_h // 2  # = 45
patch_w = latent_w // 2  # = 80

# åºåˆ—é•¿åº¦
seq_len = latent_t * patch_h * patch_w
# = 21 * 45 * 80 = 75,600
```

### 2.3 VAE ç¼–è§£ç å™¨

```mermaid
graph LR
    subgraph "WanVAE Encoder"
        VIN[Video Input<br/>B,3,T,H,W]
        CONV_IN[Conv3D In<br/>3â†’96]
        DOWN1[DownBlock 1<br/>96â†’192]
        DOWN2[DownBlock 2<br/>192â†’384<br/>Temporal â†“2]
        DOWN3[DownBlock 3<br/>384â†’384<br/>Temporal â†“2]
        MID_E[MidBlock]
        CONV_OUT[Conv3D Out<br/>384â†’32]
        LATENT[Latent<br/>B,16,T/4,H/8,W/8]
    end
    
    VIN --> CONV_IN --> DOWN1 --> DOWN2 --> DOWN3 --> MID_E --> CONV_OUT --> LATENT
```

---

## ç¬¬ä¸‰ç« ï¼šåˆ†ç‰‡ç­–ç•¥è¯¦è§£

### 3.1 Device Mesh é…ç½®

```python
# v6e-16 é…ç½®: 16 è®¾å¤‡ï¼Œmesh å½¢çŠ¶ (2, 1, 8)
# dp=2: Data Parallelism (CFG æ­£è´Ÿ prompt)
# sp=1: Sequence Parallelism (æœªä½¿ç”¨)
# axis=8: Tensor Parallelism (heads åˆ†ç‰‡)

mesh_devices = mesh_utils.create_device_mesh(
    (2, 1, 8),  # (dp, sp, axis)
    allow_split_physical_axes=True
)
mesh = Mesh(mesh_devices, ('dp', 'sp', 'axis'))
```

**Mesh ç»´åº¦è¯´æ˜**:

| ç»´åº¦ | å€¼ | ç”¨é€” | åˆ†ç‰‡å¯¹è±¡ |
|------|-----|------|----------|
| dp | 2 | Data Parallel | CFG æ­£è´Ÿ prompt |
| sp | 1 | Sequence Parallel | Cross-Attention |
| axis | 8 | Tensor Parallel | Self-Attention heads |

### 3.2 FSDP (Fully Sharded Data Parallelism)

```mermaid
graph TB
    subgraph "FSDP æƒé‡åˆ†ç‰‡"
        W[å®Œæ•´æƒé‡ W<br/>shape: 5120x5120]
        
        subgraph "è®¾å¤‡ 0-7"
            W0[W_shard_0<br/>5120x640]
            W1[W_shard_1<br/>5120x640]
            W7[W_shard_7<br/>5120x640]
        end
        
        AG[AllGather<br/>è®¡ç®—å‰æ”¶é›†]
        RS[ReduceScatter<br/>è®¡ç®—ååˆ†æ•£]
    end
    
    W --> W0 & W1 & W7
    W0 & W1 & W7 --> AG --> RS
```

**FSDP åˆ†ç‰‡è§„åˆ™**:

```python
transformer_shardings_fsdp = {
    # Self-Attention æƒé‡ (attn1)
    r'blocks.\d+.attn1.to_q.weight': (None, ('tp', 'sp')),  # åˆ—å¹¶è¡Œ
    r'blocks.\d+.attn1.to_k.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn1.to_v.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn1.to_out.0.weight': (('tp', 'sp'), None),  # è¡Œå¹¶è¡Œ
    
    # Cross-Attention æƒé‡ (attn2)
    r'blocks.\d+.attn2.to_q.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn2.to_k.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn2.to_v.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn2.to_out.0.weight': (('tp', 'sp'), None),
    
    # FFN æƒé‡
    r'blocks.\d+.ffn.net.0.proj.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.ffn.net.2.weight': (('tp', 'sp'), None),
}
```

### 3.3 Context Parallelism (CP) - Self-Attention

åœ¨ **head number** ç»´åº¦è¿›è¡Œåˆ†ç‰‡ï¼Œä¸“ç”¨äº Self-Attentionã€‚

```python
# Self-Attention åˆ†ç‰‡
q_partition_spec = P('dp', 'tp', 'sp', None)  # [batch, heads, seq, dim]
kv_partition_spec = P('dp', 'tp', None, None)  # K,V åœ¨ seq ç»´åº¦å¤åˆ¶

# 40 heads / 8 devices = 5 heads per device
```

### 3.4 Sequence Parallelism (SP) - Cross-Attention

åœ¨ **sequence** ç»´åº¦è¿›è¡Œåˆ†ç‰‡ï¼Œä¸“ç”¨äº Cross-Attentionã€‚

```python
# Cross-Attention åˆ†ç‰‡ (K,V åºåˆ—é•¿åº¦çŸ­ï¼Œä¸åˆ†ç‰‡)
q_partition_spec = P('dp', None, ('tp', 'sp'), None)  # Q åœ¨ seq ç»´åº¦åˆ†ç‰‡
kv_partition_spec = P('dp', None, None, None)          # K,V å®Œæ•´å¤åˆ¶
```

### 3.5 Data Parallelism (DP) - CFG

ç”¨äºå¤„ç† CFG çš„æ­£è´Ÿ promptã€‚

```python
# dp=2: æ­£è´Ÿ prompt å„ç”¨ä¸€åŠè®¾å¤‡
mesh_dims = (2, 1, 4)  # (dp, sp, tp)
mesh = Mesh(devices, ('dp', 'sp', 'tp'))
```

### 3.6 æ··åˆåˆ†ç‰‡ç­–ç•¥æ€»è§ˆ

![DiT åˆ†ç‰‡ç­–ç•¥è¯¦å›¾](images/dit_sharding_diagram.png)

*å›¾ï¼šDiT Transformer block çš„å®Œæ•´åˆ†ç‰‡ç­–ç•¥ï¼Œå±•ç¤º FSDP æƒé‡åˆ†ç‰‡ã€CP Self-Attentionã€SP Cross-Attention çš„ååŒ*

```mermaid
graph TB
    subgraph "æ··åˆåˆ†ç‰‡ç­–ç•¥"
        TEXT[Text Encoder]
        DIT[DiT Transformer]
        VAE[VAE Decoder]
        
        subgraph "Text Encoder ç­–ç•¥"
            TE_FSDP[FSDP<br/>æƒé‡åˆ†ç‰‡]
        end
        
        subgraph "DiT Transformer ç­–ç•¥"
            DIT_FSDP[FSDP æƒé‡]
            DIT_CP[CP Self-Attn<br/>heads åˆ†ç‰‡]
            DIT_SP[SP Cross-Attn<br/>seq åˆ†ç‰‡]
            DIT_DP[DP CFG<br/>æ­£è´Ÿ prompt]
        end
        
        subgraph "VAE ç­–ç•¥"
            VAE_REP[å¤åˆ¶æƒé‡]
            VAE_SPATIAL[Spatial æ¿€æ´»åˆ†ç‰‡<br/>Width ç»´åº¦]
        end
    end
    
    TEXT --> TE_FSDP
    DIT --> DIT_FSDP & DIT_CP & DIT_SP & DIT_DP
    VAE --> VAE_REP & VAE_SPATIAL
```

**Mesh é…ç½®ä»£ç **:

```python
import jax
from jax.sharding import Mesh, PartitionSpec as P
from jax.experimental import mesh_utils

# 8 è®¾å¤‡é…ç½®: dp=2, sp=1, tp=4
tp_dim, dp_dim, sp_dim = len(jax.devices()), 1, 1

if use_dp:
    tp_dim //= 2
    dp_dim = 2

mesh_devices = mesh_utils.create_device_mesh(
    (dp_dim, sp_dim, tp_dim),
    allow_split_physical_axes=True
)
mesh = Mesh(mesh_devices, ('dp', 'sp', 'tp'))
```

---

## ç¬¬å››ç« ï¼šSplash Attention å†…æ ¸ä¼˜åŒ–

æœ¬ç« æ˜¯å…¨æ–‡**æŠ€æœ¯æ ¸å¿ƒ**ï¼Œæˆ‘ä»¬å°†ä» Profiler æŠ“å–çš„çœŸå®æ•°æ®å‡ºå‘ï¼Œå®Œæ•´å±•ç¤ºï¼š
1. **å¦‚ä½•å‘ç°æ€§èƒ½ç“¶é¢ˆ**ï¼šé€šè¿‡ Xprof å®šä½åˆ° Softmax å  1/3 æ—¶é—´
2. **å¦‚ä½•é˜…è¯» Pallas Kernel ä»£ç **ï¼šé€è¡Œè§£é‡Š `_flash_attention_kernel` çš„æ¯ä¸€è¡Œ
3. **ä¸‰å¤§ä¼˜åŒ–æŠ€æœ¯çš„æ•°å­¦åŸç†**ï¼šexp2ã€QK Transposeã€LP LLO Scheduler

### 4.1 ä» Profiler åˆ°ä¼˜åŒ–ç‚¹ï¼šæ€§èƒ½ç“¶é¢ˆåˆ†æ

#### 4.1.1 æ€§èƒ½åŸºçº¿æµ‹é‡

è¿è¡Œ Xprof åˆ†æ Wan2.1 14B DiTï¼Œç”Ÿæˆ 720P 81å¸§è§†é¢‘ï¼Œè·å¾—ä»¥ä¸‹å…³é”®æ•°æ®ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Xprof æ“ä½œæ—¶é—´åˆ†è§£                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ“ä½œç±»å‹              â”‚ æ—¶é—´å æ¯” â”‚ FLOPs   â”‚ è¯´æ˜                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ custom-call (Splash)  â”‚ 66.8%    â”‚ N/A     â”‚ Self-Attention Kernel â”‚
â”‚ convolution fusion    â”‚ 14.3%    â”‚ æ˜¾ç¤º    â”‚ Linear + FFN          â”‚
â”‚ all-to-all            â”‚ 6.7%     â”‚ N/A     â”‚ CP é€šä¿¡               â”‚
â”‚ data formatting       â”‚ 6.45%    â”‚ N/A     â”‚ copy, reshape         â”‚
â”‚ å…¶ä»–                  â”‚ 5.75%    â”‚ N/A     â”‚ æ‚é¡¹æ“ä½œ              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®å‘ç°**:
- `custom-call` (Pallas kernel) å æ® **66.8%** æ‰§è¡Œæ—¶é—´
- `convolution fusion` (çº¿æ€§å±‚) åªå  14.3%ï¼Œä½† MFU è¾¾åˆ° **66%**
- è¿™è¡¨æ˜ **Attention Kernel æ˜¯ä¸»è¦ä¼˜åŒ–ç›®æ ‡**

#### 4.1.2 Self-Attention çš„ Roofline è®¡ç®—

> ğŸ“Š **æºæ–‡æ¡£å¼•ç”¨**: Self Attention Roofline Calculation ç« èŠ‚

è®©æˆ‘ä»¬å®Œæ•´æ¨å¯¼ Self-Attention çš„ç†è®ºæ€§èƒ½ä¸Šé™ï¼š

**Kernel Setup (å•èŠ¯ç‰‡ï¼Œ8 è·¯åˆ†ç‰‡å)**:
```python
# Q, K, V å½¢çŠ¶ï¼ˆ8 è®¾å¤‡åˆ†ç‰‡åï¼Œå•èŠ¯ç‰‡ï¼‰
# åŸå§‹: [1, 40, 75600, 128]
# åˆ†ç‰‡å: head_num = 40 / 8 = 5

Q: bf16[1, 5, 75600, 128]
K: bf16[1, 5, 75776, 128]  # padding to multiple of 256
V: bf16[1, 5, 75776, 128]
```

**Block åˆ’åˆ†**:
```python
block_q = 3024
block_kv = 2048
block_kv_compute = 1024

# è¿­ä»£æ¬¡æ•°
num_kv_iters = 75776 // 2048 = 37
num_q_iters = 75600 // 3024 = 25
total_iters = 37 * 25 = 925
```

**å• Block çš„ Roofline è®¡ç®—**:

```python
# QK çŸ©é˜µä¹˜: Q[3024, 128] @ K^T[128, 2048] = QK[3024, 2048]
qk_flops = 2 * 3024 * 2048 * 128 = 1.586e9 FLOPs
qk_compute_time = 1.586e9 / 918e12 = 1.728 Î¼s  # compute bound

# QK å†…å­˜: è¯»å– Q + K
qk_memory = (2 * 3024 * 128 + 2 * 2048 * 128) = 1.30 MB
qk_memory_time = 1.30e6 / 1638e9 = 0.794 Î¼s  # ç†è®º

# AV çŸ©é˜µä¹˜: Softmax(QK)[3024, 2048] @ V[2048, 128] = O[3024, 128]
av_flops = 2 * 3024 * 2048 * 128 = 1.586e9 FLOPs
av_compute_time = 1.586e9 / 918e12 = 1.728 Î¼s

# å• block æ€»è®¡
block_roofline = 2 * 1.728 = 3.456 Î¼s (compute bound)
```

**æ€»ä½“ Roofline**:
```python
total_roofline = 3.456 * 925 = 3.197 ms  # å• head
splash_roofline = 5 * 3.197 = 15.974 ms  # 5 heads per chip

# å®æµ‹å»¶è¿Ÿ: 43.93 ms (æºæ–‡æ¡£ image1.png)
# å®é™… MFU = 15.974 / 43.93 = 36.4% â‰ˆ 37%
```

> ğŸ” **å…³é”®æ´å¯Ÿ**: ç†è®ºä¸Š Self-Attention åº”è¯¥æ˜¯ **compute-bound**ï¼Œä½†å®é™… MFU åªæœ‰ 37%ã€‚
> é—®é¢˜å‡ºåœ¨å“ªé‡Œï¼Ÿ

#### 4.1.3 Softmax ç“¶é¢ˆï¼šVPU ä¸Šçš„ 1/3 æ—¶é—´

é€šè¿‡åœ¨ Splash Attention Kernel ä¸­æ·»åŠ  `named_scope`ï¼Œå‘ç°ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Splash Attention Block å†…éƒ¨æ—¶é—´åˆ†è§£                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ“ä½œ                  â”‚ æ—¶é—´å æ¯”                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ QK matmul (MXU)       â”‚ ~35%                                 â”‚
â”‚ Softmax (VPU)         â”‚ ~33% â† ç“¶é¢ˆï¼                        â”‚
â”‚ AV matmul (MXU)       â”‚ ~32%                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¸ºä»€ä¹ˆ Softmax è¿™ä¹ˆæ…¢ï¼Ÿ**

1. **head_dim = 128 é™åˆ¶ MXU åˆ©ç”¨ç‡**
   - TPU v6e MXU æ˜¯ 256Ã—256
   - K ç»´åº¦ = 128 < 256ï¼Œå¯¹è§’çº¿åˆ†å—åªèƒ½ç”¨ä¸€åŠ
   - ç†è®º MXU åˆ©ç”¨ç‡ä¸Šé™ = 50%

2. **exp æ“ä½œè°ƒç”¨ SFU**
   - VPU ä¸Šçš„ `exp` éœ€è¦è°ƒç”¨ SFU (Special Function Unit)
   - è¿™æ˜¯ä¸€ä¸ªé«˜å»¶è¿Ÿæ“ä½œ

3. **reduction æ“ä½œæ•ˆç‡ä½**
   - `max(axis=-1)` æ²¿æœ€åä¸€ä¸ªç»´åº¦è§„çº¦
   - TPU å†…å­˜å¸ƒå±€æ˜¯ 8Ã—128ï¼Œæ²¿ axis=-1 æ•ˆç‡ä½

```mermaid
graph TB
    subgraph "ä» Profiler åˆ°ä¼˜åŒ–ç‚¹"
        P1[Xprof æŠ“å–<br/>MFU 34%]
        P2[Roofline åˆ†æ<br/>ç†è®º MFU > 80%]
        P3[named_scope<br/>Softmax å  1/3]
        
        A1[é—®é¢˜1: head_dim=128<br/>MXU åªç”¨ 50%]
        A2[é—®é¢˜2: exp è°ƒ SFU<br/>VPU æ•ˆç‡ä½]
        A3[é—®é¢˜3: axis=-1 è§„çº¦<br/>å†…å­˜ä¸å‹å¥½]
        
        O1[ä¼˜åŒ–1: exp2<br/>é¢„ä¹˜ log2e]
        O2[ä¼˜åŒ–2: QK Transpose<br/>K @ Q]
        O3[ä¼˜åŒ–3: LP LLO<br/>VPU/MXU é‡å ]
    end
    
    P1 --> P2 --> P3
    P3 --> A1 & A2 & A3
    A1 & A2 --> O1
    A3 --> O2
    O1 & O2 --> O3
    
    style P3 fill:#ff9800
    style O1 fill:#4caf50
    style O3 fill:#2196f3
```

### 4.2 Pallas Kernel é€è¡Œæ·±åº¦è§£æ

æœ¬èŠ‚æ˜¯**å…¨æ–‡æŠ€æœ¯æ ¸å¿ƒä¸­çš„æ ¸å¿ƒ**ã€‚æˆ‘ä»¬å°†é€è¡Œè§£è¯» `custom_splash_attention.py` çš„æ¯ä¸€è¡Œä»£ç ã€‚

#### 4.2.1 æ–‡ä»¶ç»“æ„æ¦‚è§ˆ

```python
# custom_splash_attention.py ç»“æ„

# ç¬¬ 1-30 è¡Œ: å¯¼å…¥å’Œå¸¸é‡å®šä¹‰
import functools
import jax
import jax.numpy as jnp
from jax import lax
from jax.experimental import pallas as pl
from jax.experimental.pallas import tpu as pltpu

# å¸¸é‡
DEFAULT_MASK_VALUE = -0.7 * float(jnp.finfo(jnp.float32).max)
NUM_SUBLANES = 8       # TPU v6e çš„ sublane æ•°é‡
NUM_LANES = 128        # TPU v6e çš„ lane æ•°é‡
NT_DIM_NUMBERS = (((1,), (1,)), ((), ()))  # K @ Q çš„ dot_general è§„æ ¼

# ç¬¬ 31-200 è¡Œ: _flash_attention_kernel æ ¸å¿ƒå®ç°
# ç¬¬ 201-337 è¡Œ: make_splash_mha åŒ…è£…å‡½æ•°
```

#### 4.2.2 å¸¸é‡è§£è¯»

**DEFAULT_MASK_VALUE**:
```python
# ç¬¬ 28 è¡Œ
DEFAULT_MASK_VALUE = -0.7 * float(jnp.finfo(jnp.float32).max)
```

ä¸ºä»€ä¹ˆæ˜¯ `-0.7 Ã— float32_max`ï¼Ÿ
- ç”¨äºåˆå§‹åŒ– running max `m`ï¼Œéœ€è¦ä¸€ä¸ª"è´Ÿæ— ç©·"
- ä½†ä¸èƒ½ç”¨ `-inf`ï¼Œå› ä¸º `exp2(-inf)` ä¼šäº§ç”Ÿ NaN
- `-0.7 Ã— max` è¶³å¤Ÿå°ï¼Œä½¿ `exp2(-0.7 Ã— max) â‰ˆ 0`ï¼Œä¸”æ•°å€¼ç¨³å®š

**NUM_SUBLANES**:
```python
# ç¬¬ 29 è¡Œ
NUM_SUBLANES = 8
```

TPU å†…å­˜å¸ƒå±€:
- TPU çš„ VMEM æŒ‰ `(8, 128)` çš„ tile ç»„ç»‡
- 8 = å­é€šé“æ•° (sublanes)
- 128 = é€šé“æ•° (lanes)
- å­˜å‚¨ `m_scratch` å’Œ `l_scratch` æ—¶ç”¨ `(NUM_SUBLANES, bq)` å½¢çŠ¶

**NT_DIM_NUMBERS**:
```python
# ç¬¬ 31 è¡Œ
NT_DIM_NUMBERS = (((1,), (1,)), ((), ()))
```

è¿™æ˜¯ `lax.dot_general` çš„ç»´åº¦è§„æ ¼ï¼Œè¡¨ç¤ºï¼š

```python
# lax.dot_general(K, Q, NT_DIM_NUMBERS)
# æ”¶ç¼© K çš„ç¬¬ 1 ç»´å’Œ Q çš„ç¬¬ 1 ç»´
#
# K: [seq_k, head_dim]  dim1 = head_dim
# Q: [seq_q, head_dim]  dim1 = head_dim
# ç»“æœ: [seq_k, seq_q]
#
# å¯¹æ¯”æ ‡å‡† Q @ K^T:
# NN_DIM_NUMBERS = (((1,), (0,)), ((), ()))
# Q: [seq_q, head_dim]  dim1 = head_dim
# K^T: [head_dim, seq_k]  dim0 = head_dim
# ç»“æœ: [seq_q, seq_k]
#
# NT = "N transpose"ï¼Œå®é™…æ•ˆæœæ˜¯ K^T @ Q
# ä½†ä¸éœ€è¦æ˜¾å¼ transpose Kï¼
```

#### 4.2.3 Kernel ä¸»å‡½æ•°ç­¾å

```python
def _flash_attention_kernel(
    # === è¾“å…¥å¼•ç”¨ (Pallas ç”¨ Ref è€Œéå€¼ä¼ é€’) ===
    q_ref,           # Query å—å¼•ç”¨, shape: [bq, head_dim]
    k_ref,           # Key å—å¼•ç”¨, shape: [bkv, head_dim]  
    v_ref,           # Value å—å¼•ç”¨, shape: [bkv, head_dim_v]
    
    # === Scratch memory å¼•ç”¨ (åœ¨ VMEM ä¸­åˆ†é…) ===
    m_scratch_ref,   # running max, shape: [NUM_SUBLANES, bq]
    l_scratch_ref,   # running sum, shape: [NUM_SUBLANES, bq]
    o_scratch_ref,   # ç´¯ç§¯è¾“å‡º, shape: [head_dim_v, bq]
    
    # === è¾“å‡ºå¼•ç”¨ ===
    o_ref,           # æœ€ç»ˆè¾“å‡º, shape: [num_heads, head_dim_v, seq_q]
    
    # === Kernel å‚æ•° (ç¼–è¯‘æ—¶å¸¸é‡) ===
    *, 
    mask_value,      # åˆå§‹åŒ– m çš„å€¼
    grid_width,      # KV æ–¹å‘çš„ grid å®½åº¦
    bq,              # Q block size
    bkv,             # KV block size  
    bkv_compute,     # å†…éƒ¨è®¡ç®—çš„ KV å—å¤§å°
    bkv_compute_in,  # æœ€å†…å±‚è¿­ä»£çš„å—å¤§å°
    head_dim_v,      # Value çš„ head dimension
    kv_seq_len,      # KV åºåˆ—é•¿åº¦ (ç”¨äºå¤„ç† padding)
):
```

**å‚æ•°è¯´æ˜è¡¨**:

| å‚æ•° | å…¸å‹å€¼ | ä½œç”¨ |
|------|--------|------|
| `bq` | 3328 | Q å—å¤§å°ï¼Œå½±å“ VMEM å ç”¨ |
| `bkv` | 2816 | KV å—å¤§å°ï¼Œä» HBM åŠ è½½çš„å•ä½ |
| `bkv_compute` | 256 | å†…éƒ¨è¿­ä»£å—ï¼Œå½±å“ Softmax ç²’åº¦ |
| `bkv_compute_in` | 256 | æœ€å†…å±‚è¿­ä»£ï¼Œç”¨äºæµæ°´çº¿ |

#### 4.2.4 Grid ä½ç½®è·å–

```python
    float32 = jnp.float32
    head_dim_v_repeats = head_dim_v // NUM_SUBLANES  # 128 // 8 = 16
    
    # è·å–å½“å‰ grid ä½ç½®
    h, i, j = pl.program_id(0), pl.program_id(1), pl.program_id(2)
    # h: head index [0, num_heads)
    # i: Q block index [0, seq_q // bq)
    # j: KV block index [0, seq_k // bkv)
```

**Grid çš„å«ä¹‰**:
```
Grid = (num_heads, num_q_blocks, num_kv_blocks)
     = (5, 23, 27)  # å¯¹äºå•èŠ¯ç‰‡ (40/8=5 heads, 75600/3328â‰ˆ23, 75776/2816â‰ˆ27)

æ¯ä¸ª grid ç‚¹å¤„ç†:
- 1 ä¸ª attention head
- 1 ä¸ª Q block (bq tokens)
- 1 ä¸ª KV block (bkv tokens)
```

#### 4.2.5 åˆå§‹åŒ–é€»è¾‘

```python
    @pl.when(j == 0)  # åªåœ¨ç¬¬ä¸€ä¸ª KV block æ‰§è¡Œ
    def init():
        # åˆå§‹åŒ–ç´¯ç§¯è¾“å‡ºä¸º 0
        o_scratch_ref[...] = jnp.zeros_like(o_scratch_ref)
        
        # åˆå§‹åŒ– running max ä¸º "è´Ÿæ— ç©·"
        m_scratch_ref[...] = jnp.full_like(m_scratch_ref, mask_value)
        
        # åˆå§‹åŒ– running sum ä¸º 0
        l_scratch_ref[...] = jnp.zeros_like(l_scratch_ref)
```

**ä¸ºä»€ä¹ˆç”¨ `@pl.when`ï¼Ÿ**
- Pallas çš„æ¡ä»¶æ‰§è¡ŒåŸè¯­
- æ¯” Python `if` é«˜æ•ˆï¼Œç¼–è¯‘æˆæ¡ä»¶åˆ†æ”¯æŒ‡ä»¤
- é¿å…åœ¨æ¯ä¸ª KV block éƒ½é‡å¤åˆå§‹åŒ–

#### 4.2.6 ä¸»è®¡ç®—å¾ªç¯ (æœ€å¤æ‚éƒ¨åˆ†)

```python
    def body(kv_compute_index, _):
        """å¤„ç†ä¸€ä¸ª kv_compute å¤§å°çš„å—"""
        
        # è®¡ç®—å½“å‰ K çš„åˆ‡ç‰‡èŒƒå›´
        slice_k = pl.ds(kv_compute_index * bkv_compute, bkv_compute)
        # slice_k = [kv_compute_index * 256, (kv_compute_index + 1) * 256)
        
        # è¯»å–ä¹‹å‰çš„ running stats
        m_prev, l_prev = m_scratch_ref[...], l_scratch_ref[...]
```

**`pl.ds` æ˜¯ä»€ä¹ˆï¼Ÿ**
- `pl.ds(start, size)` = dynamic slice
- è¿”å›ä¸€ä¸ªåˆ‡ç‰‡å¯¹è±¡ï¼Œä¸æ˜¯å®é™…åˆ‡ç‰‡
- è®© Pallas çŸ¥é“è®¿é—®æ¨¡å¼ï¼Œä¼˜åŒ– HBMâ†’VMEM ä¼ è¾“

```python
        # è¯»å– Q å’Œå½“å‰ K å—
        q = q_ref[...]          # æ•´ä¸ª Q block: [bq, head_dim]
        k = k_ref[slice_k, :]   # åˆ‡ç‰‡ K: [bkv_compute, head_dim]
```

**ä¸ºä»€ä¹ˆ Q è¯»å…¨éƒ¨ï¼ŒK è¯»åˆ‡ç‰‡ï¼Ÿ**
- Flash Attention çš„æ ¸å¿ƒï¼šQ å›ºå®šï¼Œéå† K
- æ¯ä¸ªå†…å±‚è¿­ä»£åªéœ€è¦ `bkv_compute` å¤§å°çš„ K
- å‡å°‘ VMEM å ç”¨

```python
        # ğŸ”¥ æ ¸å¿ƒï¼šK @ Q çŸ©é˜µä¹˜
        # æ³¨æ„ï¼ä¸æ˜¯ Q @ K^Tï¼Œæ˜¯ K @ Qï¼
        qk = lax.dot_general(
            k, q, 
            NT_DIM_NUMBERS,  # æ”¶ç¼© K.dim1 å’Œ Q.dim1
            preferred_element_type=float32  # ç”¨ float32 ç´¯ç§¯
        )
        # qk å½¢çŠ¶: [bkv_compute, bq]
        # ä¸æ˜¯ [bq, bkv_compute]ï¼
```

**ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šä¸ºä»€ä¹ˆç”¨ K @ Qï¼Ÿ**

| æ–¹é¢ | Q @ K^T | K @ Q (ä¼˜åŒ–) |
|------|---------|--------------|
| ç»“æœå½¢çŠ¶ | [bq, bkv] | [bkv, bq] |
| max è§„çº¦æ–¹å‘ | axis=-1 (æ•ˆç‡ä½) | axis=0 (æ•ˆç‡é«˜) |
| å†…å­˜è®¿é—® | K éœ€è¦ transpose | K è‡ªç„¶é¡ºåº |

#### 4.2.7 Softmax + Output ç´¯ç§¯ (Online Softmax æ ¸å¿ƒ)

```python
        # è¯»å–ä¹‹å‰çš„ç´¯ç§¯è¾“å‡ºå’Œ V
        o_prev = o_scratch_ref[:]
        v = v_ref[slice_k, :].astype(float32)  # [bkv_compute, head_dim_v]
        
        step = bkv_compute_in  # å†…å±‚è¿­ä»£æ­¥é•¿ = 256
        
        # å†…å±‚è¿­ä»£ï¼Œè¿›ä¸€æ­¥ç»†åˆ†
        for idx in range(0, qk.shape[0], step):
            # === Step 1: è®¡ç®—å½“å‰å—çš„ max ===
            # qk[idx:idx+step] å½¢çŠ¶: [step, bq]
            # max æ²¿ axis=0ï¼ˆç¬¬ä¸€ä¸ªç»´åº¦ï¼‰æ›´å¿«ï¼
            m_curr = qk[idx:idx+step].max(axis=0)[None, :]
            # m_curr å½¢çŠ¶: [1, bq]
            
            # æ›´æ–°å…¨å±€ max
            m_next = jnp.maximum(m_prev, m_curr)
            # m_next å½¢çŠ¶: [1, bq]
```

**ä¸ºä»€ä¹ˆ `max(axis=0)` æ¯” `max(axis=-1)` å¿«ï¼Ÿ**

TPU å†…å­˜å¸ƒå±€æ˜¯ `(8, 128)` çš„ tileï¼š
- `axis=0` è§„çº¦ï¼šåœ¨ 8 ä¸ª sublane é—´è§„çº¦ï¼Œä¸€æ¬¡æŒ‡ä»¤
- `axis=-1` è§„çº¦ï¼šéœ€è¦è·¨ 128 ä¸ª laneï¼Œå¤šæ¬¡æŒ‡ä»¤

```python
            # === Step 2: è®¡ç®— exp2(qk - max) ===
            # ğŸ”¥ ä½¿ç”¨ exp2ï¼Œä¸æ˜¯ expï¼
            s_curr = jnp.exp2(qk[idx:idx+step] - m_next[0:1])
            # s_curr å½¢çŠ¶: [step, bq]
            
            # å› ä¸º Q é¢„ä¹˜äº† log2(e)ï¼Œæ‰€ä»¥:
            # qk = (Q * scale * log2e) @ K^T
            # exp2(qk - max) = exp(scale * Q@K^T - max')
            # æ•°å­¦ä¸Šç­‰ä»·äºæ ‡å‡† softmaxï¼
```

```python
            # === Step 3: è®¡ç®— running sum ===
            l_curr = s_curr.sum(axis=0, keepdims=True)
            # l_curr å½¢çŠ¶: [1, bq]
            
            # ç¼©æ”¾å› å­ï¼šä¹‹å‰çš„ max å˜äº†ï¼Œéœ€è¦ä¿®æ­£ä¹‹å‰çš„ sum
            alpha = jnp.exp2(m_prev - m_next)
            # alpha: ä¿®æ­£å› å­ï¼Œå½“ m_next > m_prev æ—¶ï¼Œalpha < 1
            
            l_next = l_curr + alpha * l_prev
            # Online Softmax çš„ç²¾é«“ï¼š
            # æ–°çš„ sum = å½“å‰å—çš„ sum + ä¿®æ­£åçš„ä¹‹å‰ sum
```

**Online Softmax æ•°å­¦åŸç†**:

æ ‡å‡† Softmax éœ€è¦ä¸¤æ¬¡éå†ï¼š
1. ç¬¬ä¸€æ¬¡ï¼šè®¡ç®—å…¨å±€ max
2. ç¬¬äºŒæ¬¡ï¼šè®¡ç®— exp(x - max) / sum

Online Softmax åªéœ€ä¸€æ¬¡éå†ï¼š
```
m_new = max(m_old, m_curr)
l_new = exp(m_old - m_new) * l_old + sum(exp(x_curr - m_new))
```

```python
            # === Step 4: è®¡ç®— Softmax(QK) @ V ===
            sv_dims = (((0,), (0,)), ((), ()))
            # V[step, head_dim_v] @ S^T[step, bq]
            # æ”¶ç¼© dim0 (step ç»´åº¦)
            o_curr = lax.dot_general(v[idx:idx+step], s_curr, sv_dims)
            # o_curr å½¢çŠ¶: [head_dim_v, bq]ï¼ˆæ³¨æ„æ˜¯è½¬ç½®çš„ï¼ï¼‰
            
            # === Step 5: æ›´æ–°ç´¯ç§¯è¾“å‡º ===
            # åŒæ ·éœ€è¦ç”¨ alpha ä¿®æ­£ä¹‹å‰çš„è¾“å‡º
            o_prev = alpha[0:1, ...] * o_prev + o_curr
            # o_prev å½¢çŠ¶: [head_dim_v, bq]
            
            # æ›´æ–° running stats
            m_prev, l_prev = m_next, l_next
```

```python
        # å­˜å‚¨æ›´æ–°åçš„ running stats
        m_scratch_ref[...], l_scratch_ref[...] = m_next, l_next
        o_scratch_ref[:] = o_prev
```

#### 4.2.8 å¾ªç¯è°ƒåº¦

```python
    # å±•å¼€çš„ fori_loop
    # bkv // bkv_compute = 2816 // 256 = 11 æ¬¡è¿­ä»£
    lax.fori_loop(0, bkv // bkv_compute, body, None, unroll=True)
```

**ä¸ºä»€ä¹ˆ `unroll=True`ï¼Ÿ**
- å±•å¼€å¾ªç¯ï¼Œè®©ç¼–è¯‘å™¨çœ‹åˆ°å®Œæ•´çš„æ•°æ®ä¾èµ–
- å…è®¸ VPU å’Œ MXU é‡å æ‰§è¡Œ
- ä»£ä»·æ˜¯æ›´å¤§çš„ IR / ç¼–è¯‘æ—¶é—´

#### 4.2.9 æœ€ç»ˆå½’ä¸€åŒ–

```python
    @pl.when(j == grid_width - 1)  # åªåœ¨æœ€åä¸€ä¸ª KV block æ‰§è¡Œ
    def end():
        """æœ€ç»ˆå½’ä¸€åŒ–: O = O_unnorm / L"""
        l = l_scratch_ref[...]
        
        # æ‰©å±• l åˆ° [head_dim_v, bq] å½¢çŠ¶
        l_inv = pltpu.repeat(1.0 / l, head_dim_v_repeats, axis=0)
        # l å½¢çŠ¶: [NUM_SUBLANES, bq] = [8, bq]
        # l_inv å½¢çŠ¶: [head_dim_v, bq] = [128, bq]
        
        # å½’ä¸€åŒ–å¹¶è½¬æ¢ç±»å‹
        o_ref[...] = (o_scratch_ref[...] * l_inv).astype(o_ref.dtype)
```

**`pltpu.repeat` çš„ä½œç”¨**:
- åœ¨ axis=0 æ–¹å‘é‡å¤ `head_dim_v_repeats` æ¬¡
- æŠŠ `[8, bq]` å˜æˆ `[128, bq]`
- ç”¨äºå¹¿æ’­é™¤æ³•

### 4.3 exp2 æ•°å­¦æ¨å¯¼ä¸å®ç°

#### 4.3.1 æ•°å­¦ç­‰ä»·æ€§è¯æ˜

**å‘½é¢˜**: `exp(x) = exp2(x * logâ‚‚(e))`

**è¯æ˜**:
```
è®¾ y = exp(x) = e^x

å¯¹ä¸¤è¾¹å– logâ‚‚:
logâ‚‚(y) = logâ‚‚(e^x) = x * logâ‚‚(e)

æ‰€ä»¥:
y = 2^(x * logâ‚‚(e)) = exp2(x * logâ‚‚(e))

å…¶ä¸­ logâ‚‚(e) = 1 / ln(2) â‰ˆ 1.44269504
```

#### 4.3.2 åœ¨ Attention ä¸­çš„åº”ç”¨

**åŸå§‹ Softmax**:
```
softmax(QK)[i,j] = exp(QK[i,j] - max_j) / Î£_k exp(QK[i,k] - max_j)
```

**è½¬æ¢ä¸º exp2**:
```
ä»¤ C = logâ‚‚(e) â‰ˆ 1.4427

softmax(QK)[i,j] 
= exp2((QK[i,j] - max_j) * C) / Î£_k exp2((QK[i,k] - max_j) * C)
= exp2(QK[i,j] * C - max_j * C) / Î£_k exp2(QK[i,k] * C - max_j * C)
```

**é¢„ä¹˜ä¼˜åŒ–**:
```python
# é¢„å…ˆå°† scale å’Œ log2(e) èåˆåˆ° Q ä¸­
# åŸå§‹: Q' = Q * scale
# ä¼˜åŒ–: Q'' = Q * scale * log2(e)

# è¿™æ · QK å°±ç›´æ¥æ˜¯ log2 scaleï¼š
# QK'' = Q'' @ K^T = (Q * scale * log2e) @ K^T
#      = scale * log2e * Q @ K^T

# Softmax è®¡ç®—å˜ä¸º:
# exp2(QK'' - max) = exp2(scale * log2e * QK - max)
#                  = exp(scale * QK - max / log2e)  # è¿‘ä¼¼ç­‰ä»·
```

#### 4.3.3 ä»£ç å®ç°ä½ç½®

**åœ¨ generate_flax.py ä¸­é¢„ä¹˜**:
```python
# generate_flax.py ç¬¬ 387-391 è¡Œ
def _attention_on_slices(q, k, v):
    scale_factor = 1.0 / math.sqrt(q.shape[-1]) if scale is None else scale
    
    # ğŸ”¥ å…³é”®ï¼é¢„ä¹˜ log2(e)
    _LOG2_E = 1.44269504
    q = q * scale_factor * _LOG2_E
    
    # ä¹‹ååœ¨ kernel ä¸­å¯ä»¥ç›´æ¥ç”¨ exp2
```

**åœ¨ kernel ä¸­ä½¿ç”¨ exp2**:
```python
# custom_splash_attention.py ç¬¬ 89, 93 è¡Œ
# ç›´æ¥ä½¿ç”¨ exp2ï¼Œä¸éœ€è¦å†ä¹˜ log2(e)
s_curr = jnp.exp2(qk[idx:idx+step] - m_next[0:1])
alpha = jnp.exp2(m_prev - m_next)
```

### 4.4 QK Transpose ä¼˜åŒ–åŸç†

#### 4.4.1 ä¸ºä»€ä¹ˆç¿»è½¬ç»´åº¦ï¼Ÿ

**TPU å†…å­˜å¸ƒå±€**: `(8 sublanes, 128 lanes)`

| è§„çº¦æ–¹å‘ | ç¡¬ä»¶æ“ä½œ | æ•ˆç‡ |
|----------|----------|------|
| axis=0 (sublane) | 8 è·¯å¹¶è¡Œè§„çº¦ | âš¡ å¿« |
| axis=-1 (lane) | 128 è·¯ä¸²è¡Œè§„çº¦ | ğŸ¢ æ…¢ |

**æ ‡å‡† Attention**:
```python
# Q @ K^T -> [seq_q, seq_k]
# softmax æ²¿ axis=-1 (seq_k æ–¹å‘)
# max/sum éœ€è¦è§„çº¦ 128 ä¸ªå€¼
```

**ä¼˜åŒ– Attention**:
```python
# K @ Q -> [seq_k, seq_q]  
# softmax æ²¿ axis=0 (seq_k æ–¹å‘)
# max/sum åªéœ€è§„çº¦ 8 ä¸ª sublane çš„å€¼
```

#### 4.4.2 ä»£ç å®ç°

```python
# custom_splash_attention.py ç¬¬ 31 è¡Œ
NT_DIM_NUMBERS = (((1,), (1,)), ((), ()))

# ç¬¬ 78 è¡Œ
qk = lax.dot_general(k, q, NT_DIM_NUMBERS, preferred_element_type=float32)
# ç»“æœ: qk[bkv_compute, bq]ï¼Œseq_k åœ¨å‰ï¼
```

**åç»­ä»£ç é€‚é…**:
```python
# max æ²¿ axis=0ï¼ˆseq_k æ–¹å‘ï¼‰
m_curr = qk[idx:idx+step].max(axis=0)[None, :]

# sum æ²¿ axis=0
l_curr = s_curr.sum(axis=0, keepdims=True)

# S @ V çš„ç»´åº¦ä¹Ÿç›¸åº”è°ƒæ•´
# S: [bkv_compute, bq] (seq_k, seq_q)
# V: [bkv_compute, head_dim_v] (seq_k, head_dim)
# æ”¶ç¼© seq_kï¼Œå¾—åˆ° [head_dim_v, seq_q]ï¼ˆè½¬ç½®çš„è¾“å‡ºï¼‰
sv_dims = (((0,), (0,)), ((), ()))  # æ”¶ç¼©ä¸¤è€…çš„ dim0
o_curr = lax.dot_general(v[idx:idx+step], s_curr, sv_dims)
```

### 4.5 LP LLO Scheduler è°ƒåº¦æœºåˆ¶

#### 4.5.1 ä»€ä¹ˆæ˜¯ LP LLOï¼Ÿ

**LP** = Low Precision (ä½ç²¾åº¦)
**LLO** = Low-Level Optimizer (ä½çº§ä¼˜åŒ–å™¨)

LP LLO Scheduler æ˜¯ XLA ç¼–è¯‘å™¨çš„ä¸€ç§è°ƒåº¦ç­–ç•¥ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ– TPU ä¸Šçš„è®¡ç®—é‡å ã€‚

#### 4.5.2 ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ

```mermaid
sequenceDiagram
    participant MXU
    participant VPU
    
    Note over MXU,VPU: é»˜è®¤è°ƒåº¦ï¼ˆä¸²è¡Œï¼‰
    MXU->>MXU: QK matmul (block i)
    VPU->>VPU: Softmax (block i)
    MXU->>MXU: AV matmul (block i)
    MXU->>MXU: QK matmul (block i+1)
    VPU->>VPU: Softmax (block i+1)
    
    Note over MXU,VPU: LP LLO è°ƒåº¦ï¼ˆé‡å ï¼‰
    par å¹¶è¡Œ
        MXU->>MXU: QK matmul (block i+1)
        VPU->>VPU: Softmax (block i)
    end
    par å¹¶è¡Œ
        MXU->>MXU: AV matmul (block i)
        VPU->>VPU: (ç­‰å¾…)
    end
```

**å…³é”®æ´å¯Ÿ**:
- MXU å’Œ VPU æ˜¯ç‹¬ç«‹çš„ç¡¬ä»¶å•å…ƒ
- Softmax åœ¨ VPU ä¸Šæ‰§è¡Œï¼Œmatmul åœ¨ MXU ä¸Šæ‰§è¡Œ
- å¦‚æœä¸é‡å ï¼Œä¸€ä¸ªå•å…ƒç©ºé—²æ—¶å¦ä¸€ä¸ªåœ¨å·¥ä½œ
- LP LLO è°ƒåº¦è®©å®ƒä»¬å°½å¯èƒ½å¹¶è¡Œ

#### 4.5.3 ä»£ç å®ç°

```python
# custom_splash_attention.py ç¬¬ 212-216 è¡Œ

compiler_params = pltpu.CompilerParams(
    # å‘Šè¯‰ç¼–è¯‘å™¨å„ç»´åº¦çš„è¯­ä¹‰
    dimension_semantics=("parallel", "arbitrary", "arbitrary"),
    # å¼ºåˆ¶ä½¿ç”¨ LP LLO è°ƒåº¦å™¨
    flags={"XLA_TPU_FORCE_LP_LLO_SCHEDULER": True}
)
```

**dimension_semantics è§£é‡Š**:
- `"parallel"`: head ç»´åº¦ï¼Œå®Œå…¨ç‹¬ç«‹å¯å¹¶è¡Œ
- `"arbitrary"`: Q/KV block ç»´åº¦ï¼Œç¼–è¯‘å™¨å¯è‡ªç”±é‡æ’

**ä¸ºä»€ä¹ˆ Q/KV æ˜¯ arbitraryï¼Ÿ**
- å…è®¸ç¼–è¯‘å™¨é‡æ’è¿­ä»£é¡ºåº
- å®ç°æµæ°´çº¿ï¼šblock i çš„ Softmax å’Œ block i+1 çš„ QK é‡å 

#### 4.5.4 æ€§èƒ½å½±å“

> ğŸ“Š **æºæ–‡æ¡£æ•°æ®**:
> - æ—  LP LLO: 135.2s
> - æœ‰ LP LLO: 130.1s
> - æå‡: **3.7%**

è¿™ä¸ªä¼˜åŒ–çœ‹èµ·æ¥ä¸å¤§ï¼Œä½†å®ƒæ˜¯"å…è´¹"çš„â€”â€”åªéœ€è¦ä¸€ä¸ªç¼–è¯‘å™¨ flagï¼

### 4.6 Block Size é…ç½®åŸç†

#### 4.6.1 æœ€ä¼˜é…ç½®

```python
# æœ€ä¼˜é…ç½®ï¼ˆ720P 81å¸§ï¼‰
BQSIZE = 3328           # Q å—å¤§å°
BKVSIZE = 2816          # KV å—å¤§å°
BKVCOMPUTESIZE = 256    # å†…éƒ¨è®¡ç®—å—å¤§å°
BKVCOMPUTEINSIZE = 256  # æœ€å†…å±‚è¿­ä»£å—å¤§å°
```

#### 4.6.2 é€‰æ‹©åŸç†

**BQSIZE = 3328**:
```python
# 75600 / 3328 â‰ˆ 22.7ï¼Œå‘ä¸Šå–æ•´ = 23 ä¸ª Q å—
# 75600 = 3328 * 22 + 2784
# æœ€åä¸€ä¸ªå—æœ‰ paddingï¼Œä½†å½±å“ä¸å¤§
```

**BKVSIZE = 2816**:
```python
# 75776 / 2816 â‰ˆ 26.9ï¼Œå‘ä¸Šå–æ•´ = 27 ä¸ª KV å—
# 2816 = 256 * 11ï¼Œæ˜¯ 256 çš„æ•´æ•°å€
# è¿™ç¡®ä¿äº† bkv_compute = 256 èƒ½æ•´é™¤ bkv
```

**BKVCOMPUTESIZE = 256**:
```python
# å¿…é¡»æ˜¯ NUM_LANES = 128 çš„æ•´æ•°å€
# æ›´å°çš„å€¼ (128) ä¼šå¢åŠ è¿­ä»£æ¬¡æ•°
# æ›´å¤§çš„å€¼ (512) ä¼šå¢åŠ  VMEM å ç”¨
# 256 = æœ€ä¼˜å¹³è¡¡ç‚¹
```

### 4.7 å®Œæ•´çš„ make_splash_mha åŒ…è£…å‡½æ•°

```python
def make_splash_mha(block_sizes, bkv_compute_in, interpret=False):
    """
    åˆ›å»º Splash Attention å‡½æ•°
    
    ä½¿ç”¨æ–¹æ³•:
        splash_fn = make_splash_mha(block_sizes, bkv_compute_in)
        output = splash_fn(q, k, v)
    
    æ³¨æ„: Q å¿…é¡»é¢„ä¹˜ log2(e)ï¼
    """
    def _splash_attention(q, k, v):
        num_q_heads, q_seq_len, head_dim_qk = q.shape
        head_dim_v = v.shape[-1]
        num_kv_heads = k.shape[0]
        kv_seq_len = k.shape[1]
        q_heads_per_kv_head = num_q_heads // num_kv_heads
        
        bq, bkv = block_sizes.block_q, block_sizes.block_kv
        bkv_compute = block_sizes.block_kv_compute

        # Index maps: å®šä¹‰æ¯ä¸ªç½‘æ ¼ç‚¹è¯»å–å“ªä¸ªæ•°æ®å—
        def q_index_map(h, i, j, *_): return (h, i, 0)
        def k_index_map(h, i, j, *_): return (h // q_heads_per_kv_head, j, 0)
        def v_index_map(h, i, j, *_): return (h // q_heads_per_kv_head, j, 0)
        def out_index_map(h, i, j, *_): return h, 0, i

        # Input/Output specifications
        in_specs = [
            pl.BlockSpec((None, bq, head_dim_qk), q_index_map),
            pl.BlockSpec((None, bkv, head_dim_qk), k_index_map),
            pl.BlockSpec((None, bkv, head_dim_v), v_index_map),
        ]
        
        # Scratch memory + output shapes
        out_shapes = [
            jax.ShapeDtypeStruct((NUM_SUBLANES, bq), jnp.float32),  # m_scratch
            jax.ShapeDtypeStruct((NUM_SUBLANES, bq), jnp.float32),  # l_scratch
            jax.ShapeDtypeStruct((head_dim_v, bq), jnp.float32),    # o_scratch
            jax.ShapeDtypeStruct((num_q_heads, head_dim_v, q_seq_len), q.dtype),  # output
        ]
        
        out_specs = [
            pl.BlockSpec((NUM_SUBLANES, bq), lambda *_: (0, 0)),
            pl.BlockSpec((NUM_SUBLANES, bq), lambda *_: (0, 0)),
            pl.BlockSpec((head_dim_v, bq), lambda *_: (0, 0)),
            pl.BlockSpec((None, head_dim_v, bq), out_index_map),
        ]
        
        # è®¡ç®—ç½‘æ ¼
        grid_width = kv_seq_len // bkv
        grid = (num_q_heads, q_seq_len // bq, grid_width)

        # è°ƒç”¨ Pallas
        return pl.pallas_call(
            functools.partial(
                _flash_attention_kernel,
                mask_value=DEFAULT_MASK_VALUE,
                grid_width=grid_width,
                bq=bq, bkv=bkv,
                bkv_compute=bkv_compute,
                bkv_compute_in=bkv_compute_in,
                head_dim_v=head_dim_v,
            ),
            grid_spec=pltpu.PrefetchScalarGridSpec(
                num_scalar_prefetch=0,
                in_specs=in_specs,
                out_specs=out_specs,
                grid=grid,
            ),
            # ğŸ”¥ LP LLO Scheduler
            compiler_params=pltpu.CompilerParams(
                dimension_semantics=("parallel", "arbitrary", "arbitrary"),
                flags={"XLA_TPU_FORCE_LP_LLO_SCHEDULER": True}
            ),
            out_shape=out_shapes,
            interpret=interpret,
        )(q, k, v)[-1]  # åªè¿”å›æœ€ç»ˆè¾“å‡º
    
    return _splash_attention
```

### 4.8 æ€§èƒ½ä¼˜åŒ–æ—¶é—´çº¿

![ä¼˜åŒ–æ—¶é—´çº¿è¯¦å›¾](images/optimization_timeline.png)

*å›¾ï¼šå®Œæ•´ä¼˜åŒ–è·¯å¾„çš„æ—¶é—´æ¼”è¿›ï¼Œä»åŸºçº¿åˆ°å„é˜¶æ®µä¼˜åŒ–çš„ç´¯ç§¯æ•ˆæœ*

| é˜¶æ®µ | ä¼˜åŒ–å†…å®¹ | æ—¶é—´ (720P 50æ­¥) | æå‡ |
|------|----------|------------------|------|
| åŸºçº¿ | æ ‡å‡† SDPA | 428s | - |
| é˜¶æ®µ1 | Splash Attention | 285s | 33% â†“ |
| é˜¶æ®µ2 | + exp2 ä¼˜åŒ– | 265s | 7% â†“ |
| é˜¶æ®µ3 | + QK Transpose | 255s | 4% â†“ |
| é˜¶æ®µ4 | + LP LLO Scheduler | 245s | 4% â†“ |
| é˜¶æ®µ5 | + Block Size è°ƒä¼˜ | **124.9s** | 49% â†“ |
| **æ€»è®¡** | **æ‰€æœ‰ä¼˜åŒ–** | **124.9s** | **3.4x** |

---

## ç¬¬äº”ç« ï¼šVAE åœ¨ Torchax ä¸Šçš„å·¥ä½œåŸç†ä¸å¹¶è¡Œè®¾è®¡

### 5.1 æŒ‘æˆ˜ï¼šPyTorch VAE åˆ° TPU

#### 5.1.1 åŸå§‹é—®é¢˜

Wan VAE æ˜¯ç”¨ PyTorch å®ç°çš„ 3D å› æœå·ç§¯ç½‘ç»œã€‚ç›´æ¥åœ¨ TPU ä¸Šè¿è¡Œé¢ä¸´å¤šä¸ªæŒ‘æˆ˜ï¼š

1. **3D å·ç§¯å†…å­˜æ¶ˆè€—å¤§**ï¼š720P è§†é¢‘è§£ç éœ€è¦å¤§é‡å†…å­˜
2. **å› æœå·ç§¯éœ€è¦ç‰¹æ®Šå¤„ç†**ï¼šæ—¶é—´ç»´åº¦çš„å› æœ padding
3. **å¤šè®¾å¤‡å¹¶è¡Œå›°éš¾**ï¼šå·ç§¯æ“ä½œéš¾ä»¥ç›´æ¥åˆ†ç‰‡

```python
# åŸå§‹ VAE æ¶æ„å¤æ‚åº¦
# è¾“å…¥: [B, 16, 21, 90, 160] (latent)
# è¾“å‡º: [B, 3, 81, 720, 1280] (è§†é¢‘)
# ä¸­é—´ç‰¹å¾å›¾æœ€å¤§åˆ° [B, 384, 21, 90, 160]
```

#### 5.1.2 è§£å†³æ–¹æ¡ˆæ¦‚è§ˆ

```mermaid
graph TB
    subgraph "VAE å¹¶è¡ŒåŒ–æ–¹æ¡ˆ"
        DIFF[Diffusers VAE<br/>PyTorch å®ç°]
        TORCHAX[Torchax æ¡¥æ¥å±‚<br/>PyTorch â†’ JAX]
        SHARD[Spatial Sharding<br/>Width ç»´åº¦åˆ†ç‰‡]
        TPU[TPU æ‰§è¡Œ<br/>8 chips å¹¶è¡Œ]
    end
    
    DIFF --> TORCHAX --> SHARD --> TPU
    
    style SHARD fill:#4caf50
```

### 5.2 Spatial Partitioningï¼šWidth ç»´åº¦åˆ†ç‰‡

![VAE Spatial Partitioning è¯¦å›¾](images/vae_spatial_partitioning.png)

*å›¾ï¼šVAE è§£ç å™¨çš„ Spatial Partitioning ç­–ç•¥ï¼Œæ¯ä¸ª TPU chip å¤„ç†è§†é¢‘çš„ä¸€ä¸ªå‚ç›´æ¡å¸¦ï¼Œé€šè¿‡ halo exchange å¤„ç†è¾¹ç•Œ*

#### 5.2.1 ä¸ºä»€ä¹ˆé€‰æ‹© Width ç»´åº¦ï¼Ÿ

| åˆ†ç‰‡ç»´åº¦ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|----------|------|------|
| Batch | ç®€å• | è§†é¢‘ç”Ÿæˆé€šå¸¸ batch=1 |
| Channel | é€šé“ç‹¬ç«‹ | æ‰“ç ´é€šé“é—´ä¾èµ– |
| Temporal | æ—¶é—´ç‹¬ç«‹ | å› æœå·ç§¯éœ€è¦æ—¶é—´è¿ç»­ |
| Height | è¡Œç‹¬ç«‹ | æŸäº›å·ç§¯è·¨è¡Œ |
| **Width** | **åˆ—ç‹¬ç«‹ï¼Œå·ç§¯å‹å¥½** | **éœ€è¦ halo å¤„ç†** |

**Width åˆ†ç‰‡çš„å…³é”®ä¼˜åŠ¿**:
1. 3D å·ç§¯çš„ kernel é€šå¸¸æ˜¯ 3Ã—3Ã—3ï¼Œè·¨åˆ—çš„ä¾èµ–å¯ä»¥é€šè¿‡ halo exchange å¤„ç†
2. å®½åº¦ 160 å¯ä»¥è¢« 8 æ•´é™¤ï¼ˆ160 / 8 = 20ï¼‰
3. æ¯ä¸ª TPU chip å¤„ç†è§†é¢‘çš„ä¸€ä¸ªå‚ç›´æ¡å¸¦

#### 5.2.2 æ ¸å¿ƒä»£ç å®ç°

```python
# autoencoder_kl_wan.py æ ¸å¿ƒå®ç°

from torchax import interop
from jax.sharding import PartitionSpec as P

# åˆ›å»º JAX sharding çº¦æŸçš„ PyTorch è§†å›¾
mark_sharding = interop.torch_view(jax.lax.with_sharding_constraint)


class WanCausalConv3d(nn.Conv3d):
    """å¸¦æœ‰ TPU Spatial Sharding çš„ 3D å› æœå·ç§¯"""
    
    def forward(self, x, cache_x=None):
        # åº”ç”¨ padding
        x = F.pad(x, self._padding)
        
        # ğŸ”¥ æ ¸å¿ƒï¼šåœ¨ Width ç»´åº¦åº”ç”¨ sharding
        success = False
        
        # ç­–ç•¥ 1: dp + tp è”åˆåˆ†ç‰‡
        try:
            x = mark_sharding(x, P(None, None, None, None, ("dp", "tp")))
            success = True
        except ValueError:
            pass
        
        # ç­–ç•¥ 2: ä»… tp åˆ†ç‰‡
        if not success:
            try:
                x = mark_sharding(x, P(None, None, None, None, ("tp",)))
                success = True
            except ValueError:
                pass
        
        # ç­–ç•¥ 3: ä»… dp åˆ†ç‰‡
        if not success:
            try:
                x = mark_sharding(x, P(None, None, None, None, ("dp",)))
                success = True
            except ValueError:
                pass
        
        # æ‰§è¡Œå·ç§¯
        return super().forward(x)
```

### 5.3 VAE è§£ç å™¨çš„é€å¸§å¤„ç†

#### 5.3.1 é€å¸§è§£ç ç­–ç•¥

```python
# autoencoder_kl_wan.py

def _decode(self, z: torch.Tensor, return_dict: bool = True):
    """è§£ç  latent åˆ°è§†é¢‘ï¼ˆé€å¸§å¤„ç†ï¼‰"""
    _, _, num_frame, height, width = z.shape
    
    # æ¸…ç†ç¼“å­˜
    self.clear_cache()
    
    # åé‡åŒ–å·ç§¯
    x = self.post_quant_conv(z)
    
    # ğŸ”¥ é€å¸§è§£ç 
    for i in range(num_frame):
        if i == 0:
            # ç¬¬ä¸€å¸§ï¼šåˆå§‹åŒ–ç¼“å­˜
            out, self._feat_map = self.decoder(
                x[:, :, i : i + 1, :, :],
                feat_cache=self._feat_map,
                first_chunk=True,
            )
        else:
            # åç»­å¸§ï¼šä½¿ç”¨ç¼“å­˜
            out_, self._feat_map = self.decoder(
                x[:, :, i : i + 1, :, :],
                feat_cache=self._feat_map
            )
            out = torch.cat([out, out_], 2)
    
    return DecoderOutput(sample=out)
```

#### 5.3.2 Cache æœºåˆ¶çº¯å‡½æ•°åŒ–

åŸå§‹ VAE ä½¿ç”¨æœ‰çŠ¶æ€çš„ç¼“å­˜ï¼Œè¿™å¯¹ JAX JIT ç¼–è¯‘ä¸å‹å¥½ã€‚è§£å†³æ–¹æ¡ˆæ˜¯å°†ç¼“å­˜ä½œä¸ºå‡½æ•°å‚æ•°ä¼ é€’ï¼š

```python
# åŸå§‹å®ç°ï¼ˆæœ‰çŠ¶æ€ï¼ŒJIT ä¸å‹å¥½ï¼‰
class WanResidualBlock:
    def forward(self, x):
        # ç¼“å­˜å­˜å‚¨åœ¨ self._cache ä¸­
        if self._cache is not None:
            x = torch.cat([self._cache, x], dim=2)
        self._cache = x[:, :, -2:]
        return self.conv(x)

# ä¼˜åŒ–åï¼ˆæ— çŠ¶æ€ï¼ŒJIT å‹å¥½ï¼‰
class WanResidualBlock:
    def forward(self, x, feat_cache=None, feat_idx=[0]):
        # ç¼“å­˜ä½œä¸ºå‚æ•°ä¼ é€’
        if feat_cache is not None:
            idx = feat_idx[0]
            if feat_cache[idx] is not None:
                x = torch.cat([feat_cache[idx], x], dim=2)
            feat_cache[idx] = x[:, :, -2:]
            feat_idx[0] += 1
        return self.conv(x), feat_cache
```

### 5.4 Halo Exchange å¤„ç†è¾¹ç•Œ

å½“åœ¨ Width ç»´åº¦åˆ†ç‰‡åï¼Œ3Ã—3Ã—3 å·ç§¯åœ¨è¾¹ç•Œå¤„éœ€è¦ç›¸é‚»åˆ†ç‰‡çš„æ•°æ®ï¼š

```mermaid
graph LR
    subgraph "Width åˆ†ç‰‡åçš„ Halo Exchange"
        S0[Shard 0<br/>W: 0-19]
        S1[Shard 1<br/>W: 20-39]
        S2[Shard 2<br/>W: 40-59]
        
        H01[Halo<br/>W: 19-20]
        H12[Halo<br/>W: 39-40]
    end
    
    S0 <-->|è¾¹ç•Œäº¤æ¢| H01 <-->|è¾¹ç•Œäº¤æ¢| S1
    S1 <-->|è¾¹ç•Œäº¤æ¢| H12 <-->|è¾¹ç•Œäº¤æ¢| S2
```

**XLA è‡ªåŠ¨å¤„ç†**:
```python
# XLA ç¼–è¯‘å™¨è¯†åˆ«å·ç§¯æ“ä½œéœ€è¦ halo exchange
# è‡ªåŠ¨æ’å…¥ collective-permute æ“ä½œ
# ä»£ç ä¸­æ— éœ€æ˜¾å¼å¤„ç†ï¼

x = mark_sharding(x, P(None, None, None, None, ("dp", "tp")))
# XLA ä¼šåœ¨éœ€è¦æ—¶è‡ªåŠ¨äº¤æ¢è¾¹ç•Œæ•°æ®
```

### 5.5 I2V ç‰¹æ®Šä¼˜åŒ–ï¼šæ¶ˆé™¤ segment_id

I2V åœºæ™¯ä¸‹ï¼Œç¬¬ä¸€å¸§æ˜¯è¾“å…¥å›¾åƒï¼Œä¸éœ€è¦ padding maskã€‚å¯ä»¥é€šè¿‡ä¿®æ”¹ kernel æ¶ˆé™¤ `segment_id` å‚æ•°ï¼š

```python
# åŸå§‹ kernelï¼ˆéœ€è¦ segment_id å¤„ç† paddingï¼‰
def attention_kernel(q, k, v, segment_id):
    # æ ¹æ® segment_id åˆ›å»º mask
    mask = create_mask(segment_id)
    qk = q @ k.T
    qk = qk + mask  # åº”ç”¨ mask
    return softmax(qk) @ v

# ä¼˜åŒ–åï¼ˆæ—  segment_idï¼‰
def attention_kernel_no_segment(q, k, v):
    # å‡è®¾æ—  paddingï¼Œç›´æ¥è®¡ç®—
    qk = q @ k.T
    return softmax(qk) @ v
```

### 5.6 VAE æ€§èƒ½å¯¹æ¯”

| é…ç½® | æ—¶é—´ | å†…å­˜/chip |
|------|------|-----------|
| å•è®¾å¤‡ | OOM | - |
| 8 è®¾å¤‡ (æ— åˆ†ç‰‡) | 45s | 24GB |
| 8 è®¾å¤‡ (Width åˆ†ç‰‡) | **12s** | **8GB** |

---

## ç¬¬å…­ç« ï¼šæ€§èƒ½åˆ†ææ–¹æ³•è®º

### 6.1 MFU è®¡ç®—æ–¹æ³•

```python
def compute_dit_flops_per_step(
    batch_size=2,       # CFG æ­£è´Ÿ prompt
    num_blocks=40,      # DiT blocks
    hidden_dim=5120,    # Hidden dimension
    num_heads=40,       # Attention heads
    head_dim=128,       # Head dimension
    ffn_dim=13824,      # FFN hidden dimension
    seq_len=75600,      # Video sequence length
    text_seq_len=226,   # Text sequence length
):
    """è®¡ç®—å•æ­¥ DiT FLOPs"""
    
    # === Self-Attention FLOPs ===
    # Q, K, V æŠ•å½±: 3 Ã— 2 Ã— S Ã— D Ã— D
    qkv_proj = 3 * 2 * seq_len * hidden_dim * hidden_dim
    
    # QK çŸ©é˜µä¹˜: 2 Ã— B Ã— H Ã— S Ã— d Ã— S
    qk_matmul = 2 * batch_size * num_heads * seq_len * head_dim * seq_len
    
    # AV çŸ©é˜µä¹˜: 2 Ã— B Ã— H Ã— S Ã— S Ã— d
    av_matmul = 2 * batch_size * num_heads * seq_len * seq_len * head_dim
    
    # è¾“å‡ºæŠ•å½±: 2 Ã— S Ã— D Ã— D
    out_proj = 2 * seq_len * hidden_dim * hidden_dim
    
    self_attn = qkv_proj + qk_matmul + av_matmul + out_proj
    
    # === Cross-Attention FLOPs ===
    q_proj = 2 * seq_len * hidden_dim * hidden_dim
    kv_proj = 2 * 2 * text_seq_len * hidden_dim * hidden_dim
    cross_qk = 2 * batch_size * num_heads * seq_len * head_dim * text_seq_len
    cross_av = 2 * batch_size * num_heads * seq_len * text_seq_len * head_dim
    cross_out = 2 * seq_len * hidden_dim * hidden_dim
    
    cross_attn = q_proj + kv_proj + cross_qk + cross_av + cross_out
    
    # === FFN FLOPs ===
    # SwiGLU: gate_up + gate_mul + down
    ffn = 2 * 2 * seq_len * hidden_dim * ffn_dim + \
          seq_len * ffn_dim + \
          2 * seq_len * ffn_dim * hidden_dim
    
    # æ€»è®¡ (æ‰€æœ‰ blocks)
    total_flops = num_blocks * (self_attn + cross_attn + ffn)
    
    return total_flops

# MFU è®¡ç®—
flops_per_step = compute_dit_flops_per_step()  # â‰ˆ 2.85e15
step_time = 2.5  # ç§’
peak_tflops = 14688e12  # v6e-16 å³°å€¼ bf16

mfu = flops_per_step / (peak_tflops * step_time)  # â‰ˆ 34%
```

### 6.2 DiT Step æ—¶é—´åˆ†è§£

| æ“ä½œ | æ—¶é—´å æ¯” | MFU | ç“¶é¢ˆç±»å‹ |
|------|----------|-----|----------|
| Self-Attention | 66.8% | 37% | VPU-bound |
| Convolution Fusion | 14.3% | 66% | Compute-bound |
| All-to-All | 6.7% | - | ICI å¸¦å®½ |
| Data Formatting | 6.45% | - | å†…å­˜å¸¦å®½ |

### 6.3 Profiler ä½¿ç”¨æŒ‡å—

```python
# 1. å¯ç”¨ profiler
with jax.profiler.trace("/dev/shm/tensorboard"):
    output = pipe(prompt=prompt, num_inference_steps=3)
    jax.effects_barrier()

# 2. æŸ¥çœ‹ TensorBoard
# tensorboard --logdir=/dev/shm/tensorboard

# 3. åˆ†æå…³é”®æŒ‡æ ‡
# - MXU åˆ©ç”¨ç‡
# - å†…å­˜å¸¦å®½åˆ©ç”¨ç‡
# - é€šä¿¡å¼€é”€
```

---

## ç¬¬ä¸ƒç« ï¼šTorchax æ¡¥æ¥ä¸ä»£ç å®ç°

### 7.1 Torchax åˆå§‹åŒ–

```python
import torchax

# å…¨å±€å¯ç”¨ torchax
torchax.enable_globally()
env = torchax.default_env()

# é…ç½® mesh
env._mesh = mesh
env._initial_content.mesh = mesh
env.config.use_tpu_splash_attention = True
```

### 7.2 ç®—å­æ³¨å†Œä¸è¦†ç›–

```python
from torchax.ops import ops_registry

def scaled_dot_product_attention(query, key, value, env=None, **kwargs):
    """è‡ªå®šä¹‰ attention å®ç°"""
    if getattr(env.config, 'use_tpu_splash_attention', False):
        jquery, jkey, jvalue = env.t2j_iso((query, key, value))
        
        if USE_K_SMOOTH:
            key_mean = jnp.mean(jkey, axis=2, keepdims=True)
            jkey = jkey - key_mean
        
        if jkey.shape[2] > 10000 and USE_CUSTOM_ATTENTION:
            res = _tpu_custom_attention(jquery, jkey, jvalue, env)
        else:
            res = _tpu_splash_attention(jquery, jkey, jvalue, env)
        
        return env.j2t_iso(res)
    
    return _sdpa_reference(query, key, value, **kwargs)

# æ³¨å†Œ
env._ops[torch.nn.functional.scaled_dot_product_attention] = \
    ops_registry.Operator(
        torch.nn.functional.scaled_dot_product_attention,
        functools.partial(scaled_dot_product_attention, env=env),
        is_jax_function=False, is_user_defined=True,
        needs_env=False, is_view_op=False,
    )
```

### 7.3 æƒé‡åˆ†ç‰‡

```python
import re
from jax.sharding import NamedSharding, PartitionSpec as P

def shard_weight_dict(weight_dict, sharding_dict, mesh):
    """æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™åˆ†ç‰‡æƒé‡"""
    result = {}
    for k, v in weight_dict.items():
        matched = False
        for pattern, sharding in sharding_dict.items():
            if re.fullmatch(pattern, k) is not None:
                v.apply_jax_(jax.device_put,
                            NamedSharding(mesh, P(*sharding)))
                matched = True
                break
        if not matched:
            # é»˜è®¤å¤åˆ¶
            v.apply_jax_(jax.device_put, NamedSharding(mesh, P()))
        result[k] = v
    return result
```

---

## ç¬¬å…«ç« ï¼šå®Œæ•´ä»£ç ç¤ºä¾‹ä¸å®æˆ˜

### 8.1 ç¯å¢ƒé…ç½®

```bash
# å®‰è£…ä¾èµ–
pip install torch --index-url https://download.pytorch.org/whl/cpu
pip install -U jax[tpu] torchax
pip install transformers accelerate safetensors flax optax

# å®‰è£…ä¿®æ”¹ç‰ˆ diffusers
git clone https://github.com/yangwhale/diffusers-tpu.git
cd diffusers-tpu && pip install -e .
```

### 8.2 å®Œæ•´ T2V Pipeline

```python
"""Wan 2.1 Text-to-Video on TPU v6e"""

import jax
import torch
import torchax
from jax.sharding import Mesh, PartitionSpec as P
from jax.experimental import mesh_utils

MODEL_ID = "Wan-AI/Wan2.1-T2V-14B-Diffusers"
HEIGHT, WIDTH, FRAMES = 720, 1280, 81
NUM_STEPS = 50

def main():
    # JAX é…ç½®
    jax.config.update("jax_compilation_cache_dir", "/dev/shm/jax_cache")
    torch.set_default_dtype(torch.bfloat16)
    
    # åˆ›å»º Mesh
    num_devices = len(jax.devices())
    mesh_devices = mesh_utils.create_device_mesh(
        (2, 1, num_devices // 2),
        allow_split_physical_axes=True
    )
    mesh = Mesh(mesh_devices, ('dp', 'sp', 'tp'))
    
    # åˆå§‹åŒ– torchax
    torchax.enable_globally()
    env = torchax.default_env()
    env._mesh = mesh
    env.config.use_tpu_splash_attention = True
    
    # åŠ è½½ Pipeline
    from diffusers import WanPipeline, UniPCMultistepScheduler
    
    torchax.disable_globally()
    scheduler = UniPCMultistepScheduler(
        prediction_type='flow_prediction',
        use_flow_sigmas=True,
        flow_shift=5.0
    )
    pipe = WanPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16)
    pipe.scheduler = scheduler
    torchax.enable_globally()
    
    # ç”Ÿæˆ
    prompt = "A cat and a dog baking a cake together in a kitchen."
    
    with mesh:
        output = pipe(
            prompt=prompt,
            height=HEIGHT, width=WIDTH, num_frames=FRAMES,
            num_inference_steps=NUM_STEPS,
            guidance_scale=5.0,
            use_dp=True,
        )
    
    from diffusers.utils import export_to_video
    export_to_video(output.frames[0], "output.mp4", fps=16)

if __name__ == "__main__":
    main()
```

---

## ç¬¬ä¹ç« ï¼šImage-to-Video ä¸“é¡¹ä¼˜åŒ–

### 9.1 I2V ä¸ T2V çš„å…³é”®å·®å¼‚

```mermaid
graph TB
    subgraph "T2V Pipeline"
        T_NOISE[Random Noise<br/>å…¨éƒ¨å¸§]
        T_DIT[DiT Denoising<br/>æ‰€æœ‰å¸§ç›¸åŒ timestep]
    end
    
    subgraph "I2V Pipeline"
        I_IMG[Input Image<br/>ç¬¬ä¸€å¸§]
        I_NOISE[Random Noise<br/>åç»­å¸§]
        I_CONCAT[Concatenate<br/>Image + Noise]
        I_DIT[DiT Denoising<br/>expand_timesteps]
    end
    
    T_NOISE --> T_DIT
    I_IMG --> I_CONCAT
    I_NOISE --> I_CONCAT
    I_CONCAT --> I_DIT
    
    style I_DIT fill:#fff3e0
```

### 9.2 expand_timesteps æœºåˆ¶

```python
def expand_timesteps(timesteps, num_frames, device):
    """
    I2V çš„ timestep æ‰©å±•
    
    ç¬¬ä¸€å¸§: timestep = 0 (å¹²å‡€å›¾åƒ)
    å…¶ä½™å¸§: timestep = t (æ­£å¸¸å»å™ª)
    """
    expanded = torch.zeros(num_frames, device=device)
    expanded[1:] = timesteps
    return expanded

# ä½¿ç”¨
for t in timesteps:
    t_expanded = expand_timesteps(t, num_frames=81, device=device)
    # t_expanded = [0, t, t, t, ..., t]
```

### 9.3 I2V æ€§èƒ½æ•°æ®

![I2V æ¶æ„å›¾](images/i2v_architecture.png)

*å›¾ï¼šI2V Pipeline æ¶æ„ï¼Œå±•ç¤ºç¬¬ä¸€å¸§ä½œä¸ºè¾“å…¥å›¾åƒçš„ç‰¹æ®Šå¤„ç†*

| é…ç½® | T2V æ—¶é—´ | I2V æ—¶é—´ |
|------|----------|----------|
| v6e-8 | 225s | 184.7s |
| v6e-16 | 124.9s | **94.5s** |

**I2V æ›´å¿«çš„åŸå› **:
1. ç¬¬ä¸€å¸§ä¸éœ€è¦å»å™ªï¼ˆtimestep=0ï¼‰
2. Image ä½œä¸ºå¼ºå¼•å¯¼ï¼Œæ”¶æ•›æ›´å¿«
3. å¯ä½¿ç”¨æ›´å°‘çš„ inference steps

---

## ç¬¬åç« ï¼šè°ƒè¯•ä¸æ•…éšœæ’é™¤

### 10.1 å¸¸è§é—®é¢˜

#### é—®é¢˜ 1: VAE é¢œè‰²åè½¬

**ç—‡çŠ¶**: ç”Ÿæˆçš„è§†é¢‘é¢œè‰²ä¸é¢„æœŸç›¸å

**è§£å†³æ–¹æ¡ˆ**:
```python
# è¾“å‡ºåå¤„ç†
video = 255 - video
```

#### é—®é¢˜ 2: bfloat16 ä¿å­˜å¤±è´¥

**ç—‡çŠ¶**: `safetensors` ä¸æ”¯æŒ bf16 ç›´æ¥ä¿å­˜

**è§£å†³æ–¹æ¡ˆ**:
```python
def save_bf16_tensor(tensor, path):
    if tensor.dtype == torch.bfloat16:
        tensor_save = tensor.to(torch.float32)
        metadata = {'original_dtype': 'bfloat16'}
    else:
        tensor_save = tensor
        metadata = {}
    save_file({'tensor': tensor_save}, path, metadata=metadata)
```

#### é—®é¢˜ 3: OOM

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨åˆ†ç‰‡
mesh = Mesh(devices, ('dp', 'sp', 'tp'))

# 2. åˆ†é˜¶æ®µé‡Šæ”¾å†…å­˜
del text_encoder
gc.collect()

# 3. ä½¿ç”¨ donation
@jax.jit(donate_argnums=(0,))
def step(state, inputs):
    return new_state
```

### 10.2 è°ƒè¯•æŠ€å·§

```python
def debug_sharding(tensor, name="tensor"):
    """æ‰“å° tensor çš„åˆ†ç‰‡ä¿¡æ¯"""
    if hasattr(tensor, '_jax_array'):
        jax_arr = tensor._jax_array
        print(f"{name}:")
        print(f"  Shape: {jax_arr.shape}")
        print(f"  Sharding: {jax_arr.sharding}")
        print(f"  Devices: {jax_arr.devices()}")
```

---

## é™„å½•

### A. æœ¯è¯­è¡¨

| æœ¯è¯­ | å…¨ç§° | è¯´æ˜ |
|------|------|------|
| MFU | Model FLOPs Utilization | æ¨¡å‹è®¡ç®—åˆ©ç”¨ç‡ |
| MXU | Matrix Multiply Unit | çŸ©é˜µä¹˜æ³•å•å…ƒ |
| VPU | Vector Processing Unit | å‘é‡å¤„ç†å•å…ƒ |
| HBM | High Bandwidth Memory | é«˜å¸¦å®½å†…å­˜ |
| ICI | Inter-Chip Interconnect | èŠ¯ç‰‡é—´äº’è” |
| FSDP | Fully Sharded Data Parallel | å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ |
| CP | Context Parallelism | ä¸Šä¸‹æ–‡å¹¶è¡Œ |
| SP | Sequence Parallelism | åºåˆ—å¹¶è¡Œ |
| DP | Data Parallelism | æ•°æ®å¹¶è¡Œ |

### B. æ€§èƒ½æ•°æ®æ±‡æ€»

| åœºæ™¯ | é…ç½® | æ—¶é—´ | MFU |
|------|------|------|-----|
| T2V 720P 81å¸§ | v6e-8 åŸºçº¿ | 428s | 12% |
| T2V 720P 81å¸§ | v6e-8 ä¼˜åŒ–å | 225s | 23% |
| T2V 720P 81å¸§ | v6e-16 ä¼˜åŒ–å | 124.9s | 34% |
| I2V 720P 81å¸§ | v6e-8 ä¼˜åŒ–å | 184.7s | 28% |
| I2V 720P 81å¸§ | v6e-16 ä¼˜åŒ–å | 94.5s | 38% |

### C. æœ¬æ–‡æ¡£å›¾è¡¨ç´¢å¼•

æœ¬æ–‡æ¡£åŒ…å«ä»¥ä¸‹å…³é”®å›¾è¡¨ï¼ˆå‡ä½äº `images/` æ–‡ä»¶å¤¹ï¼‰ï¼š

| æ–‡ä»¶å | æè¿° | æ¥æº |
|--------|------|------|
| `profiler_self_attention_latency.png` | Self-Attention å»¶è¿Ÿ 43.93ms | FLOPs Analysis |
| `profiler_kernel_breakdown.png` | Kernel å†…éƒ¨æ—¶é—´åˆ†è§£ | FLOPs Analysis |
| `profiler_time_distribution.png` | æ“ä½œç±»å‹æ—¶é—´åˆ†å¸ƒé¥¼å›¾ | FLOPs Analysis |
| `profiler_overall_mfu.png` | æ•´ä½“ MFU 34% | FLOPs Analysis |
| `optimization_timeline.png` | ä¼˜åŒ–æ—¶é—´çº¿ 428sâ†’124.9s | Optimization Report |
| `dit_sharding_diagram.png` | DiT åˆ†ç‰‡ç­–ç•¥å›¾ | Optimization Report |
| `i2v_architecture.png` | I2V æ¶æ„å›¾ | I2V Report |
| `vae_spatial_partitioning.png` | VAE Spatial Partitioning | I2V Report |

### D. å‚è€ƒèµ„æº

**å®˜æ–¹ä»“åº“**:
- [Wan-AI/Wan2.1](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B-Diffusers)
- [AI-Hypercomputer/maxdiffusion](https://github.com/AI-Hypercomputer/maxdiffusion)

**æŠ€æœ¯æ–‡æ¡£**:
- [JAX Pallas Guide](https://jax.readthedocs.io/en/latest/pallas/)
- [TPU Performance Guide](https://cloud.google.com/tpu/docs/performance-guide)
- [Flash Attention Paper](https://arxiv.org/abs/2205.14135)

---

## ç»“è¯­

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº† Wan æ¨¡å‹åœ¨ TPU v6e ä¸Šçš„è¿ç§»ä¸ä¼˜åŒ–è¿‡ç¨‹ã€‚é€šè¿‡è¿™äº›ä¼˜åŒ–ï¼Œå®ç°äº†ï¼š

- **T2V**: 428s â†’ 124.9s (**3.4x æå‡**)
- **I2V**: 94.5s on v6e-16 (**æœ€ä½³æ€§èƒ½**)

**æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯**:
1. **Splash Attention** + exp2 + QK Transpose + LP LLO
2. **Spatial Partitioning** for VAE
3. **FSDP + CP + SP + DP** æ··åˆåˆ†ç‰‡ç­–ç•¥

```mermaid
graph TB
    subgraph "ä¼˜åŒ–è·¯å¾„æ€»ç»“"
        O1[Context Parallelism<br/>Self-Attention heads åˆ†ç‰‡]
        O2[Sequence Parallelism<br/>Cross-Attention seq åˆ†ç‰‡]
        O3[exp2 ä¼˜åŒ–<br/>VPU åŸç”ŸæŒ‡ä»¤]
        O4[QK Transpose<br/>çŸ©é˜µä¹˜æ³•ä¼˜åŒ–]
        O5[LP LLO è°ƒåº¦<br/>VPU/MXU é‡å ]
        O6[Spatial Partitioning<br/>VAE å®½åº¦åˆ†ç‰‡]
    end
    
    O1 --> O2 --> O3 --> O4 --> O5 --> O6
    
    style O3 fill:#4caf50
    style O5 fill:#2196f3
```

å¸Œæœ›æœ¬æ–‡æ¡£èƒ½ä¸º TPU å¤§æ¨¡å‹ä¼˜åŒ–æä¾›æœ‰ä»·å€¼çš„å‚è€ƒã€‚