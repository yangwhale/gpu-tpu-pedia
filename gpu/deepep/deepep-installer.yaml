apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: deepep-installer-ubuntu
  namespace: kube-system
  labels:
    k8s-app: deepep-installer-ubuntu
spec:
  selector:
    matchLabels:
      k8s-app: deepep-installer-ubuntu
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        name: deepep-installer-ubuntu
        k8s-app: deepep-installer-ubuntu
    spec:
      priorityClassName: system-node-critical
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: cloud.google.com/gke-accelerator
                    operator: In
                    values:
                      - nvidia-b200
                  - key: cloud.google.com/gke-nodepool
                    operator: In
                    values:
                      - a4-highgpu-ubuntu
                  - key: cloud.google.com/gke-os-distribution
                    operator: In
                    values:
                      - ubuntu
                  # - key: kubernetes.io/hostname
                  #   operator: In
                  #   values:
                  #     - "gke-chrisya-gke-a4-a4-highgpu-ubuntu--752c886b-kkgn"
      tolerations:
        - operator: "Exists"
      hostNetwork: true
      hostPID: true
      volumes:
        - name: root
          hostPath:
            path: /
      initContainers:
        - image: gke.gcr.io/gke-distroless/bash:latest
          name: deepep-installer
          resources:
            requests:
              cpu: 150m
          securityContext:
            privileged: true
          volumeMounts:
            - name: root
              mountPath: /
          env:
            - name: NVSHMEM_IBGDA_SUPPORT
              value: "1"
            - name: NVSHMEM_USE_GDRCOPY
              value: "1"
            - name: GDRCOPY_HOME
              value: /opt/deepep/gdrcopy
            - name: NVSHMEM_HOME
              value: /opt/deepep/nvshmem
            - name: USE_NVPEERMEM
              value: "1"
            - name: CUDA_HOME
              value: /usr/local/cuda
            - name: TORCH_CUDA_ARCH_LIST_B200
              value: "10.0"
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -ex
              pushd ~
              export DEBIAN_FRONTEND=noninteractive
              apt-get update -y -qq

              # Install DOCA OFED
              if ! command -v ofed_info &> /dev/null; then
                echo "DOCA OFED not found, installing now..."
                wget https://www.mellanox.com/downloads/DOCA/DOCA_v3.0.0/host/doca-host_3.0.0-058000-25.04-ubuntu2404_amd64.deb
                dpkg -i doca-host_3.0.0-058000-25.04-ubuntu2404_amd64.deb
                apt-get update -y -qq && apt-get -y install doca-ofed
                ofed_info -s
              else
                echo "DOCA OFED is already installed"
                ofed_info -s
              fi

              # Install GPU driver and CUDA
              if ! lsmod | grep -q nvidia; then
                echo "GPU driver module not found, installing now..."
                wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
                dpkg -i cuda-keyring_1.1-1_all.deb
                apt-get update -y && apt install nvidia-open-575 -y
                apt-get -y install cuda-toolkit-12.9
                nvidia-smi
              else
                echo "GPU driver is already installed"
                nvidia-smi
              fi

              # Build gdrcopy
              if [ "$NVSHMEM_USE_GDRCOPY" == "1" ]; then
                echo "NVSHMEM_USE_GDRCOPY=1, checking if gdrcopy is installed..."
                if ! ls /usr/lib/x86_64-linux-gnu/libgdr* >/dev/null 2>&1; then
                  echo "gdrcopy library not found, installing now..."
                  # Install dependencies
                  apt-get install -y -qq --no-install-recommends build-essential devscripts debhelper fakeroot pkg-config dkms -y
                  # Build gdrcopy
                  rm -rf /tmp/gdrcopy
                  git clone https://github.com/NVIDIA/gdrcopy.git /tmp/gdrcopy
                  cd /tmp/gdrcopy && git checkout tags/v2.5.1
                  make -j$(nproc)
                  make prefix=/opt/deepep/gdrcopy CUDA=/usr/local/cuda install
                  cd packages
                  CUDA=/usr/local/cuda ./build-deb-packages.sh
                  dpkg -i *.deb
                  echo "gdrcopy installation complete"
                  # cleanup
                  pushd ~
                  rm -rf /tmp/gdrcopy
                  # run gdrcopy_copybw to verify
                  # /opt/deepep/gdrcopy/bin/gdrcopy_copybw
                else
                  echo "gdrcopy is already installed"
                fi

                # SGLang need this
                if ! grep -q "PeerMappingOverride=1" "/etc/modprobe.d/nvidia-graphics-drivers-kms.conf" 2>/dev/null; then
                  echo 'options nvidia NVreg_EnableStreamMemOPs=1 NVreg_RegistryDwords="PeerMappingOverride=1;"' | \
                      sudo tee -a /etc/modprobe.d/nvidia-graphics-drivers-kms.conf
                else
                  cat /etc/modprobe.d/nvidia-graphics-drivers-kms.conf
                fi
              fi

              # Load nvpeermem
              if [ "$USE_NVPEERMEM" == "1" ]; then
                echo "USE_NVPEERMEM=1, loading nvidia_peermem..."
                modprobe nvidia_peermem || echo "Warning: Failed to load nvidia_peermem module, continuing anyway..."
                if lsmod | grep -q nvidia_peermem; then
                    echo "nvidia_peermem loaded successfully"
                else
                    echo "Warning: nvidia_peermem module not loaded, but continuing installation..."
                fi
              fi
              
              # Reboot the node
              REBOOT_NEEDED=false

              # 检查是否需要重启的条件
              if ! lsmod | grep -q nvidia; then
                  REBOOT_NEEDED=true
                  echo "Rebooting the node due to nvidia module not loaded..."
              elif [ "$USE_NVPEERMEM" == "1" ] && ! lsmod | grep -q nvidia_peermem; then
                  REBOOT_NEEDED=true
                  echo "Rebooting the node due to nvidia_peermem module not loaded..."
              elif [ "$NVSHMEM_USE_GDRCOPY" != "1" ] && ! grep -q "PeerMappingOverride=1" /proc/driver/nvidia/params 2>/dev/null; then
                  REBOOT_NEEDED=true
                  echo "Rebooting the node due to PeerMappingOverride not set..."
              elif [ "$NVSHMEM_USE_GDRCOPY" == "1" ] && ! lsmod | grep -q gdrdrv; then
                  REBOOT_NEEDED=true
                  echo "Rebooting the node due to gdrdrv module not loaded..."
              fi

              if [ "$REBOOT_NEEDED" == "true" ]; then
                  echo "Rebooting the node to load driver and modules..."
                  reboot
              else
                  echo "Modules are loaded correctly"
              fi

              # Build nvshmem
              NVSHMEM_INFO_CMD="${NVSHMEM_HOME}/bin/nvshmem-info"
              if [ ! -x "$NVSHMEM_INFO_CMD" ]; then
                echo "NVSHMEM not found, installing now..."
                # Install dependencies
                apt-get install -y -qq --no-install-recommends \
                    python3-venv python3-pip ninja-build cmake \
                    python3.12-dev python3.12 \
                    build-essential devscripts debhelper dkms git
                # Build nvshmem
                BUILD_DIR="/tmp/nvshmem_build_src"
                rm -rf "$BUILD_DIR"
                mkdir -p "$BUILD_DIR"
                wget https://developer.nvidia.com/downloads/assets/secure/nvshmem/nvshmem_src_3.2.5-1.txz -O "${BUILD_DIR}/nvshmem_src_cuda12-all.tar.gz"
                tar -xvf "${BUILD_DIR}/nvshmem_src_cuda12-all.tar.gz" -C "$BUILD_DIR"
                cd "${BUILD_DIR}/nvshmem_src"

                CUDA_HOME="$CUDA_HOME" GDRCOPY_HOME="$GDRCOPY_HOME" \
                NVSHMEM_SHMEM_SUPPORT=0 NVSHMEM_UCX_SUPPORT=0 NVSHMEM_USE_NCCL=0 \
                NVSHMEM_MPI_SUPPORT=0 NVSHMEM_PMIX_SUPPORT=0 NVSHMEM_TIMEOUT_DEVICE_POLLING=0 \
                NVSHMEM_USE_GDRCOPY="$NVSHMEM_USE_GDRCOPY" \
                NVSHMEM_IBGDA_SUPPORT="$NVSHMEM_IBGDA_SUPPORT" \
                cmake -GNinja -S . -B build/ \
                    -DCMAKE_INSTALL_PREFIX="$NVSHMEM_HOME" \
                    -DCMAKE_CUDA_ARCHITECTURES=100
                cmake --build build/ --target install
                echo "NVSHMEM installation complete."

                # cleanup
                pushd ~
                #rm -rf "$BUILD_DIR"
               else
                echo "NVSHMEM is already installed"
              fi

              echo "Verifying NVSHMEM configuration..."
              if [ -x "$NVSHMEM_INFO_CMD" ]; then
                  if "$NVSHMEM_INFO_CMD" -a | grep -q "NVSHMEM_IBGDA_SUPPORT=ON"; then
                      echo "NVSHMEM_IBGDA_SUPPORT is enabled correctly."
                  else
                      echo "NVSHMEM_IBGDA_SUPPORT is not enabled, exiting..."
                      exit 1
                  fi
              else
                  echo "nvshmem-info command not found after installation, exiting..."
                  exit 1
              fi

              # Build DeepEP
              if ! python3 -c "import deep_ep" &> /dev/null; then
                  echo "deepep not found, installing now..."
                  # Install pytorch
                  pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129 --break-system-packages

                  BUILD_DIR="/tmp/deepep_build"
                  rm -rf "$BUILD_DIR"
                  git clone https://github.com/deepseek-ai/DeepEP.git "$BUILD_DIR" && cd "$BUILD_DIR"

                  export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:/opt/deepep/nvshmem/lib:/opt/deepep/gdrcopy/lib:$LD_LIBRARY_PATH
                  TORCH_CUDA_ARCH_LIST=$TORCH_CUDA_ARCH_LIST_B200 NVSHMEM_DIR=/opt/deepep/nvshmem python3 setup.py build
                  ln -s build/lib.linux-x86_64-cpython-312/deep_ep_cpp.cpython-312-x86_64-linux-gnu.so
                  TORCH_CUDA_ARCH_LIST=$TORCH_CUDA_ARCH_LIST_B200 NVSHMEM_DIR=/opt/deepep/nvshmem python3 setup.py install
                  # check installation
                  if ! python3 -c "import deep_ep" &> /dev/null; then
                      echo "Failed to install DeepEP, exiting..."
                      exit 1
                  else
                    echo "DeepEP installation complete."
                  fi
              else
                  echo "deepep is already installed"
              fi

              echo "copying binaries to GKE GPU driver path..."
              mkdir -p /home/kubernetes/bin/nvidia/lib64
              mkdir -p /home/kubernetes/bin/nvidia/bin
              mkdir -p /home/kubernetes/bin/nvidia/deepep
              # GPU driver
              cp -r /lib/firmware/nvidia/ /home/kubernetes/bin/nvidia/firmware
              cp -r /lib/modules/$(uname -r)/updates/dkms /home/kubernetes/bin/nvidia/drivers
              cp -r /usr/lib/x86_64-linux-gnu/libnvidia* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/libcuda* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/libEGL* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/libGL* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/libnvcuvid* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/libnvoptix* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/pkgconfig /home/kubernetes/bin/nvidia/lib64/pkgconfig
              cp -r /usr/lib/x86_64-linux-gnu/vdpau /home/kubernetes/bin/nvidia/lib64/vdpau
              # ib, rdma, gdrapi, nvshmem
              cp -r /usr/lib/x86_64-linux-gnu/libib* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/libnl* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/librdma* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/libgdrapi* /home/kubernetes/bin/nvidia/lib64
              cp -r /usr/lib/x86_64-linux-gnu/libmlx5* /home/kubernetes/bin/nvidia/lib64
              cp -r /opt/deepep/nvshmem/lib/* /home/kubernetes/bin/nvidia/lib64
              # deepep
              cp -r /tmp/deepep_build/build/lib.linux-x86_64-cpython-312/* /home/kubernetes/bin/nvidia/deepep
              # bin
              sudo cp -r /usr/bin/nvidia* /home/kubernetes/bin/nvidia/bin

              echo "installation complete, exiting..."
              sleep infinity

      containers:
        - image: "gke.gcr.io/pause:3.8@sha256:880e63f94b145e46f1b1082bb71b85e21f16b99b180b9996407d61240ceb9830"
          name: pause