# Wan æ¨¡å‹ TPU è¿ç§»ä¸ä¼˜åŒ–å®Œå…¨æŒ‡å—

> **ç‰ˆæœ¬**: 3.0 | **æ›´æ–°æ—¥æœŸ**: 2024å¹´12æœˆ
> 
> æœ¬æ–‡æ¡£æ±‡é›†äº† Wan 2.1/2.2 æ¨¡å‹åœ¨ Google Cloud TPU v6e ä¸Šçš„è¿ç§»ä¸ä¼˜åŒ–å…¨éƒ¨ç²¾åï¼ŒåŒ…å«è¯¦ç»†çš„ç¡¬ä»¶æ¶æ„åˆ†æã€åˆ†ç‰‡ç­–ç•¥ã€Splash Attention å†…æ ¸ä¼˜åŒ–ã€VAE ä¼˜åŒ–æŠ€æœ¯ä»¥åŠå®Œæ•´çš„å®ç°ä»£ç ã€‚

---

## ç›®å½•

- [ç¬¬ä¸€ç« ï¼šTPU v6e ç¡¬ä»¶æ¶æ„ä¸æ€§èƒ½ç‰¹æ€§](#ç¬¬ä¸€ç« tpu-v6e-ç¡¬ä»¶æ¶æ„ä¸æ€§èƒ½ç‰¹æ€§)
- [ç¬¬äºŒç« ï¼šWan æ¨¡å‹æ¶æ„æ·±åº¦è§£æ](#ç¬¬äºŒç« wan-æ¨¡å‹æ¶æ„æ·±åº¦è§£æ)
- [ç¬¬ä¸‰ç« ï¼šåˆ†ç‰‡ç­–ç•¥è¯¦è§£](#ç¬¬ä¸‰ç« åˆ†ç‰‡ç­–ç•¥è¯¦è§£)
- [ç¬¬å››ç« ï¼šSplash Attention å†…æ ¸ä¼˜åŒ–](#ç¬¬å››ç« splash-attention-å†…æ ¸ä¼˜åŒ–)
- [ç¬¬äº”ç« ï¼šVAE åœ¨ Torchax ä¸Šçš„å·¥ä½œåŸç†ä¸å¹¶è¡Œè®¾è®¡](#ç¬¬äº”ç« vae-åœ¨-torchax-ä¸Šçš„å·¥ä½œåŸç†ä¸å¹¶è¡Œè®¾è®¡)
- [ç¬¬å…­ç« ï¼šæ€§èƒ½åˆ†ææ–¹æ³•è®º](#ç¬¬å…­ç« æ€§èƒ½åˆ†ææ–¹æ³•è®º)
- [ç¬¬ä¸ƒç« ï¼šTorchax æ¡¥æ¥ä¸ä»£ç å®ç°](#ç¬¬ä¸ƒç« torchax-æ¡¥æ¥ä¸ä»£ç å®ç°)
- [ç¬¬å…«ç« ï¼šå®Œæ•´ä»£ç ç¤ºä¾‹ä¸å®æˆ˜](#ç¬¬å…«ç« å®Œæ•´ä»£ç ç¤ºä¾‹ä¸å®æˆ˜)
- [ç¬¬ä¹ç« ï¼šImage-to-Video ä¸“é¡¹ä¼˜åŒ–](#ç¬¬ä¹ç« image-to-video-ä¸“é¡¹ä¼˜åŒ–)
- [ç¬¬åç« ï¼šè°ƒè¯•ä¸æ•…éšœæ’é™¤](#ç¬¬åç« è°ƒè¯•ä¸æ•…éšœæ’é™¤)
- [é™„å½•](#é™„å½•)

---

## ç¬¬ä¸€ç« ï¼šTPU v6e ç¡¬ä»¶æ¶æ„ä¸æ€§èƒ½ç‰¹æ€§

### 1.1 TPU v6e æ ¸å¿ƒè§„æ ¼

TPU v6e (ä»£å· Trillium) æ˜¯ Google Cloud æœ€æ–°ä¸€ä»£å¼ é‡å¤„ç†å•å…ƒï¼Œä¸“ä¸ºå¤§è§„æ¨¡æœºå™¨å­¦ä¹ è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–ã€‚

| è§„æ ¼é¡¹ | TPU v6e å•èŠ¯ç‰‡ | TPU v6e-8 | TPU v6e-16 |
|--------|---------------|-----------|------------|
| **å³°å€¼ bf16 TFLOPs** | 918 | 7,344 | 14,688 |
| **HBM å®¹é‡** | 32 GB | 256 GB | 512 GB |
| **HBM å¸¦å®½** | 1,638 GB/s | 13,104 GB/s | 26,208 GB/s |
| **èŠ¯ç‰‡é—´äº’è”** | ICI 3.0 | ç¯å½¢æ‹“æ‰‘ | 2D Torus |
| **MXU è§„æ ¼** | 256Ã—256 | - | - |

```mermaid
graph TB
    subgraph "TPU v6e èŠ¯ç‰‡æ¶æ„"
        HBM[HBM3 32GB<br/>1638 GB/s]
        
        subgraph "TensorCore"
            MXU[MXU 256x256<br/>çŸ©é˜µä¹˜æ³•å•å…ƒ]
            VPU[VPU<br/>å‘é‡å¤„ç†å•å…ƒ]
            SFU[SFU<br/>ç‰¹æ®Šå‡½æ•°å•å…ƒ]
        end
        
        subgraph "å†…å­˜å±‚çº§"
            VMEM[VMEM<br/>å‘é‡å†…å­˜]
            SMEM[SMEM<br/>æ ‡é‡å†…å­˜]
            REG[å¯„å­˜å™¨æ–‡ä»¶]
        end
        
        ICI[ICI 3.0<br/>èŠ¯ç‰‡é—´äº’è”]
    end
    
    HBM --> VMEM
    VMEM --> MXU
    VMEM --> VPU
    VPU --> SFU
    MXU --> REG
    VPU --> REG
    
    style MXU fill:#e1f5fe
    style HBM fill:#fff3e0
    style VPU fill:#f3e5f5
```

### 1.2 è®¡ç®—å•å…ƒæ¶æ„

#### MXU (Matrix Multiply Unit)
- **è§„æ ¼**: 256Ã—256 è„‰åŠ¨é˜µåˆ—
- **æ•°æ®ç±»å‹**: bf16, int8
- **å³°å€¼æ€§èƒ½**: 918 TFLOPs (bf16)
- **å…³é”®é™åˆ¶**: å½“ K ç»´åº¦ < 256 æ—¶ï¼ŒMXU åˆ©ç”¨ç‡ä¸‹é™

```python
# MXU åˆ©ç”¨ç‡è®¡ç®—ç¤ºä¾‹
mxu_size = 256
head_dim = 128  # Wan æ¨¡å‹çš„ head dimension

# K ç»´åº¦ = head_dim = 128ï¼Œåªå ç”¨ MXU ä¸€åŠ
mxu_utilization = head_dim / mxu_size  # = 0.5 = 50%
```

#### VPU (Vector Processing Unit)
- **åŠŸèƒ½**: å‘é‡è¿ç®—ï¼ˆsoftmaxã€layernormã€æ¿€æ´»å‡½æ•°ç­‰ï¼‰
- **ç‰¹ç‚¹**:
  - `exp2` æ¯” `exp` æ›´é«˜æ•ˆï¼ˆåŸç”Ÿç¡¬ä»¶æŒ‡ä»¤ï¼‰
  - æ˜¯ attention softmax çš„ä¸»è¦æ‰§è¡Œå•å…ƒ

### 1.3 å†…å­˜å±‚çº§ä¸å¸¦å®½

```mermaid
graph LR
    subgraph "å†…å­˜å±‚çº§é‡‘å­—å¡”"
        REG[å¯„å­˜å™¨<br/>æœ€å¿«/æœ€å°]
        VMEM[VMEM<br/>å‘é‡å†…å­˜]
        SMEM[SMEM<br/>æ ‡é‡å†…å­˜]
        HBM[HBM<br/>32GB/1638GB/s]
    end
    
    REG --> VMEM --> SMEM --> HBM
    
    style REG fill:#4caf50
    style VMEM fill:#8bc34a
    style HBM fill:#ffeb3b
```

**å…³é”®å¸¦å®½æ•°æ®**:
- HBM å¸¦å®½: 1,638 GB/s
- ç®—æœ¯å¼ºåº¦é˜ˆå€¼: 918 TFLOPs Ã· 1,638 GB/s â‰ˆ **560 FLOPs/Byte**

### 1.4 Roofline æ€§èƒ½æ¨¡å‹

```
æ€§èƒ½ = min(å³°å€¼ç®—åŠ›, ç®—æœ¯å¼ºåº¦ Ã— å†…å­˜å¸¦å®½)
```

**Self-Attention çš„ Roofline åˆ†æ**:

```python
# å¯¹äº Wan 720P: S = 75,600
arithmetic_intensity = 75600 / 2  # = 37,800 FLOPs/Byte

# è¿œå¤§äº 560 çš„é˜ˆå€¼ï¼Œç†è®ºä¸Šåº”è¯¥æ˜¯ compute-bound
# ä½†å®é™… MFU åªæœ‰ 37%ï¼ŒåŸå› åœ¨äº MXU åˆ©ç”¨ç‡ (head_dim=128 < 256)
```

---

## ç¬¬äºŒç« ï¼šWan æ¨¡å‹æ¶æ„æ·±åº¦è§£æ

### 2.1 Wan 2.1 T2V 14B æ¨¡å‹ç»“æ„

```mermaid
graph TB
    subgraph "Wan 2.1 Pipeline"
        TEXT[Text Prompt]
        T5[UMT5 Text Encoder<br/>4096 dims]
        
        NOISE[Random Noise<br/>Latent Space]
        DIT[DiT Transformer<br/>14B Parameters<br/>40 Blocks]
        
        LATENT[Denoised Latents<br/>16 channels]
        VAE[WanVAE Decoder<br/>3D Causal Conv]
        VIDEO[Output Video<br/>720P 81 frames]
    end
    
    TEXT --> T5
    T5 --> DIT
    NOISE --> DIT
    DIT --> LATENT
    LATENT --> VAE
    VAE --> VIDEO
    
    style DIT fill:#e3f2fd
    style VAE fill:#f3e5f5
```

**æ¨¡å‹è§„æ ¼**:

| ç»„ä»¶ | è§„æ ¼ |
|------|------|
| Text Encoder | UMT5-XXL, 4096 hidden dims |
| DiT Blocks | 40 layers |
| Hidden Dimension | 5120 |
| Attention Heads | 40 (128 dims each) |
| FFN Dimension | 13824 (SwiGLU) |
| VAE Latent Channels | 16 |
| Temporal Compression | 4x |
| Spatial Compression | 8x |

### 2.2 DiT Transformer æ¶æ„

```mermaid
graph TB
    subgraph "DiT Block x 40"
        INPUT[Hidden States<br/>B, S, 5120]
        
        subgraph "Self-Attention"
            NORM1[RMSNorm]
            QKV[Q, K, V Projection<br/>5120 â†’ 5120 x 3]
            ATTN[Multi-Head Attention<br/>40 heads, 128 dims]
            OUT1[Output Projection<br/>5120 â†’ 5120]
        end
        
        subgraph "Cross-Attention"
            NORM2[RMSNorm]
            Q2[Q Projection]
            KV2[K, V from Text<br/>4096 â†’ 5120]
            XATTN[Cross Attention]
            OUT2[Output Projection]
        end
        
        subgraph "FFN"
            NORM3[RMSNorm]
            FFN1[Gate + Up<br/>5120 â†’ 27648]
            SILU[SiLU Activation]
            FFN2[Down<br/>13824 â†’ 5120]
        end
        
        OUTPUT[Output]
    end
    
    INPUT --> NORM1 --> QKV --> ATTN --> OUT1
    OUT1 --> NORM2 --> Q2 --> XATTN --> OUT2
    NORM2 --> KV2 --> XATTN
    OUT2 --> NORM3 --> FFN1 --> SILU --> FFN2 --> OUTPUT
```

**Self-Attention åºåˆ—é•¿åº¦è®¡ç®—**:

```python
# 720P 81å¸§è§†é¢‘çš„ latent åºåˆ—é•¿åº¦
height, width, frames = 720, 1280, 81

# VAE å‹ç¼©å
latent_h = height // 8   # = 90
latent_w = width // 8    # = 160
latent_t = (frames - 1) // 4 + 1  # = 21

# Transformer çš„ patch å¤§å°ä¸º 2
patch_h = latent_h // 2  # = 45
patch_w = latent_w // 2  # = 80

# åºåˆ—é•¿åº¦
seq_len = latent_t * patch_h * patch_w
# = 21 * 45 * 80 = 75,600
```

### 2.3 VAE ç¼–è§£ç å™¨

```mermaid
graph LR
    subgraph "WanVAE Encoder"
        VIN[Video Input<br/>B,3,T,H,W]
        CONV_IN[Conv3D In<br/>3â†’96]
        DOWN1[DownBlock 1<br/>96â†’192]
        DOWN2[DownBlock 2<br/>192â†’384<br/>Temporal â†“2]
        DOWN3[DownBlock 3<br/>384â†’384<br/>Temporal â†“2]
        MID_E[MidBlock]
        CONV_OUT[Conv3D Out<br/>384â†’32]
        LATENT[Latent<br/>B,16,T/4,H/8,W/8]
    end
    
    VIN --> CONV_IN --> DOWN1 --> DOWN2 --> DOWN3 --> MID_E --> CONV_OUT --> LATENT
```

---

## ç¬¬ä¸‰ç« ï¼šåˆ†ç‰‡ç­–ç•¥è¯¦è§£

### 3.1 FSDP (Fully Sharded Data Parallelism)

```mermaid
graph TB
    subgraph "FSDP æƒé‡åˆ†ç‰‡"
        W[å®Œæ•´æƒé‡ W<br/>shape: 5120x5120]
        
        subgraph "è®¾å¤‡ 0-7"
            W0[W_shard_0<br/>5120x640]
            W1[W_shard_1<br/>5120x640]
            W7[W_shard_7<br/>5120x640]
        end
        
        AG[AllGather<br/>è®¡ç®—å‰æ”¶é›†]
        RS[ReduceScatter<br/>è®¡ç®—ååˆ†æ•£]
    end
    
    W --> W0 & W1 & W7
    W0 & W1 & W7 --> AG --> RS
```

**FSDP åˆ†ç‰‡è§„åˆ™**:

```python
transformer_shardings_fsdp = {
    # Self-Attention æƒé‡ (attn1)
    r'blocks.\d+.attn1.to_q.weight': (None, ('tp', 'sp')),  # åˆ—å¹¶è¡Œ
    r'blocks.\d+.attn1.to_k.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn1.to_v.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn1.to_out.0.weight': (('tp', 'sp'), None),  # è¡Œå¹¶è¡Œ
    
    # Cross-Attention æƒé‡ (attn2)
    r'blocks.\d+.attn2.to_q.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn2.to_k.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn2.to_v.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.attn2.to_out.0.weight': (('tp', 'sp'), None),
    
    # FFN æƒé‡
    r'blocks.\d+.ffn.net.0.proj.weight': (None, ('tp', 'sp')),
    r'blocks.\d+.ffn.net.2.weight': (('tp', 'sp'), None),
}
```

### 3.2 Context Parallelism (CP)

åœ¨ **head number** ç»´åº¦è¿›è¡Œåˆ†ç‰‡ï¼Œä¸“ç”¨äº Self-Attentionã€‚

```python
# Self-Attention åˆ†ç‰‡
q_partition_spec = P('dp', 'tp', 'sp', None)  # [batch, heads, seq, dim]
kv_partition_spec = P('dp', 'tp', None, None)  # K,V åœ¨ seq ç»´åº¦å¤åˆ¶

# 40 heads / 8 devices = 5 heads per device
```

### 3.3 Sequence Parallelism (SP)

åœ¨ **sequence** ç»´åº¦è¿›è¡Œåˆ†ç‰‡ï¼Œä¸“ç”¨äº Cross-Attentionã€‚

```python
# Cross-Attention åˆ†ç‰‡ (K,V åºåˆ—é•¿åº¦çŸ­ï¼Œä¸åˆ†ç‰‡)
q_partition_spec = P('dp', None, ('tp', 'sp'), None)  # Q åœ¨ seq ç»´åº¦åˆ†ç‰‡
kv_partition_spec = P('dp', None, None, None)          # K,V å®Œæ•´å¤åˆ¶
```

### 3.4 Data Parallelism (DP)

ç”¨äºå¤„ç† CFG çš„æ­£è´Ÿ promptã€‚

```python
# dp=2: æ­£è´Ÿ prompt å„ç”¨ä¸€åŠè®¾å¤‡
mesh_dims = (2, 1, 4)  # (dp, sp, tp)
mesh = Mesh(devices, ('dp', 'sp', 'tp'))
```

### 3.5 æ··åˆåˆ†ç‰‡ç­–ç•¥

```mermaid
graph TB
    subgraph "æ··åˆåˆ†ç‰‡ç­–ç•¥"
        TEXT[Text Encoder]
        DIT[DiT Transformer]
        VAE[VAE Decoder]
        
        subgraph "Text Encoder ç­–ç•¥"
            TE_FSDP[FSDP<br/>æƒé‡åˆ†ç‰‡]
        end
        
        subgraph "DiT Transformer ç­–ç•¥"
            DIT_FSDP[FSDP æƒé‡]
            DIT_CP[CP Self-Attn]
            DIT_SP[SP Cross-Attn]
            DIT_DP[DP CFG]
        end
        
        subgraph "VAE ç­–ç•¥"
            VAE_REP[å¤åˆ¶æƒé‡]
            VAE_SPATIAL[Spatial æ¿€æ´»åˆ†ç‰‡]
        end
    end
    
    TEXT --> TE_FSDP
    DIT --> DIT_FSDP & DIT_CP & DIT_SP & DIT_DP
    VAE --> VAE_REP & VAE_SPATIAL
```

**Mesh é…ç½®ä»£ç **:

```python
import jax
from jax.sharding import Mesh, PartitionSpec as P
from jax.experimental import mesh_utils

# 8 è®¾å¤‡é…ç½®: dp=2, sp=1, tp=4
tp_dim, dp_dim, sp_dim = len(jax.devices()), 1, 1

if use_dp:
    tp_dim //= 2
    dp_dim = 2

mesh_devices = mesh_utils.create_device_mesh(
    (dp_dim, sp_dim, tp_dim),
    allow_split_physical_axes=True
)
mesh = Mesh(mesh_devices, ('dp', 'sp', 'tp'))
```

---

## ç¬¬å››ç« ï¼šSplash Attention å†…æ ¸ä¼˜åŒ–

æœ¬ç« æ˜¯æŠ€æœ¯æ ¸å¿ƒï¼Œæˆ‘ä»¬å°†ä» Profiler åˆ†æåˆ°ä»£ç å®ç°ï¼Œå®Œæ•´è®²è§£å¦‚ä½•å‘ç°ä¼˜åŒ–ç‚¹ã€å¦‚ä½•å®ç°ä¼˜åŒ–ã€ä»¥åŠä¼˜åŒ–çš„æ•ˆæœéªŒè¯ã€‚

### 4.1 ä¼˜åŒ–çš„å‘ç°è¿‡ç¨‹ï¼šä» Profiler åˆ°ä¼˜åŒ–ç‚¹

#### 4.1.1 åˆå§‹æ€§èƒ½åŸºçº¿

è¿è¡Œ JAX Profiler åï¼Œæˆ‘ä»¬å‘ç° Self-Attention å æ®äº† **66.8%** çš„ DiT step æ—¶é—´ï¼Œè€Œ MFU (Model FLOPs Utilization) ä»…æœ‰ **37%**ã€‚

```python
# ä½¿ç”¨ JAX Profiler æ”¶é›†æ€§èƒ½æ•°æ®
with jax.profiler.trace("/dev/shm/tensorboard"):
    output = pipe(prompt=prompt, num_inference_steps=3)
    jax.effects_barrier()
```

**Profiler åˆ†æç»“æœ**ï¼š

| æ“ä½œ | æ—¶é—´å æ¯” | MFU | é—®é¢˜ |
|------|----------|-----|------|
| Self-Attention Softmax | 28.3% | 12% | VPU bound |
| Self-Attention QK Matmul | 24.2% | 48% | MXU 50% åˆ©ç”¨ç‡ |
| Self-Attention AV Matmul | 14.3% | 52% | MXU 50% åˆ©ç”¨ç‡ |
| Linear (FFNç­‰) | 33.2% | 66% | æ¥è¿‘ç†æƒ³ |

#### 4.1.2 å‘ç°ä¸‰ä¸ªå…³é”®ä¼˜åŒ–ç‚¹

**ä¼˜åŒ–ç‚¹ 1ï¼šMXU åˆ©ç”¨ç‡ä½ (50%)**

Wan æ¨¡å‹çš„ `head_dim = 128`ï¼Œè€Œ MXU æ˜¯ 256Ã—256 çš„è„‰åŠ¨é˜µåˆ—ã€‚å½“ K ç»´åº¦ = 128 æ—¶ï¼ŒMXU åªèƒ½ç”¨ä¸€åŠã€‚

```
çŸ©é˜µä¹˜æ³• C[M,N] = A[M,K] @ B[K,N]
å¯¹äº QK ä¹˜æ³•: Q[seq, 128] @ K^T[128, seq]
K = head_dim = 128 < 256ï¼ŒMXU åˆ©ç”¨ç‡ = 128/256 = 50%
```

**ä¼˜åŒ–ç‚¹ 2ï¼šVPU ä¸Šçš„ exp æ“ä½œå¾ˆæ…¢**

Softmax ä¸­çš„ `exp` æ“ä½œåœ¨ VPU ä¸Šæ‰§è¡Œï¼Œéœ€è¦è°ƒç”¨ SFU (Special Function Unit)ï¼Œå»¶è¿Ÿè¾ƒé«˜ã€‚

```python
# æ ‡å‡† softmax çš„ exp æ“ä½œè·¯å¾„
# VPU -> SFU -> VPUï¼Œéœ€è¦å¤šæ¬¡è®¿é—®
softmax = exp(x - max) / sum(exp(x - max))
```

**ä¼˜åŒ–ç‚¹ 3ï¼šQK çŸ©é˜µä¹˜é¡ºåºä¸ä¼˜**

æ ‡å‡†å®ç°æ˜¯ `Q @ K^T`ï¼Œä½† TPU æ›´å–œæ¬¢ `K^T @ Q` çš„å†…å­˜è®¿é—®æ¨¡å¼ã€‚

#### 4.1.3 ä¼˜åŒ–ç­–ç•¥åˆ¶å®š

```mermaid
graph TB
    subgraph "å‘ç° â†’ åˆ†æ â†’ ä¼˜åŒ–"
        P1[Profiler åˆ†æ<br/>MFU 37%]
        
        A1[é—®é¢˜1: head_dim=128<br/>MXU åˆ©ç”¨ç‡ 50%]
        A2[é—®é¢˜2: exp è°ƒç”¨ SFU<br/>VPU ç“¶é¢ˆ]
        A3[é—®é¢˜3: Q@K^T é¡ºåº<br/>å†…å­˜ä¸å‹å¥½]
        
        O1[exp2 æ›¿ä»£ exp<br/>VPU åŸç”ŸæŒ‡ä»¤]
        O2[QK Transpose<br/>K^T @ Q]
        O3[LP LLO è°ƒåº¦<br/>VPU/MXU é‡å ]
        
        R[MFU 37% â†’ æ¥è¿‘ç†è®ºä¸Šé™]
    end
    
    P1 --> A1 & A2 & A3
    A1 --> O1
    A2 --> O1
    A3 --> O2
    O1 & O2 --> O3
    O3 --> R
    
    style O1 fill:#4caf50
    style O3 fill:#2196f3
```

### 4.2 ä¼˜åŒ– 1ï¼šexp2 æ›¿ä»£ expï¼ˆVPU åŸç”ŸæŒ‡ä»¤ä¼˜åŒ–ï¼‰

#### 4.2.1 æ•°å­¦ç­‰ä»·å˜æ¢

TPU çš„ VPU æœ‰ä¸“é—¨çš„ `exp2` ç¡¬ä»¶æŒ‡ä»¤ï¼Œæ¯” `exp` å¿«å¾—å¤šã€‚æˆ‘ä»¬åˆ©ç”¨æ’ç­‰å¼ï¼š

```
exp(x) = 2^(x * log2(e)) = exp2(x * log2(e))
```

å…¶ä¸­ `log2(e) â‰ˆ 1.44269504`

#### 4.2.2 åœ¨ Attention ä¸­çš„åº”ç”¨

å¯¹äº Attentionï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ï¼š
```
softmax(scale * Q @ K^T) = exp(scale * QK - max) / sum(exp(scale * QK - max))
```

æ”¹å†™ä¸º exp2ï¼š
```
= exp2((scale * QK - max) * log2(e)) / sum(exp2(...))
= exp2(scale * log2(e) * QK - max * log2(e)) / sum(...)
```

**å…³é”®ä¼˜åŒ–**ï¼šé¢„å…ˆå°† `scale * log2(e)` èåˆåˆ° Q ä¸­ï¼

```python
# generate_flax.py ä¸­çš„å®ç°
def _attention_on_slices(q, k, v):
    scale_factor = 1.0 / math.sqrt(q.shape[-1]) if scale is None else scale
    # å…³é”®ï¼šé¢„ä¹˜ log2(e)ï¼Œä¹‹åå¯ä»¥ç›´æ¥ç”¨ exp2
    _LOG2_E = 1.44269504
    q = q * scale_factor * _LOG2_E  # èåˆ scale å’Œ log2(e) åˆ° Q
    # ...
```

#### 4.2.3 Kernel å†…éƒ¨çš„ exp2 ä½¿ç”¨

åœ¨ `custom_splash_attention.py` ä¸­ï¼š

```python
def _flash_attention_kernel(...):
    # ...
    for i in range(0, qk.shape[0], step):
        m_curr = qk[i:i+step].max(axis=0)[None, :]
        m_next = jnp.maximum(m_prev, m_curr)
        
        # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨ exp2
        # å› ä¸º Q å·²ç»é¢„ä¹˜äº† log2(e)ï¼Œæ‰€ä»¥ qk å·²ç»æ˜¯ log2 scale
        s_curr = jnp.exp2(qk[i:i+step] - m_next[0:1])
        
        # æ›´æ–° running sum (ä¹Ÿç”¨ exp2)
        alpha = jnp.exp2(m_prev - m_next)
        l_next = l_curr + alpha * l_prev
        # ...
```

**ä»£ç è§£é‡Š**ï¼š
1. `qk` å·²ç»æ˜¯ `Q_scaled @ K^T` çš„ç»“æœï¼Œå…¶ä¸­ `Q_scaled = Q * scale * log2(e)`
2. æ‰€ä»¥ `qk - max` å¯ä»¥ç›´æ¥ç”¨ `exp2` è®¡ç®—
3. åŒæ ·ï¼Œ`alpha = exp2(m_prev - m_next)` ä¹Ÿæ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸º max å€¼å·²ç»åœ¨ log2 scale

### 4.3 ä¼˜åŒ– 2ï¼šQK Transposeï¼ˆçŸ©é˜µä¹˜æ³•é¡ºåºä¼˜åŒ–ï¼‰

#### 4.3.1 é—®é¢˜åˆ†æ

æ ‡å‡† Attention è®¡ç®— `Q @ K^T`ï¼š
- Q: [batch, heads, seq_q, head_dim]
- K: [batch, heads, seq_k, head_dim]
- éœ€è¦å…ˆ transpose Kï¼Œç„¶ååšçŸ©é˜µä¹˜

ä½† TPU çš„ `lax.dot_general` æ›´é«˜æ•ˆåœ°å¤„ç† "N^T @ N" å½¢å¼çš„ä¹˜æ³•ã€‚

#### 4.3.2 Transpose å‰åå¯¹æ¯”

```python
# æ–¹å¼ 1: Q @ K^T (æ ‡å‡†)
# Q: [seq_q, head_dim] @ K^T: [head_dim, seq_k]
# ç»“æœ: [seq_q, seq_k]
NN_DIM_NUMBERS = (((1,), (0,)), ((), ()))  # Q çš„ dim1 å’Œ K çš„ dim0 æ”¶ç¼©
qk = lax.dot_general(q, k_transposed, NN_DIM_NUMBERS)

# æ–¹å¼ 2: K^T @ Q (ä¼˜åŒ–)  
# K: [seq_k, head_dim] @ Q: [seq_q, head_dim]
# æ”¶ç¼© head_dim ç»´åº¦ï¼ˆä¸¤è€…çš„ dim1ï¼‰
NT_DIM_NUMBERS = (((1,), (1,)), ((), ()))  # K çš„ dim1 å’Œ Q çš„ dim1 æ”¶ç¼©
qk = lax.dot_general(k, q, NT_DIM_NUMBERS)
# ç»“æœ: [seq_k, seq_q]ï¼Œéœ€è¦åç»­è€ƒè™‘è¿™ä¸ª transpose
```

#### 4.3.3 ä»£ç å®ç°

```python
# custom_splash_attention.py ç¬¬ 31 è¡Œ
NT_DIM_NUMBERS = (((1,), (1,)), ((), ()))

def _flash_attention_kernel(...):
    # ...
    q = q_ref[...]
    k = k_ref[slice_k, :]
    
    # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šK @ Q è€Œä¸æ˜¯ Q @ K^T
    qk = lax.dot_general(k, q, NT_DIM_NUMBERS, preferred_element_type=float32)
    # qk çš„å½¢çŠ¶æ˜¯ [block_kv, block_q]ï¼Œä¸æ˜¯ [block_q, block_kv]
    # åç»­ä»£ç éœ€è¦é€‚é…è¿™ä¸ª transpose
```

**ä¸ºä»€ä¹ˆè¿™æ ·æ›´å¿«ï¼Ÿ**
- å‡å°‘äº†æ˜¾å¼çš„ transpose æ“ä½œ
- æ›´å¥½çš„å†…å­˜è®¿é—®æ¨¡å¼ï¼ˆK å’Œ Q éƒ½æŒ‰è¡Œè¯»å–ï¼‰
- TPU çš„ MXU å¯¹è¿™ç§æ¨¡å¼æœ‰ç¡¬ä»¶ä¼˜åŒ–

### 4.4 ä¼˜åŒ– 3ï¼šLP LLO è°ƒåº¦ï¼ˆVPU/MXU é‡å æ‰§è¡Œï¼‰

#### 4.4.1 ä»€ä¹ˆæ˜¯ LP LLO Scheduler

LP LLO (Low-Precision Low-Level Optimizer) Scheduler æ˜¯ XLA ç¼–è¯‘å™¨çš„ä¸€ä¸ªè°ƒåº¦ç­–ç•¥ï¼Œèƒ½è®© VPU å’Œ MXU çš„æ“ä½œé‡å æ‰§è¡Œã€‚

```python
# custom_splash_attention.py ç¬¬ 212-215 è¡Œ
compiler_params = pltpu.CompilerParams(
    dimension_semantics=("parallel", "arbitrary", "arbitrary"),
    flags={"XLA_TPU_FORCE_LP_LLO_SCHEDULER": True}
)
```

#### 4.4.2 é‡å æ‰§è¡Œçš„åŸç†

```mermaid
sequenceDiagram
    participant MXU
    participant VPU
    
    Note over MXU,VPU: æ— ä¼˜åŒ–ï¼ˆä¸²è¡Œæ‰§è¡Œï¼‰
    MXU->>MXU: QK çŸ©é˜µä¹˜
    VPU->>VPU: Softmax
    MXU->>MXU: AV çŸ©é˜µä¹˜
    
    Note over MXU,VPU: LP LLO ä¼˜åŒ–ï¼ˆé‡å æ‰§è¡Œï¼‰
    par å¹¶è¡Œæ‰§è¡Œ
        MXU->>MXU: QK çŸ©é˜µä¹˜ (block i)
        VPU->>VPU: Softmax (block i-1)
    end
    par å¹¶è¡Œæ‰§è¡Œ
        MXU->>MXU: AV çŸ©é˜µä¹˜ (block i-1)
        VPU->>VPU: Softmax (block i)
    end
```

#### 4.4.3 ä¸ºä»€ä¹ˆéœ€è¦ç‰¹å®šçš„ dimension_semantics

```python
dimension_semantics=("parallel", "arbitrary", "arbitrary")
# ç¬¬ä¸€ç»´ (heads): parallel - å®Œå…¨ç‹¬ç«‹ï¼Œå¯ä»¥å¹¶è¡Œ
# ç¬¬äºŒç»´ (q_blocks): arbitrary - ç¼–è¯‘å™¨è‡ªç”±è°ƒåº¦
# ç¬¬ä¸‰ç»´ (kv_blocks): arbitrary - ç¼–è¯‘å™¨è‡ªç”±è°ƒåº¦
```

- `parallel`ï¼šå‘Šè¯‰ç¼–è¯‘å™¨è¯¥ç»´åº¦çš„è¿­ä»£å®Œå…¨ç‹¬ç«‹
- `arbitrary`ï¼šå…è®¸ç¼–è¯‘å™¨é‡æ–°æ’åºè¿­ä»£ï¼Œå®ç°æµæ°´çº¿ä¼˜åŒ–

### 4.5 å®Œæ•´çš„è‡ªå®šä¹‰ Splash Attention å†…æ ¸

ç°åœ¨è®©æˆ‘ä»¬çœ‹å®Œæ•´çš„ä¼˜åŒ–ä»£ç ï¼Œé€æ®µè§£é‡Šï¼š

```python
"""
custom_splash_attention.py - è‡ªå®šä¹‰ TPU Splash Attention

æ ¸å¿ƒä¼˜åŒ–ï¼š
1. exp2 æ›¿ä»£ expï¼ˆVPU åŸç”ŸæŒ‡ä»¤ï¼‰
2. K @ Q æ›¿ä»£ Q @ K^Tï¼ˆå‡å°‘ transposeï¼‰
3. LP LLO Schedulerï¼ˆVPU/MXU é‡å ï¼‰
"""

import functools
import jax
import jax.numpy as jnp
from jax import lax
from jax.experimental import pallas as pl
from jax.experimental.pallas import tpu as pltpu

# å¸¸é‡å®šä¹‰
DEFAULT_MASK_VALUE = -0.7 * float(jnp.finfo(jnp.float32).max)
NUM_SUBLANES = 8  # TPU çš„ sublane æ•°é‡ï¼Œç”¨äº l å’Œ m çš„å­˜å‚¨
NT_DIM_NUMBERS = (((1,), (1,)), ((), ()))  # K @ Q çš„ç»´åº¦è§„æ ¼


def _flash_attention_kernel(
    q_ref, k_ref, v_ref,
    m_scratch_ref, l_scratch_ref, o_scratch_ref, o_ref,
    *, mask_value, grid_width, bq, bkv, bkv_compute, bkv_compute_in, head_dim_v,
):
    """
    Flash Attention æ ¸å¿ƒ Kernel
    
    å‚æ•°:
        q_ref: Query å—å¼•ç”¨ï¼Œå½¢çŠ¶ [block_q, head_dim]
        k_ref: Key å—å¼•ç”¨ï¼Œå½¢çŠ¶ [block_kv, head_dim]
        v_ref: Value å—å¼•ç”¨ï¼Œå½¢çŠ¶ [block_kv, head_dim_v]
        m_scratch_ref: å­˜å‚¨ running max çš„ scratch memory
        l_scratch_ref: å­˜å‚¨ running sum çš„ scratch memory
        o_scratch_ref: å­˜å‚¨ç´¯ç§¯è¾“å‡ºçš„ scratch memory
        o_ref: æœ€ç»ˆè¾“å‡ºå¼•ç”¨
        
    å…³é”®å‚æ•°:
        bkv_compute: å†…éƒ¨è®¡ç®—çš„ KV å—å¤§å°
        bkv_compute_in: æ›´ç»†ç²’åº¦çš„å†…éƒ¨è¿­ä»£å—å¤§å°
    """
    float32 = jnp.float32
    head_dim_v_repeats = head_dim_v // NUM_SUBLANES
    
    # è·å–å½“å‰ç½‘æ ¼ä½ç½®
    h, i, j = pl.program_id(0), pl.program_id(1), pl.program_id(2)
    # h: head index, i: q block index, j: kv block index

    # ============ åˆå§‹åŒ– ============
    @pl.when(j == 0)
    def init():
        """ç¬¬ä¸€ä¸ª KV å—æ—¶åˆå§‹åŒ– scratch memory"""
        o_scratch_ref[...] = jnp.zeros_like(o_scratch_ref)
        m_scratch_ref[...] = jnp.full_like(m_scratch_ref, mask_value)  # åˆå§‹ max = -inf
        l_scratch_ref[...] = jnp.zeros_like(l_scratch_ref)  # åˆå§‹ sum = 0

    # ============ ä¸»è®¡ç®—å¾ªç¯ ============
    def body(kv_compute_index, _):
        """å¤„ç†ä¸€ä¸ª KV è®¡ç®—å—"""
        slice_k = pl.ds(kv_compute_index * bkv_compute, bkv_compute)
        m_prev, l_prev = m_scratch_ref[...], l_scratch_ref[...]
        
        # è¯»å– Q å’Œå½“å‰ K å—
        q = q_ref[...]
        k = k_ref[slice_k, :]
        
        # ğŸ”¥ ä¼˜åŒ–2: K @ Q è€Œä¸æ˜¯ Q @ K^T
        # qk å½¢çŠ¶: [bkv_compute, bq]
        qk = lax.dot_general(k, q, NT_DIM_NUMBERS, preferred_element_type=float32)

        o_prev = o_scratch_ref[:]
        v = v_ref[slice_k, :].astype(float32)
        step = bkv_compute_in  # å†…éƒ¨è¿­ä»£æ­¥é•¿
        
        # ç»†ç²’åº¦è¿­ä»£ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜è®¿é—®
        for idx in range(0, qk.shape[0], step):
            # è®¡ç®—å½“å‰å—çš„ max
            m_curr = qk[idx:idx+step].max(axis=0)[None, :]
            m_next = jnp.maximum(m_prev, m_curr)
            
            # ğŸ”¥ ä¼˜åŒ–1: ä½¿ç”¨ exp2
            # Q å·²ç»é¢„ä¹˜äº† log2(e)ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥ç”¨ exp2
            s_curr = jnp.exp2(qk[idx:idx+step] - m_next[0:1])
            
            # æ›´æ–° running sum
            l_curr = s_curr.sum(axis=0, keepdims=True)
            alpha = jnp.exp2(m_prev - m_next)  # ä¹Ÿç”¨ exp2
            l_next = l_curr + alpha * l_prev

            # è®¡ç®— softmax(QK) @ V çš„è´¡çŒ®
            sv_dims = (((0,), (0,)), ((), ()))
            o_curr = lax.dot_general(v[idx:idx+step], s_curr, sv_dims)
            
            # æ›´æ–°ç´¯ç§¯è¾“å‡ºï¼ˆonline softmax çš„æ ¸å¿ƒï¼‰
            o_prev = alpha[0:1, ...] * o_prev + o_curr
            m_prev, l_prev = m_next, l_next

        # å­˜å‚¨æ›´æ–°åçš„ running stats
        m_scratch_ref[...], l_scratch_ref[...] = m_next, l_next
        o_scratch_ref[:] = o_prev

    # å¾ªç¯å¤„ç†æ‰€æœ‰ KV å—
    lax.fori_loop(0, bkv // bkv_compute, body, None, unroll=True)

    # ============ æœ€ç»ˆå½’ä¸€åŒ– ============
    @pl.when(j == grid_width - 1)
    def end():
        """æœ€åä¸€ä¸ª KV å—æ—¶è¿›è¡Œæœ€ç»ˆå½’ä¸€åŒ–"""
        l = l_scratch_ref[...]
        l_inv = pltpu.repeat(1.0 / l, head_dim_v_repeats, axis=0)
        o_ref[...] = (o_scratch_ref[...] * l_inv).astype(o_ref.dtype)
```

### 4.6 Block Size é…ç½®çš„é€‰æ‹©åŸç†

```python
# æœ€ä¼˜é…ç½®ï¼ˆç»è¿‡å®éªŒç¡®å®šï¼‰
BQSIZE = 3328       # Query å—å¤§å°
BKVSIZE = 2816      # KV å—å¤§å°  
BKVCOMPUTESIZE = 256    # å†…éƒ¨è®¡ç®—å—å¤§å°
BKVCOMPUTEINSIZE = 256  # æœ€å†…å±‚è¿­ä»£å—å¤§å°
```

**ä¸ºä»€ä¹ˆæ˜¯è¿™äº›å€¼ï¼Ÿ**

1. **BQSIZE = 3328**: 
   - 75600 / 3328 â‰ˆ 22.7ï¼Œéœ€è¦ 23 ä¸ª Q å—
   - æ¥è¿‘èƒ½æ•´é™¤ 75600 çš„å€¼ï¼Œå‡å°‘ padding æµªè´¹

2. **BKVSIZE = 2816**:
   - 75600 / 2816 â‰ˆ 26.8ï¼Œéœ€è¦ 27 ä¸ª KV å—
   - ä¸ BQSIZE é…åˆï¼Œä½¿ç½‘æ ¼å¤§å°åˆç†

3. **BKVCOMPUTESIZE = 256**:
   - 2816 / 256 = 11ï¼Œæ­£å¥½æ•´é™¤
   - 256 æ˜¯ TPU VMEM å‹å¥½çš„å—å¤§å°

4. **BKVCOMPUTEINSIZE = 256**:
   - æ›´ç»†ç²’åº¦çš„è¿­ä»£ï¼Œä¼˜åŒ–æµæ°´çº¿

### 4.7 Pallas Kernel çš„å®Œæ•´åŒ…è£…

```python
def make_splash_mha(block_sizes, bkv_compute_in, interpret=False):
    """
    åˆ›å»º Splash Attention å‡½æ•°
    
    ä½¿ç”¨æ–¹æ³•:
        splash_fn = make_splash_mha(block_sizes, bkv_compute_in)
        output = splash_fn(q, k, v)
    
    æ³¨æ„: Q å¿…é¡»é¢„ä¹˜ log2(e)ï¼
    """
    def _splash_attention(q, k, v):
        num_q_heads, q_seq_len, head_dim_qk = q.shape
        head_dim_v = v.shape[-1]
        num_kv_heads = k.shape[0]
        kv_seq_len = k.shape[1]
        q_heads_per_kv_head = num_q_heads // num_kv_heads
        
        bq, bkv = block_sizes.block_q, block_sizes.block_kv
        bkv_compute = block_sizes.block_kv_compute

        # Index maps: å®šä¹‰æ¯ä¸ªç½‘æ ¼ç‚¹è¯»å–å“ªä¸ªæ•°æ®å—
        def q_index_map(h, i, j, *_): return (h, i, 0)
        def k_index_map(h, i, j, *_): return (h // q_heads_per_kv_head, j, 0)
        def v_index_map(h, i, j, *_): return (h // q_heads_per_kv_head, j, 0)
        def out_index_map(h, i, j, *_): return h, 0, i

        # Input/Output specifications
        in_specs = [
            pl.BlockSpec((None, bq, head_dim_qk), q_index_map),
            pl.BlockSpec((None, bkv, head_dim_qk), k_index_map),
            pl.BlockSpec((None, bkv, head_dim_v), v_index_map),
        ]
        
        # Scratch memory + output shapes
        out_shapes = [
            jax.ShapeDtypeStruct((NUM_SUBLANES, bq), jnp.float32),  # m_scratch
            jax.ShapeDtypeStruct((NUM_SUBLANES, bq), jnp.float32),  # l_scratch
            jax.ShapeDtypeStruct((head_dim_v, bq), jnp.float32),    # o_scratch
            jax.ShapeDtypeStruct((num_q_heads, head_dim_v, q_seq_len), q.dtype),  # output
        ]
        
        out_specs = [
            pl.BlockSpec((NUM_SUBLANES, bq), lambda *_: (0, 0)),
            pl.BlockSpec((NUM_SUBLANES, bq), lambda *_: (0, 0)),
            pl.BlockSpec((head_dim_v, bq), lambda *_: (0, 0)),
            pl.BlockSpec((None, head_dim_v, bq), out_index_map),
        ]
        
        # è®¡ç®—ç½‘æ ¼
        grid_width = kv_seq_len // bkv
        grid = (num_q_heads, q_seq_len // bq, grid_width)

        # è°ƒç”¨ Pallas
        return pl.pallas_call(
            functools.partial(
                _flash_attention_kernel,
                mask_value=DEFAULT_MASK_VALUE,
                grid_width=grid_width,
                bq=bq, bkv=bkv,
                bkv_compute=bkv_compute,
                bkv_compute_in=bkv_compute_in,
                head_dim_v=head_dim_v,
            ),
            grid_spec=pltpu.PrefetchScalarGridSpec(
                num_scalar_prefetch=0,
                in_specs=in_specs,
                out_specs=out_specs,
                grid=grid,
            ),
            # ğŸ”¥ ä¼˜åŒ–3: LP LLO Scheduler
            compiler_params=pltpu.CompilerParams(
                dimension_semantics=("parallel", "arbitrary", "arbitrary"),
                flags={"XLA_TPU_FORCE_LP_LLO_SCHEDULER": True}
            ),
            out_shape=out_shapes,
            interpret=interpret,
        )(q, k, v)[-1]  # åªè¿”å›æœ€ç»ˆè¾“å‡º
    
    return _splash_attention
```

### 4.8 åœ¨ Pipeline ä¸­çš„é›†æˆ

```python
# generate_flax.py ä¸­çš„é›†æˆä»£ç 

def _tpu_custom_attention(query, key, value, env, scale=None, ...):
    """åœ¨ torchax ç¯å¢ƒä¸­è°ƒç”¨è‡ªå®šä¹‰ attention"""
    mesh = getattr(env, '_mesh', None) or env.param.mesh
    
    def _attention_on_slices(q, k, v):
        scale_factor = 1.0 / math.sqrt(q.shape[-1]) if scale is None else scale
        
        # ğŸ”¥ å…³é”®: é¢„ä¹˜ log2(e)
        _LOG2_E = 1.44269504
        q = q * scale_factor * _LOG2_E
        
        def kernel_3d(q_3d, k_3d, v_3d):
            # Padding åˆ°å—å¤§å°çš„æ•´æ•°å€
            q_3d_padded, q_orig_len = pad_to_multiple(q_3d, BQSIZE, axis=1)
            k_3d_padded, k_orig_len = pad_to_multiple(k_3d, BKVSIZE, axis=1)
            v_3d_padded, v_orig_len = pad_to_multiple(v_3d, BKVSIZE, axis=1)
            
            # åˆ›å»º block sizes
            block_sizes = _BlockSizes(
                block_q=min(BQSIZE, padded_q_seq_len),
                block_kv=min(BKVSIZE, padded_kv_seq_len),
                block_kv_compute=min(BKVCOMPUTESIZE, padded_kv_seq_len),
            )
            
            # è°ƒç”¨è‡ªå®šä¹‰ kernel
            splash_kernel = custom_splash_attention.make_splash_mha(
                block_sizes=block_sizes, bkv_compute_in=BKVCOMPUTEINSIZE
            )
            out = splash_kernel(
                q_3d_padded.astype(jnp.float32),
                k_3d_padded.astype(jnp.float32),
                v_3d_padded.astype(jnp.float32)
            ).astype(q_3d_padded.dtype)
            
            # ç§»é™¤ paddingï¼Œäº¤æ¢è½´
            out = jnp.swapaxes(out, 1, 2)
            return out[:, :q_orig_len, ...]
        
        return jax.vmap(kernel_3d)(q, k, v)
    
    # ä½¿ç”¨ shard_map è¿›è¡Œåˆ†å¸ƒå¼æ‰§è¡Œ
    sharded_fn = shard_map(
        _attention_on_slices,
        mesh=mesh,
        in_specs=(q_partition_spec, kv_partition_spec, kv_partition_spec),
        out_specs=q_partition_spec,
        check_rep=False,
    )
    return sharded_fn(query, key, value)
```

### 4.9 K-Smoothing ä¼˜åŒ–

å¦ä¸€ä¸ªæå‡æ•°å€¼ç¨³å®šæ€§å’Œæ€§èƒ½çš„ä¼˜åŒ–ï¼š

```python
# generate_flax.py ç¬¬ 399-401 è¡Œ
if USE_K_SMOOTH:
    key_mean = jnp.mean(jkey, axis=2, keepdims=True)
    jkey = jkey - key_mean
```

**åŸç†**ï¼š
- å‡å» K çš„å‡å€¼ï¼Œä½¿æ•°å€¼æ›´ç¨³å®š
- ä¸å½±å“ Attention çš„ç»“æœï¼ˆå› ä¸º softmax å¯¹å¸¸æ•°åç§»ä¸æ•æ„Ÿï¼‰
- å‡å°‘äº†æ•°å€¼æº¢å‡ºçš„é£é™©

### 4.10 æ€§èƒ½æå‡æ€»ç»“

| ä¼˜åŒ–é˜¶æ®µ | æŠ€æœ¯ | æ—¶é—´ (720P 50æ­¥) | æå‡ |
|----------|------|------------------|------|
| åŸºçº¿ | æ ‡å‡† SDPA | 428s | - |
| é˜¶æ®µ1 | Splash Attention | 285s | 33% â†“ |
| é˜¶æ®µ2 | + exp2 ä¼˜åŒ– | 265s | 7% â†“ |
| é˜¶æ®µ3 | + QK Transpose | 255s | 4% â†“ |
| é˜¶æ®µ4 | + LP LLO Scheduler | 245s | 4% â†“ |
| é˜¶æ®µ5 | + Block Size è°ƒä¼˜ | 125s | 49% â†“ |
| **æ€»è®¡** | **æ‰€æœ‰ä¼˜åŒ–** | **125s** | **3.4x** |

---

## ç¬¬äº”ç« ï¼šVAE åœ¨ Torchax ä¸Šçš„å·¥ä½œåŸç†ä¸å¹¶è¡Œè®¾è®¡

æœ¬ç« è¯¦ç»†è®²è§£å¦‚ä½•è®© PyTorch å®ç°çš„ Diffusers VAE åœ¨ Torchax æ¡¥æ¥ä¸‹äº TPU ä¸Šé«˜æ•ˆè¿è¡Œï¼ŒåŒ…æ‹¬å¹¶è¡Œç­–ç•¥è®¾è®¡ã€åˆ†ç‰‡å®ç°å’Œé—®é¢˜è§£å†³ã€‚

### 5.1 æŒ‘æˆ˜ï¼šPyTorch VAE åˆ° TPU

#### 5.1.1 åŸå§‹é—®é¢˜

Wan VAE æ˜¯ç”¨ PyTorch å®ç°çš„ 3D å› æœå·ç§¯ç½‘ç»œã€‚ç›´æ¥åœ¨ TPU ä¸Šè¿è¡Œé¢ä¸´å¤šä¸ªæŒ‘æˆ˜ï¼š

1. **3D å·ç§¯å†…å­˜æ¶ˆè€—å¤§**ï¼š720P è§†é¢‘è§£ç éœ€è¦å¤§é‡å†…å­˜
2. **å› æœå·ç§¯éœ€è¦ç‰¹æ®Šå¤„ç†**ï¼šæ—¶é—´ç»´åº¦çš„å› æœ padding
3. **å¤šè®¾å¤‡å¹¶è¡Œå›°éš¾**ï¼šå·ç§¯æ“ä½œéš¾ä»¥ç›´æ¥åˆ†ç‰‡

```python
# åŸå§‹ VAE æ¶æ„å¤æ‚åº¦
# è¾“å…¥: [B, 16, 21, 90, 160] (latent)
# è¾“å‡º: [B, 3, 81, 720, 1280] (è§†é¢‘)
# ä¸­é—´ç‰¹å¾å›¾æœ€å¤§åˆ° [B, 384, 21, 90, 160]
```

#### 5.1.2 è§£å†³æ–¹æ¡ˆæ¶æ„

```mermaid
graph TB
    subgraph "VAE å¹¶è¡ŒåŒ–æ–¹æ¡ˆ"
        DIFF[Diffusers VAE<br/>PyTorch å®ç°]
        TORCHAX[Torchax æ¡¥æ¥å±‚<br/>PyTorch â†’ JAX]
        SHARD[Spatial Sharding<br/>Width ç»´åº¦åˆ†ç‰‡]
        TPU[TPU æ‰§è¡Œ<br/>8 chips å¹¶è¡Œ]
    end
    
    DIFF --> TORCHAX --> SHARD --> TPU
    
    style SHARD fill:#4caf50
```

### 5.2 Torchax æ¡¥æ¥åŸç†

#### 5.2.1 ä»€ä¹ˆæ˜¯ Torchax

Torchax æ˜¯ä¸€ä¸ªè®© PyTorch ä»£ç åœ¨ JAX/TPU ä¸Šè¿è¡Œçš„åº“ã€‚æ ¸å¿ƒæœºåˆ¶ï¼š

```python
import torchax

# 1. å…¨å±€å¯ç”¨ torchax
torchax.enable_globally()

# 2. è·å–é»˜è®¤ç¯å¢ƒ
env = torchax.default_env()

# 3. PyTorch æ“ä½œä¼šè‡ªåŠ¨è½¬ä¸º JAX æ“ä½œ
# torch.nn.Conv3d(...) â†’ jax.lax.conv_general_dilated(...)
```

#### 5.2.2 ç®—å­è¦†ç›–æœºåˆ¶

```python
# æ›¿æ¢ PyTorch çš„ scaled_dot_product_attention
from torchax.ops import ops_registry

def custom_attention(query, key, value, env=None, **kwargs):
    # è½¬æ¢ä¸º JAX
    jquery, jkey, jvalue = env.t2j_iso((query, key, value))
    # è°ƒç”¨ JAX å®ç°
    result = splash_attention(jquery, jkey, jvalue)
    # è½¬å› PyTorch
    return env.j2t_iso(result)

# æ³¨å†Œæ›¿æ¢
env._ops[torch.nn.functional.scaled_dot_product_attention] = \
    ops_registry.Operator(
        torch.nn.functional.scaled_dot_product_attention,
        functools.partial(custom_attention, env=env),
        is_jax_function=False,
        is_user_defined=True,
        needs_env=False,
        is_view_op=False,
    )
```

### 5.3 Spatial Partitioningï¼šåœ¨å®½åº¦ç»´åº¦åˆ†ç‰‡

#### 5.3.1 è®¾è®¡åŸåˆ™

**ä¸ºä»€ä¹ˆé€‰æ‹© Width ç»´åº¦ï¼Ÿ**

| åˆ†ç‰‡ç»´åº¦ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|----------|------|------|
| Batch | ç®€å• | è§†é¢‘ç”Ÿæˆé€šå¸¸ batch=1 |
| Channel | é€šé“ç‹¬ç«‹ | æ‰“ç ´é€šé“é—´ä¾èµ– |
| Temporal | æ—¶é—´ç‹¬ç«‹ | å› æœå·ç§¯éœ€è¦æ—¶é—´è¿ç»­ |
| Height | è¡Œç‹¬ç«‹ | æŸäº›å·ç§¯è·¨è¡Œ |
| **Width** | **åˆ—ç‹¬ç«‹ï¼Œå·ç§¯å‹å¥½** | **éœ€è¦ padding å¤„ç†** |

**Width åˆ†ç‰‡çš„å…³é”®ä¼˜åŠ¿**ï¼š
1. 3D å·ç§¯çš„ kernel é€šå¸¸æ˜¯ 3Ã—3Ã—3ï¼Œè·¨åˆ—çš„ä¾èµ–å¯ä»¥é€šè¿‡ padding å¤„ç†
2. å®½åº¦ 160 å¯ä»¥è¢« 8 æ•´é™¤ï¼ˆ160 / 8 = 20ï¼‰
3. æ¯ä¸ª TPU chip å¤„ç†è§†é¢‘çš„ä¸€ä¸ªå‚ç›´æ¡å¸¦

#### 5.3.2 åˆ†ç‰‡å®ç°

```python
# autoencoder_kl_wan.py æ ¸å¿ƒå®ç°

import jax
from torchax import interop
from jax.sharding import PartitionSpec as P

# åˆ›å»º JAX sharding çº¦æŸçš„ PyTorch è§†å›¾
mark_sharding = interop.torch_view(jax.lax.with_sharding_constraint)


class WanCausalConv3d(nn.Conv3d):
    """
    å¸¦æœ‰ TPU Spatial Sharding çš„ 3D å› æœå·ç§¯
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__(in_channels, out_channels, kernel_size, stride, padding)
        
        # è®¾ç½®å› æœ padding
        # æ—¶é—´ç»´åº¦åª pad è¿‡å»ï¼ˆå› æœæ€§ï¼‰
        # ç©ºé—´ç»´åº¦å¯¹ç§° pad
        self._padding = (
            self.padding[2], self.padding[2],  # W: left, right
            self.padding[1], self.padding[1],  # H: top, bottom
            2 * self.padding[0], 0             # T: past only, no future
        )
        self.padding = (0, 0, 0)  # å®é™…å·ç§¯ä¸ padï¼Œæˆ‘ä»¬æ‰‹åŠ¨ pad
    
    def forward(self, x, cache_x=None):
        """
        å‰å‘ä¼ æ’­ï¼Œå¸¦æœ‰ sharding çº¦æŸ
        
        Args:
            x: è¾“å…¥å¼ é‡ [B, C, T, H, W]
            cache_x: ç¼“å­˜çš„å†å²å¸§ï¼ˆç”¨äºæµå¼è§£ç ï¼‰
        """
        padding = list(self._padding)
        
        # å¤„ç†æ—¶é—´ç¼“å­˜
        if cache_x is not None and self._padding[4] > 0:
            cache_x = cache_x.to(x.device)
            x = torch.cat([cache_x, x], dim=2)  # æ‹¼æ¥å†å²å¸§
            padding[4] -= cache_x.shape[2]
        
        # åº”ç”¨ padding
        x = F.pad(x, padding)
        
        # ğŸ”¥ æ ¸å¿ƒï¼šåœ¨ Width ç»´åº¦åº”ç”¨ sharding
        # å°è¯•å¤šç§åˆ†ç‰‡ç­–ç•¥ï¼Œé€‰æ‹©å¯è¡Œçš„
        success = False
        
        # ç­–ç•¥ 1: dp + tp è”åˆåˆ†ç‰‡
        try:
            x = mark_sharding(x, P(None, None, None, None, ("dp", "tp")))
            success = True
            print("[DEBUG] Shard conv width along ('dp', 'tp')")
        except ValueError:
            pass
        
        # ç­–ç•¥ 2: ä»… tp åˆ†ç‰‡
        if not success:
            try:
                x = mark_sharding(x, P(None, None, None, None, ("tp",)))
                success = True
                print("[DEBUG] Shard conv width along ('tp')")
            except ValueError:
                pass
        
        # ç­–ç•¥ 3: ä»… dp åˆ†ç‰‡
        if not success:
            try:
                x = mark_sharding(x, P(None, None, None, None, ("dp",)))
                success = True
                print("[DEBUG] Shard conv width along ('dp')")
            except ValueError:
                pass
        
        # æ‰§è¡Œå·ç§¯
        return super().forward(x)
```

#### 5.3.3 ä¸ºä»€ä¹ˆç”¨ try-except

```python
# åˆ†ç‰‡å¯èƒ½å¤±è´¥çš„åŸå› ï¼š
# 1. å¼ é‡å½¢çŠ¶ä¸èƒ½è¢« mesh ç»´åº¦æ•´é™¤
# 2. æŸäº› mesh ç»´åº¦æœªä½¿ç”¨
# 3. å¤šä¸»æœºç¯å¢ƒä¸‹çš„è®¾å¤‡ä¸å¯å¯»å€

try:
    x = mark_sharding(x, P(None, None, None, None, ("dp", "tp")))
except ValueError:
    # å®½åº¦ä¸èƒ½è¢« dp*tp æ•´é™¤ï¼Œå›é€€åˆ°å…¶ä»–ç­–ç•¥
    pass
```

### 5.4 VAE è§£ç å™¨çš„å®Œæ•´æµç¨‹

#### 5.4.1 é€å¸§è§£ç ç­–ç•¥

ç”±äº 3D å› æœå·ç§¯éœ€è¦æ—¶é—´è¿ç»­æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨é€å¸§è§£ç ï¼š

```python
# autoencoder_kl_wan.py ç¬¬ 1237-1271 è¡Œ

def _decode(self, z: torch.Tensor, return_dict: bool = True):
    """
    è§£ç  latent åˆ°è§†é¢‘
    
    ç­–ç•¥ï¼šé€å¸§å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰å¸§åˆ°å†…å­˜
    """
    _, _, num_frame, height, width = z.shape
    
    # æ¸…ç†ç¼“å­˜
    self.clear_cache()
    
    # åé‡åŒ–å·ç§¯
    x = self.post_quant_conv(z)
    
    # ğŸ”¥ é€å¸§è§£ç 
    for i in range(num_frame):
        if i == 0:
            # ç¬¬ä¸€å¸§ï¼šåˆå§‹åŒ–ç¼“å­˜
            out, self._feat_map = self.decoder(
                x[:, :, i : i + 1, :, :],
                feat_cache=self._feat_map,
                first_chunk=True,  # æ ‡è®°ä¸ºç¬¬ä¸€å¸§
            )
        else:
            # åç»­å¸§ï¼šä½¿ç”¨ç¼“å­˜
            out_, self._feat_map = self.decoder(
                x[:, :, i : i + 1, :, :], 
                feat_cache=self._feat_map
            )
            out = torch.cat([out, out_], 2)  # æ‹¼æ¥æ—¶é—´ç»´åº¦
    
    # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
    out = torch.clamp(out, min=-1.0, max=1.0)
    
    self.clear_cache()
    return DecoderOutput(sample=out)
```

#### 5.4.2 ç‰¹å¾ç¼“å­˜æœºåˆ¶

å› æœå·ç§¯éœ€è¦å†å²å¸§çš„ç‰¹å¾ï¼Œæˆ‘ä»¬ç”¨ç¼“å­˜ä¼˜åŒ–ï¼š

```python
# ç¼“å­˜ç»“æ„
CACHE_T = 2  # ç¼“å­˜æœ€è¿‘ 2 å¸§çš„ç‰¹å¾

class WanResidualBlock(nn.Module):
    def forward(self, x, feat_cache=None, feat_idx=[0]):
        # è®¡ç®—æ®‹å·®è¿æ¥
        h = self.conv_shortcut(x)
        
        x = self.norm1(x)
        x = self.nonlinearity(x)
        
        # ğŸ”¥ ä½¿ç”¨ç¼“å­˜
        if feat_cache is not None:
            idx = feat_idx
            
            # ç¼“å­˜å½“å‰å¸§çš„ç‰¹å¾
            cache_x = x[:, :, -CACHE_T:, :, :].clone()
            
            # å¦‚æœå½“å‰å¸§æ•°ä¸è¶³ï¼Œè¡¥å……å†å²ç¼“å­˜
            if cache_x.shape[2] < 2 and feat_cache[idx] is not None:
                cache_x = torch.cat([
                    feat_cache[idx][:, :, -1, :, :].unsqueeze(2).to(cache_x.device),
                    cache_x
                ], dim=2)
            
            # ä½¿ç”¨ç¼“å­˜è¿›è¡Œå·ç§¯
            x = self.conv1(x, feat_cache[idx])
            feat_cache[idx] = cache_x
            feat_idx += 1
        else:
            x = self.conv1(x)
        
        # ... åç»­å¤„ç†
        return x + h, feat_idx, feat_cache
```

### 5.5 å¤šä¸»æœºç¯å¢ƒä¸‹çš„ç‰¹æ®Šå¤„ç†

#### 5.5.1 æœ€ç»ˆè¾“å‡ºçš„å¤åˆ¶

è§£ç åçš„è§†é¢‘éœ€è¦åœ¨æ‰€æœ‰ä¸»æœºä¸Šå¯è®¿é—®ï¼š

```python
# autoencoder_kl_wan.py ç¬¬ 943-944 è¡Œ

def forward(self, x, feat_cache=None, first_chunk=False):
    # ... è§£ç é€»è¾‘ ...
    
    # ğŸ”¥ å…³é”®ï¼šå¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡
    # é¿å…å¤šä¸»æœºç¯å¢ƒä¸‹çš„ "non-addressable devices" é”™è¯¯
    x = mark_sharding(x, P())  # ç©º PartitionSpec = å¤åˆ¶
    return x, feat_cache
```

**ä¸ºä»€ä¹ˆéœ€è¦è¿™æ­¥ï¼Ÿ**

```mermaid
graph LR
    subgraph "å¤šä¸»æœºç¯å¢ƒ"
        H1[Host 1<br/>TPU 0-3]
        H2[Host 2<br/>TPU 4-7]
        
        D1[åˆ†ç‰‡æ•°æ®<br/>åªåœ¨éƒ¨åˆ†è®¾å¤‡]
        D2[å®Œæ•´æ•°æ®<br/>æ‰€æœ‰è®¾å¤‡å¯è®¿é—®]
    end
    
    D1 -->|mark_sharding P| D2
    
    style D2 fill:#4caf50
```

### 5.6 VAE æƒé‡åŠ è½½ä¸åˆ†ç‰‡

#### 5.6.1 æƒé‡è½¬æ¢æµç¨‹

```python
# generate_flax.py ç¬¬ 563-636 è¡Œ

def load_wan_vae_fixed(pretrained_model_name_or_path, eval_shapes, device):
    """
    åŠ è½½ VAE æƒé‡ï¼Œå¤„ç†ç±»å‹è½¬æ¢é¿å… torchax é—®é¢˜
    """
    from huggingface_hub import hf_hub_download
    from safetensors import safe_open
    
    # ä¸‹è½½æƒé‡
    ckpt_path = hf_hub_download(
        pretrained_model_name_or_path,
        subfolder="vae",
        filename="diffusion_pytorch_model.safetensors"
    )
    
    # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ CPU è®¾å¤‡é¿å… torchax ç±»å‹è½¬æ¢é—®é¢˜
    with jax.default_device('cpu'):
        # åœ¨ torchax ç¦ç”¨æ—¶åŠ è½½æƒé‡
        import torchax
        torchax.disable_globally()
        
        state_dict = {}
        with safe_open(ckpt_path, framework="pt") as f:
            for key in f.keys():
                tensor = f.get_tensor(key)
                # è½¬æ¢ä¸º bfloat16
                if tensor.dtype == torch.float32:
                    tensor = tensor.to(torch.bfloat16)
                state_dict[key] = tensor
        
        # åˆ›å»º VAE å®ä¾‹
        vae = AutoencoderKLWan(
            in_channels=3,
            out_channels=3,
            latent_channels=16,
            # ... å…¶ä»–å‚æ•°
        )
        
        # åŠ è½½æƒé‡
        vae.load_state_dict(state_dict, strict=True)
        
        torchax.enable_globally()
    
    return vae
```

#### 5.6.2 æƒé‡ä¸åˆ†ç‰‡çš„åŸå› 

```python
# VAE æƒé‡ç›¸å¯¹è¾ƒå°ï¼ˆçº¦ 500MBï¼‰ï¼Œç›´æ¥å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡
# è€Œ Transformer æƒé‡å¾ˆå¤§ï¼ˆ14B å‚æ•°ï¼‰ï¼Œå¿…é¡»åˆ†ç‰‡

# VAE åˆ†ç‰‡ç­–ç•¥ï¼šæƒé‡å¤åˆ¶ï¼Œæ¿€æ´»åˆ†ç‰‡
vae_sharding = {
    # æ‰€æœ‰æƒé‡éƒ½å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡
    r'.*': P(),  # ç©º PartitionSpec = å¤åˆ¶
}
```

### 5.7 Halo Exchange å¤„ç†è¾¹ç•Œä¾èµ–

#### 5.7.1 å·ç§¯è¾¹ç•Œé—®é¢˜

å½“åœ¨ Width ç»´åº¦åˆ†ç‰‡åï¼Œ3Ã—3Ã—3 å·ç§¯åœ¨è¾¹ç•Œå¤„éœ€è¦ç›¸é‚»åˆ†ç‰‡çš„æ•°æ®ï¼š

```mermaid
graph LR
    subgraph "Width åˆ†ç‰‡åçš„ Halo Exchange"
        S0[Shard 0<br/>W: 0-19]
        S1[Shard 1<br/>W: 20-39]
        S2[Shard 2<br/>W: 40-59]
        
        H01[Halo<br/>W: 19-20]
        H12[Halo<br/>W: 39-40]
    end
    
    S0 <-->|äº¤æ¢è¾¹ç•Œ| H01 <-->|äº¤æ¢è¾¹ç•Œ| S1
    S1 <-->|äº¤æ¢è¾¹ç•Œ| H12 <-->|äº¤æ¢è¾¹ç•Œ| S2
```

#### 5.7.2 XLA è‡ªåŠ¨å¤„ç†

å¥½æ¶ˆæ¯æ˜¯ XLA ç¼–è¯‘å™¨ä¼šè‡ªåŠ¨æ’å…¥å¿…è¦çš„é€šä¿¡ï¼š

```python
# XLA ç¼–è¯‘å™¨è¯†åˆ«å·ç§¯æ“ä½œéœ€è¦ halo exchange
# è‡ªåŠ¨æ’å…¥ collective-permute æ“ä½œ

# ä»£ç ä¸­æ— éœ€æ˜¾å¼å¤„ç†ï¼
# åªéœ€æ­£ç¡®æ ‡è®° sharding
x = mark_sharding(x, P(None, None, None, None, ("dp", "tp")))
# XLA ä¼šåœ¨éœ€è¦æ—¶è‡ªåŠ¨äº¤æ¢è¾¹ç•Œæ•°æ®
```

### 5.8 å®Œæ•´çš„ VAE åˆå§‹åŒ–æµç¨‹

```python
def setup_wan_vae_for_tpu(model_id, mesh, env):
    """
    å®Œæ•´çš„ TPU VAE åˆå§‹åŒ–æµç¨‹
    """
    # 1. ç¦ç”¨ torchax åŠ è½½æƒé‡
    import torchax
    torchax.disable_globally()
    
    # 2. åŠ è½½ VAEï¼ˆåœ¨ CPU ä¸Šï¼‰
    with jax.default_device('cpu'):
        vae = load_wan_vae_fixed(model_id, eval_shapes=None, device='cpu')
    
    # 3. é‡æ–°å¯ç”¨ torchax
    torchax.enable_globally()
    
    # 4. è®¾ç½® mesh åˆ°ç¯å¢ƒ
    env._mesh = mesh
    env._initial_content.mesh = mesh
    
    # 5. ç§»åŠ¨ VAE æƒé‡åˆ° XLA
    with mesh:
        state_dict = vae.state_dict()
        state_dict = env.to_xla(state_dict)
        vae.load_state_dict(state_dict, assign=True)
    
    return vae
```

### 5.9 VAE æ€§èƒ½å¯¹æ¯”

| é…ç½® | å•è®¾å¤‡ | 8 è®¾å¤‡ (æ— åˆ†ç‰‡) | 8 è®¾å¤‡ (Width åˆ†ç‰‡) |
|------|--------|----------------|---------------------|
| å†…å­˜ä½¿ç”¨ | OOM | 24GB/chip | 8GB/chip |
| è§£ç æ—¶é—´ | - | 45s | 12s |
| æå‡ | - | åŸºçº¿ | **3.75x** |

### 5.10 VAE ä¼˜åŒ–æ€»ç»“

```mermaid
graph TB
    subgraph "VAE ä¼˜åŒ–è·¯çº¿å›¾"
        P1[é—®é¢˜1: å†…å­˜è¿‡å¤§]
        P2[é—®é¢˜2: å¤šè®¾å¤‡å¹¶è¡Œ]
        P3[é—®é¢˜3: è¾¹ç•Œä¾èµ–]
        
        S1[è§£å†³1: Width åˆ†ç‰‡<br/>mark_sharding]
        S2[è§£å†³2: å¤šç­–ç•¥å›é€€<br/>try-except]
        S3[è§£å†³3: XLA è‡ªåŠ¨ Halo<br/>æ— éœ€æ‰‹åŠ¨]
        
        S4[è§£å†³4: é€å¸§è§£ç <br/>ç‰¹å¾ç¼“å­˜]
        S5[è§£å†³5: æœ€ç»ˆå¤åˆ¶<br/>P ç©ºè§„æ ¼]
    end
    
    P1 --> S1
    P2 --> S2
    P3 --> S3
    S1 & S2 & S3 --> S4
    S4 --> S5
    
    style S1 fill:#4caf50
    style S4 fill:#2196f3
```

---

## ç¬¬å…­ç« ï¼šæ€§èƒ½åˆ†ææ–¹æ³•è®º

### 6.1 MFU è®¡ç®—æ–¹æ³•

```python
def compute_dit_flops_per_step(
    batch_size=2, num_blocks=40, hidden_dim=5120,
    num_heads=40, head_dim=128, ffn_dim=13824,
    seq_len=75600, text_seq_len=226,
):
    # Self-Attention
    qkv_proj = 3 * 2 * seq_len * hidden_dim * hidden_dim
    qk_matmul = 2 * batch_size * num_heads * seq_len * head_dim * seq_len
    av_matmul = 2 * batch_size * num_heads * seq_len * seq_len * head_dim
    out_proj = 2 * seq_len * hidden_dim * hidden_dim
    self_attn = qkv_proj + qk_matmul + av_matmul + out_proj
    
    # Cross-Attention
    q_proj = 2 * seq_len * hidden_dim * hidden_dim
    kv_proj = 2 * 2 * text_seq_len * hidden_dim * hidden_dim
    cross_attn = q_proj + kv_proj + ...
    
    # FFN
    ffn = 2 * 2 * seq_len * hidden_dim * ffn_dim + ...
    
    return num_blocks * (self_attn + cross_attn + ffn)

# MFU = FLOPs / (å³°å€¼TFLOPs Ã— æ—¶é—´)
mfu = compute_dit_flops_per_step() / (14688e12 * 2.5)
```

### 6.2 DiT Step æ—¶é—´åˆ†è§£

| æ“ä½œ | æ—¶é—´å æ¯” | MFU | ç“¶é¢ˆç±»å‹ |
|------|----------|-----|----------|
| Self-Attention | 66.8% | 37% | VPU-bound |
| Convolution Fusion | 14.3% | - | é€šä¿¡ |
| All-to-All | 6.7% | - | ICI å¸¦å®½ |
| Linear | - | 66% | Compute-bound |

### 6.3 Profiler ä½¿ç”¨

```python
with jax.profiler.trace("/dev/shm/tensorboard"):
    output = pipe(prompt=prompt, num_inference_steps=3)
    jax.effects_barrier()
```

---

## ç¬¬ä¸ƒç« ï¼šTorchax æ¡¥æ¥ä¸ä»£ç å®ç°

### 7.1 PyTorch åˆ° JAX çš„æ¡¥æ¥

```python
import torchax

torchax.enable_globally()
env = torchax.default_env()

env._mesh = mesh
env._initial_content.mesh = mesh
env.config.use_tpu_splash_attention = True
```

### 7.2 ç®—å­æ³¨å†Œä¸è¦†ç›–

```python
from torchax.ops import ops_registry

def scaled_dot_product_attention(query, key, value,
                                  env=None, **kwargs):
    if getattr(env.config, 'use_tpu_splash_attention', False):
        jquery, jkey, jvalue = env.t2j_iso((query, key, value))
        
        if USE_K_SMOOTH:
            key_mean = jnp.mean(jkey, axis=2, keepdims=True)
            jkey = jkey - key_mean
        
        if jkey.shape[2] > 10000 and USE_CUSTOM_ATTENTION:
            res = _tpu_custom_attention(jquery, jkey, jvalue, env)
        else:
            res = _tpu_splash_attention(jquery, jkey, jvalue, env)
        
        return env.j2t_iso(res)
    
    return _sdpa_reference(query, key, value, **kwargs)

# æ³¨å†Œ
env._ops[torch.nn.functional.scaled_dot_product_attention] = \
    ops_registry.Operator(
        torch.nn.functional.scaled_dot_product_attention,
        functools.partial(scaled_dot_product_attention, env=env),
        is_jax_function=False, is_user_defined=True,
        needs_env=False, is_view_op=False,
    )
```

### 7.3 æƒé‡è½¬æ¢ä¸åŠ è½½

```python
import re
from jax.sharding import NamedSharding, PartitionSpec as P

def shard_weight_dict(weight_dict, sharding_dict, mesh):
    result = {}
    for k, v in weight_dict.items():
        matched = False
        for target, sharding in sharding_dict.items():
            if re.fullmatch(target, k) is not None:
                v.apply_jax_(jax.device_put,
                            NamedSharding(mesh, P(*sharding)))
                matched = True
                break
        if not matched:
            v.apply_jax_(jax.device_put, NamedSharding(mesh, P()))
        result[k] = v
    return result

# ç§»åŠ¨æ¨¡å—åˆ° XLA
def _move_module(module, env):
    with jax.default_device('cpu'):
        state_dict = module.state_dict()
        state_dict = env.to_xla(state_dict)
        module.load_state_dict(state_dict, assign=True)
```

### 7.4 æ··åˆç²¾åº¦ç­–ç•¥

```python
torch.set_default_dtype(torch.bfloat16)

# VAE æƒé‡è½¬æ¢ä¸º bf16
params = jax.tree_util.tree_map(
    lambda x: x.astype(jnp.bfloat16), params
)

# Attention è®¡ç®—ä½¿ç”¨ float32
def attention_kernel(q, k, v):
    out = splash_kernel(
        q.astype(jnp.float32),
        k.astype(jnp.float32),
        v.astype(jnp.float32)
    )
    return out.astype(q.dtype)
```

---

## ç¬¬å…«ç« ï¼šå®Œæ•´ä»£ç ç¤ºä¾‹ä¸å®æˆ˜

### 8.1 ç¯å¢ƒé…ç½®

```bash
# å®‰è£…ä¾èµ–
pip install torch --index-url https://download.pytorch.org/whl/cpu
pip install -U jax[tpu] torchax
pip install transformers accelerate safetensors flax optax

# å®‰è£…ä¿®æ”¹ç‰ˆ diffusers
git clone https://github.com/yangwhale/diffusers-tpu.git
cd diffusers-tpu && pip install -e .

# å®‰è£… MaxDiffusion
git clone https://github.com/AI-Hypercomputer/maxdiffusion.git
cd maxdiffusion && pip install -e .
```

### 8.2 Text-to-Video å®Œæ•´æµç¨‹

```python
"""Wan 2.1 Text-to-Video on TPU v6e"""

import jax
import torch
import torchax
from jax.sharding import Mesh, PartitionSpec as P
from jax.experimental import mesh_utils

# é…ç½®
MODEL_ID = "Wan-AI/Wan2.1-T2V-14B-Diffusers"
HEIGHT, WIDTH, FRAMES = 720, 1280, 81
NUM_STEPS = 50

def main():
    # JAX é…ç½®
    jax.config.update("jax_compilation_cache_dir", "/dev/shm/jax_cache")
    torch.set_default_dtype(torch.bfloat16)
    
    # åˆ›å»º Mesh
    num_devices = len(jax.devices())
    mesh_devices = mesh_utils.create_device_mesh(
        (2, 1, num_devices // 2),
        allow_split_physical_axes=True
    )
    mesh = Mesh(mesh_devices, ('dp', 'sp', 'tp'))
    
    # åˆå§‹åŒ– torchax
    torchax.enable_globally()
    env = torchax.default_env()
    env._mesh = mesh
    env.config.use_tpu_splash_attention = True
    
    # åŠ è½½ Pipeline
    from diffusers.pipelines.wan.pipeline_wan_flax import WanPipeline
    from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler
    
    torchax.disable_globally()
    scheduler = UniPCMultistepScheduler(
        prediction_type='flow_prediction',
        use_flow_sigmas=True,
        flow_shift=5.0
    )
    pipe = WanPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16)
    pipe.scheduler = scheduler
    torchax.enable_globally()
    
    # è®¾ç½® Splash Attention å’Œ VAE
    setup_splash_attention(pipe, mesh, env)
    pipe.vae = setup_wan_vae(MODEL_ID, mesh)
    
    # ç”Ÿæˆ
    prompt = "A cat and a dog baking a cake together in a kitchen."
    
    with mesh:
        output = pipe(
            prompt=prompt,
            height=HEIGHT, width=WIDTH, num_frames=FRAMES,
            num_inference_steps=NUM_STEPS,
            guidance_scale=5.0,
            use_dp=True,
        )
    
    from diffusers.utils import export_to_video
    export_to_video(output.frames[0], "output.mp4", fps=16)

if __name__ == "__main__":
    main()
```

### 8.3 ä¸‰é˜¶æ®µæ¨ç†æ¶æ„

ä¸‰é˜¶æ®µæ¨ç†å°†ç”Ÿæˆè¿‡ç¨‹æ‹†åˆ†ä¸ºç‹¬ç«‹æ­¥éª¤ï¼Œä¾¿äºè°ƒè¯•å’Œèµ„æºç®¡ç†ã€‚

```mermaid
graph LR
    subgraph "Stage 1: Text Encoder"
        PROMPT[Text Prompt]
        T5[UMT5 Encoding]
        EMB[Embeddings<br/>safetensors]
    end
    
    subgraph "Stage 2: Transformer"
        EMB2[Load Embeddings]
        DIT[DiT Denoising<br/>50 steps]
        LAT[Latents<br/>safetensors]
    end
    
    subgraph "Stage 3: VAE"
        LAT2[Load Latents]
        VAE[VAE Decode]
        VIDEO[Video MP4]
    end
    
    PROMPT --> T5 --> EMB --> EMB2 --> DIT --> LAT --> LAT2 --> VAE --> VIDEO
```

**Stage 1: Text Encoder**

```python
# stage1_text_encoder.py
def encode_prompts(pipe, prompt, negative_prompt):
    prompt_embeds, negative_prompt_embeds = pipe.encode_prompt(
        prompt=prompt,
        negative_prompt=negative_prompt,
        do_classifier_free_guidance=True,
    )
    
    # ä¿å­˜åˆ° safetensors
    save_embeddings_to_safetensors({
        'prompt_embeds': prompt_embeds,
        'negative_prompt_embeds': negative_prompt_embeds,
    }, 'stage1_embeddings.safetensors')
```

**Stage 2: Transformer**

```python
# stage2_transformer.py
def run_transformer_inference(pipe, embeddings, config):
    # åŠ è½½ embeddings
    prompt_embeds = embeddings['prompt_embeds'].to('jax')
    negative_prompt_embeds = embeddings['negative_prompt_embeds'].to('jax')
    
    # è¿è¡Œ denoising
    latents = pipe(
        prompt_embeds=prompt_embeds,
        negative_prompt_embeds=negative_prompt_embeds,
        output_type='latent',  # ä¸è§£ç 
    ).frames
    
    # ä¿å­˜ latents
    save_latents_to_safetensors(latents, 'stage2_latents.safetensors')
```

**Stage 3: VAE Decode**

```python
# stage3_vae_decoder.py
def decode_latents(vae, latents, config):
    # åå½’ä¸€åŒ–
    latents_mean = jnp.array(vae.latents_mean).reshape(1, 16, 1, 1, 1)
    latents_std = 1.0 / jnp.array(vae.latents_std).reshape(1, 16, 1, 1, 1)
    latents = latents / latents_std + latents_mean
    
    # è§£ç 
    video = vae.decode(latents)
    
    # å¯¼å‡º
    export_to_video(video, 'output.mp4', fps=16)
```

### 8.4 æ€§èƒ½åŸºå‡†æµ‹è¯•

**æµ‹è¯•ç¯å¢ƒ**: TPU v6e-8, Wan 2.1 14B, 720P 81å¸§

| é…ç½® | æ—¶é—´ | æ¯æ­¥æ—¶é—´ |
|------|------|----------|
| æ ‡å‡† Attention | 428s | ~8.5s |
| Splash Attention | 285s | ~5.7s |
| + exp2 ä¼˜åŒ– | 265s | ~5.3s |
| + LP LLO è°ƒåº¦ | 245s | ~4.9s |
| + æœ€ç»ˆä¼˜åŒ– | **125s** | **~2.5s** |

---

## ç¬¬ä¹ç« ï¼šImage-to-Video ä¸“é¡¹ä¼˜åŒ–

### 9.1 I2V ä¸ T2V çš„å…³é”®å·®å¼‚

```mermaid
graph TB
    subgraph "T2V Pipeline"
        T_NOISE[Random Noise<br/>å…¨éƒ¨å¸§]
        T_DIT[DiT Denoising<br/>æ‰€æœ‰å¸§ç›¸åŒ timestep]
    end
    
    subgraph "I2V Pipeline"
        I_IMG[Input Image<br/>ç¬¬ä¸€å¸§]
        I_NOISE[Random Noise<br/>åç»­å¸§]
        I_CONCAT[Concatenate<br/>Image + Noise]
        I_DIT[DiT Denoising<br/>expand_timesteps]
    end
    
    T_NOISE --> T_DIT
    I_IMG --> I_CONCAT
    I_NOISE --> I_CONCAT
    I_CONCAT --> I_DIT
    
    style I_DIT fill:#fff3e0
```

### 9.2 expand_timesteps æœºåˆ¶

I2V çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ `expand_timesteps`ï¼šç¬¬ä¸€å¸§ä½¿ç”¨å›ºå®š timestep=0ï¼Œå…¶ä½™å¸§ä½¿ç”¨æ­£å¸¸ timestepã€‚

```python
def expand_timesteps(timesteps, num_frames, device):
    """
    æ‰©å±• timestep ç”¨äº I2V
    
    ç¬¬ä¸€å¸§: timestep = 0 (å¹²å‡€å›¾åƒ)
    å…¶ä½™å¸§: timestep = t (æ­£å¸¸å»å™ª)
    """
    # åŸå§‹ timestep: [t]
    # æ‰©å±•å: [0, t, t, t, ..., t]
    expanded = torch.zeros(num_frames, device=device)
    expanded[1:] = timesteps
    return expanded

# åœ¨ pipeline ä¸­ä½¿ç”¨
timesteps = self.scheduler.timesteps
for t in timesteps:
    t_expanded = expand_timesteps(t, num_frames=81, device=device)
    # t_expanded.shape = [81]
    # t_expanded = [0, t, t, t, ...]
    
    # ç¬¬ä¸€å¸§ä¸åŠ å™ª
    latents[:, :, 0] = clean_image_latent
    
    # å…¶ä½™å¸§æ­£å¸¸å»å™ª
    latents[:, :, 1:] = denoise(latents[:, :, 1:], t)
```

### 9.3 I2V Attention ä¼˜åŒ–

```python
def i2v_attention_with_image_conditioning(
    query, key, value,
    image_latent,
    mesh,
    env,
):
    """
    I2V ç‰¹æ®Š attention å¤„ç†
    
    å…³é”®ç‚¹:
    1. ç¬¬ä¸€å¸§å‚ä¸ KVï¼Œä½†ä¸éœ€è¦å»å™ª
    2. KV åºåˆ—é•¿åº¦ = è§†é¢‘å¸§ + æ–‡æœ¬ tokens
    3. éœ€è¦å¤„ç† padding
    """
    # å°† image latent ä½œä¸º context
    image_k = project_to_kv(image_latent)  # æŠ•å½±ä¸º KV
    
    # æ‹¼æ¥ image KV å’Œ video KV
    full_k = torch.cat([image_k, key], dim=2)
    full_v = torch.cat([image_v, value], dim=2)
    
    # è®¡ç®— attention
    if full_k.shape[2] > 10000:
        # ä½¿ç”¨è‡ªå®šä¹‰ kernel
        output = custom_splash_attention(query, full_k, full_v)
    else:
        output = standard_attention(query, full_k, full_v)
    
    return output
```

### 9.4 I2V å®Œæ•´å®ç°

```python
"""Wan 2.2 Image-to-Video on TPU"""

from diffusers import WanImageToVideoPipeline
from PIL import Image

def run_i2v(
    image_path: str,
    prompt: str,
    output_path: str = "output_i2v.mp4",
):
    # åŠ è½½ pipeline
    pipe = WanImageToVideoPipeline.from_pretrained(
        "Wan-AI/Wan2.2-I2V-14B-Diffusers",
        torch_dtype=torch.bfloat16,
    )
    
    # è®¾ç½® TPU ä¼˜åŒ–
    setup_tpu_optimizations(pipe)
    
    # åŠ è½½è¾“å…¥å›¾åƒ
    image = Image.open(image_path).resize((1280, 720))
    
    # ç”Ÿæˆè§†é¢‘
    with mesh:
        output = pipe(
            image=image,
            prompt=prompt,
            height=720,
            width=1280,
            num_frames=81,
            num_inference_steps=50,
            guidance_scale=5.0,
        )
    
    # å¯¼å‡º
    export_to_video(output.frames[0], output_path, fps=16)

# ä½¿ç”¨ç¤ºä¾‹
run_i2v(
    image_path="cat.jpg",
    prompt="A cat walking in the garden",
    output_path="cat_walking.mp4"
)
```

### 9.5 I2V æ€§èƒ½æ•°æ®

| é…ç½® | T2V æ—¶é—´ | I2V æ—¶é—´ | æå‡ |
|------|----------|----------|------|
| åŸºçº¿ | 428s | 450s | - |
| ä¼˜åŒ–å | 125s | 94.5s | **4.8x** |

**I2V æ¯” T2V æ›´å¿«çš„åŸå› **:
1. ç¬¬ä¸€å¸§ä¸éœ€è¦å»å™ªï¼ˆtimestep=0ï¼‰
2. Image latent ä½œä¸ºé¢å¤– contextï¼Œattention è®¡ç®—é‡ç•¥å¢ä½†å¼•å¯¼æ•ˆæœæ›´å¥½
3. æ”¶æ•›æ›´å¿«ï¼Œå¯ä»¥ä½¿ç”¨æ›´å°‘çš„æ­¥æ•°

---

## ç¬¬åç« ï¼šè°ƒè¯•ä¸æ•…éšœæ’é™¤

### 10.1 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### é—®é¢˜ 1: VAE é¢œè‰²åè½¬

**ç—‡çŠ¶**: ç”Ÿæˆçš„è§†é¢‘é¢œè‰²ä¸é¢„æœŸç›¸å

**åŸå› **: MaxDiffusion VAE å®ç°çš„è¾“å‡ºèŒƒå›´ä¸ PyTorch ç‰ˆæœ¬ä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ³• 1: è¾“å‡ºåå¤„ç†
video = 255 - video

# æ–¹æ³• 2: ä¿®æ”¹ VAE forward
def patched_forward(self, x):
    output = self.original_forward(x)
    return 1 - output  # åè½¬
```

#### é—®é¢˜ 2: bfloat16 ä¿å­˜å¤±è´¥

**ç—‡çŠ¶**: `safetensors` ä¸æ”¯æŒ bf16 ç›´æ¥ä¿å­˜

**è§£å†³æ–¹æ¡ˆ**:
```python
def save_bf16_tensor(tensor, path):
    """ä¿å­˜ bf16 tensor çš„å…¼å®¹æ–¹æ¡ˆ"""
    metadata = {}
    
    if tensor.dtype == torch.bfloat16:
        # è½¬æ¢ä¸º float32 ä¿å­˜
        tensor_save = tensor.to(torch.float32)
        metadata['original_dtype'] = 'bfloat16'
    else:
        tensor_save = tensor
    
    save_file({'tensor': tensor_save}, path, metadata=metadata)

def load_bf16_tensor(path):
    """åŠ è½½å¹¶æ¢å¤ bf16 tensor"""
    with safe_open(path, framework='pt') as f:
        tensor = f.get_tensor('tensor')
        metadata = f.metadata()
    
    if metadata.get('original_dtype') == 'bfloat16':
        tensor = tensor.to(torch.bfloat16)
    
    return tensor
```

#### é—®é¢˜ 3: PyTree æœªæ³¨å†Œ

**ç—‡çŠ¶**: `KeyError: <class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>`

**è§£å†³æ–¹æ¡ˆ**:
```python
from jax.tree_util import register_pytree_node
from transformers import modeling_outputs

# æ³¨å†Œæ‰€æœ‰éœ€è¦çš„ç±»å‹
output_classes = [
    modeling_outputs.BaseModelOutputWithPastAndCrossAttentions,
    modeling_outputs.BaseModelOutput,
    modeling_outputs.CausalLMOutputWithCrossAttentions,
]

for cls in output_classes:
    register_pytree_node(
        cls,
        lambda obj: (tuple(getattr(obj, f) for f in obj.keys()), type(obj)),
        lambda aux, children: aux(**dict(zip(aux.__dataclass_fields__.keys(), children)))
    )
```

#### é—®é¢˜ 4: OOM (Out of Memory)

**ç—‡çŠ¶**: å†…å­˜ä¸è¶³å¯¼è‡´ç¨‹åºå´©æºƒ

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨æ›´æ¿€è¿›çš„åˆ†ç‰‡
mesh = Mesh(devices, ('dp', 'sp', 'tp'))  # ç¡®ä¿ä½¿ç”¨åˆ†ç‰‡

# 2. å¯ç”¨å†…å­˜ä¼˜åŒ–
jax.config.update("jax_default_prng_impl", "threefry")
jax.config.update("jax_enable_x64", False)

# 3. åˆ†é˜¶æ®µå¤„ç†
# ä¸è¦ä¸€æ¬¡åŠ è½½æ‰€æœ‰æ¨¡å‹
del text_encoder  # ç¼–ç å®Œæˆåé‡Šæ”¾
gc.collect()

# 4. ä½¿ç”¨ donation
@jax.jit(donate_argnums=(0,))
def step(state, inputs):
    return new_state
```

#### é—®é¢˜ 5: Torchax ç‰ˆæœ¬å…¼å®¹

**ç—‡çŠ¶**: `env.auto_shard_inputs` æ–¹æ³•ä¸å­˜åœ¨

**è§£å†³æ–¹æ¡ˆ**:
```python
# torchax 0.0.11+ éœ€è¦æ‰‹åŠ¨è®¾ç½® mesh
env._mesh = mesh
env._initial_content.mesh = mesh

# æ‰‹åŠ¨åº”ç”¨åˆ†ç‰‡
def apply_input_sharding(tensor, use_dp=False):
    if use_dp:
        pspec = P('dp', None, None, None, None)
    else:
        pspec = P()
    
    sharding = NamedSharding(mesh, pspec)
    tensor.apply_jax_(jax.device_put, sharding)
    return tensor
```

### 10.2 æ€§èƒ½è°ƒè¯•

#### ä½¿ç”¨ JAX Profiler

```python
# 1. å¯ç”¨ profiler
with jax.profiler.trace("/dev/shm/tensorboard"):
    output = pipe(prompt=prompt, num_inference_steps=3)
    jax.effects_barrier()

# 2. æŸ¥çœ‹ TensorBoard
# tensorboard --logdir=/dev/shm/tensorboard

# 3. åˆ†æå…³é”®æŒ‡æ ‡
# - MXU åˆ©ç”¨ç‡
# - å†…å­˜å¸¦å®½åˆ©ç”¨ç‡
# - é€šä¿¡å¼€é”€
```

#### æ‰“å°ä¸­é—´çŠ¶æ€

```python
def debug_sharding(tensor, name="tensor"):
    """æ‰“å° tensor çš„åˆ†ç‰‡ä¿¡æ¯"""
    if hasattr(tensor, '_jax_array'):
        jax_arr = tensor._jax_array
        print(f"{name}:")
        print(f"  Shape: {jax_arr.shape}")
        print(f"  Sharding: {jax_arr.sharding}")
        print(f"  Devices: {jax_arr.devices()}")
    else:
        print(f"{name}: Not on JAX")

# åœ¨ forward ä¸­ä½¿ç”¨
debug_sharding(hidden_states, "hidden_states")
```

### 10.3 æ—¥å¿—å’Œç›‘æ§

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TimingContext:
    """è®¡æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    def __init__(self, name):
        self.name = name
    
    def __enter__(self):
        self.start = time.time()
        return self
    
    def __exit__(self, *args):
        elapsed = time.time() - self.start
        logger.info(f"{self.name}: {elapsed:.2f}s")

# ä½¿ç”¨
with TimingContext("DiT Transformer"):
    latents = transformer(latents, timestep, encoder_hidden_states)

with TimingContext("VAE Decode"):
    video = vae.decode(latents)
```

---

## é™„å½•

### A. å¸¸è§é—®é¢˜å¿«é€Ÿç´¢å¼•

| é—®é¢˜ | ç« èŠ‚ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| MXU åˆ©ç”¨ç‡ä½ | 4.3 | exp2 ä¼˜åŒ– |
| å†…å­˜ä¸è¶³ | 3, 10.1 | åˆ†ç‰‡ç­–ç•¥ |
| é¢œè‰²åè½¬ | 10.1 | åå¤„ç† |
| bf16 ä¿å­˜ | 10.1 | è½¬æ¢æ–¹æ¡ˆ |

### B. æœ¯è¯­è¡¨

| æœ¯è¯­ | å…¨ç§° | è¯´æ˜ |
|------|------|------|
| MFU | Model FLOPs Utilization | æ¨¡å‹è®¡ç®—åˆ©ç”¨ç‡ |
| MXU | Matrix Multiply Unit | çŸ©é˜µä¹˜æ³•å•å…ƒ |
| VPU | Vector Processing Unit | å‘é‡å¤„ç†å•å…ƒ |
| HBM | High Bandwidth Memory | é«˜å¸¦å®½å†…å­˜ |
| ICI | Inter-Chip Interconnect | èŠ¯ç‰‡é—´äº’è” |
| FSDP | Fully Sharded Data Parallel | å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ |
| CP | Context Parallelism | ä¸Šä¸‹æ–‡å¹¶è¡Œ |
| SP | Sequence Parallelism | åºåˆ—å¹¶è¡Œ |
| DP | Data Parallelism | æ•°æ®å¹¶è¡Œ |
| DiT | Diffusion Transformer | æ‰©æ•£ Transformer |
| CFG | Classifier-Free Guidance | æ— åˆ†ç±»å™¨å¼•å¯¼ |
| VAE | Variational AutoEncoder | å˜åˆ†è‡ªç¼–ç å™¨ |

### C. å‚è€ƒèµ„æº

**å®˜æ–¹ä»“åº“**:
- [Wan-AI/Wan2.1](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B-Diffusers)
- [AI-Hypercomputer/maxdiffusion](https://github.com/AI-Hypercomputer/maxdiffusion)
- [diffusers-tpu](https://github.com/yangwhale/diffusers-tpu)

**æŠ€æœ¯æ–‡æ¡£**:
- [JAX Pallas Guide](https://jax.readthedocs.io/en/latest/pallas/)
- [TPU Performance Guide](https://cloud.google.com/tpu/docs/performance-guide)
- [Flash Attention Paper](https://arxiv.org/abs/2205.14135)

---

## ç»“è¯­

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº† Wan æ¨¡å‹åœ¨ TPU v6e ä¸Šçš„è¿ç§»ä¸ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»ç¡¬ä»¶æ¶æ„ç†è§£åˆ°åˆ†ç‰‡ç­–ç•¥è®¾è®¡ï¼Œä» Splash Attention å†…æ ¸ä¼˜åŒ–åˆ° VAE æ€§èƒ½è°ƒä¼˜ã€‚é€šè¿‡è¿™äº›ä¼˜åŒ–ï¼ŒWan 2.1 14B æ¨¡å‹çš„ 720P 81å¸§è§†é¢‘ç”Ÿæˆæ—¶é—´ä» 428 ç§’é™ä½åˆ° 125 ç§’ï¼Œæå‡äº† **3.4 å€**ã€‚I2V ä»»åŠ¡æ›´æ˜¯è¾¾åˆ° 94.5 ç§’çš„æè‡´æ€§èƒ½ã€‚

**å…³é”®ä¼˜åŒ–ç‚¹æ€»ç»“**:

```mermaid
graph TB
    subgraph "ä¼˜åŒ–è·¯å¾„"
        O1[Context Parallelism<br/>Self-Attention head åˆ†ç‰‡]
        O2[Sequence Parallelism<br/>Cross-Attention seq åˆ†ç‰‡]
        O3[exp2 ä¼˜åŒ–<br/>VPU åŸç”ŸæŒ‡ä»¤]
        O4[QK Transpose<br/>çŸ©é˜µä¹˜æ³•ä¼˜åŒ–]
        O5[LP LLO è°ƒåº¦<br/>VPU/MXU é‡å ]
        O6[Spatial Partitioning<br/>VAE å®½åº¦åˆ†ç‰‡]
    end
    
    O1 --> O2 --> O3 --> O4 --> O5 --> O6
    
    style O3 fill:#4caf50
    style O5 fill:#2196f3
```

å¸Œæœ›æœ¬æ–‡æ¡£èƒ½ä¸ºä»äº‹ TPU å¤§æ¨¡å‹ä¼˜åŒ–çš„å·¥ç¨‹å¸ˆæä¾›æœ‰ä»·å€¼çš„å‚è€ƒã€‚