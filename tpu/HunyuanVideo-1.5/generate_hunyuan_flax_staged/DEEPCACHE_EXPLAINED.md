# DeepCache åŸç†è¯¦è§£ï¼šä» GPU åˆ° TPU çš„å®ç°ä¹‹è·¯

æœ¬æ–‡æ¡£ç³»ç»Ÿæ€§åœ°è®²è§£ DeepCache çš„åŸç†ã€è®¾è®¡ç†å¿µï¼Œä»¥åŠå¦‚ä½•åœ¨ TPU/torchax ç¯å¢ƒä¸‹ä»é›¶å®ç°ã€‚

---

## ğŸ“š ç›®å½•

1. [DeepCache æ˜¯ä»€ä¹ˆ](#1-deepcache-æ˜¯ä»€ä¹ˆ)
2. [åŸç†ä¸è®¾è®¡ç†å¿µ](#2-åŸç†ä¸è®¾è®¡ç†å¿µ)
3. [GPU ç‰ˆæœ¬å®ç°åˆ†æ](#3-gpu-ç‰ˆæœ¬å®ç°åˆ†æ)
4. [ä¾èµ–åº“åˆ†æ](#4-ä¾èµ–åº“åˆ†æ)
5. [ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨](#5-ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨)
6. [TPU ç‰ˆæœ¬å®ç°](#6-tpu-ç‰ˆæœ¬å®ç°)
7. [æ€§èƒ½å¯¹æ¯”](#7-æ€§èƒ½å¯¹æ¯”)

---

## 1. DeepCache æ˜¯ä»€ä¹ˆ

### 1.1 èƒŒæ™¯é—®é¢˜

Diffusion æ¨¡å‹æ¨ç†éœ€è¦å¤šæ¬¡è¿­ä»£ï¼ˆé€šå¸¸ 20-50 æ­¥ï¼‰ï¼Œæ¯æ­¥éƒ½è¦å®Œæ•´æ‰§è¡Œ Transformer å‰å‘ä¼ æ’­ï¼Œè®¡ç®—é‡å·¨å¤§ã€‚

```mermaid
flowchart LR
    subgraph æ ‡å‡†æ¨ç†
        A[Step 1] --> B[Step 2]
        B --> C[Step 3]
        C --> D[...]
        D --> E[Step N]
    end
    
    F[æ¯æ­¥éƒ½å®Œæ•´æ‰§è¡Œ<br/>æ‰€æœ‰ Transformer å±‚] --> G[è®¡ç®—é‡ = N Ã— å…¨éƒ¨å±‚]
    
    style F fill:#ffcccc
```

### 1.2 æ ¸å¿ƒè§‚å¯Ÿ

DeepCache è®ºæ–‡å‘ç°ï¼š**ç›¸é‚»å»å™ªæ­¥éª¤çš„é«˜å±‚ç‰¹å¾å˜åŒ–å¾ˆå°**ã€‚

```mermaid
flowchart TB
    subgraph ç‰¹å¾å˜åŒ–åˆ†æ
        direction LR
        A["Step t"] --> B["Step t+1"]
        
        subgraph StepT["Step t ç‰¹å¾"]
            T1[æµ…å±‚ç‰¹å¾<br/>å˜åŒ–å¤§]
            T2[æ·±å±‚ç‰¹å¾<br/>å˜åŒ–å°]
        end
        
        subgraph StepT1["Step t+1 ç‰¹å¾"]
            T3[æµ…å±‚ç‰¹å¾<br/>å˜åŒ–å¤§]
            T4[æ·±å±‚ç‰¹å¾<br/>â‰ˆ Step t]
        end
        
        T1 -.-> T3
        T2 ==> T4
    end
    
    style T2 fill:#ccffcc
    style T4 fill:#ccffcc
```

### 1.3 DeepCache æ€æƒ³

æ—¢ç„¶æ·±å±‚ç‰¹å¾å˜åŒ–å°ï¼Œå¯ä»¥**ç¼“å­˜å¹¶å¤ç”¨**ï¼Œåªè®¡ç®—æµ…å±‚ï¼š

```mermaid
flowchart TB
    subgraph DeepCacheç­–ç•¥
        direction TB
        
        S1["Step 1: å®Œæ•´è®¡ç®— â†’ ç¼“å­˜æ·±å±‚ç‰¹å¾"]
        S2["Step 2: å¤ç”¨ç¼“å­˜ â†’ åªç®—æµ…å±‚"]
        S3["Step 3: å¤ç”¨ç¼“å­˜ â†’ åªç®—æµ…å±‚"]
        S4["Step 4: åˆ·æ–°ç¼“å­˜ â†’ å®Œæ•´è®¡ç®—"]
        S5["Step 5: å¤ç”¨ç¼“å­˜ â†’ åªç®—æµ…å±‚"]
        
        S1 --> S2 --> S3 --> S4 --> S5
    end
    
    S1 -.- |"å®Œæ•´è®¡ç®—"| Full
    S2 -.- |"ç¼“å­˜åŠ é€Ÿ"| Cache
    S3 -.- |"ç¼“å­˜åŠ é€Ÿ"| Cache
    S4 -.- |"åˆ·æ–°ç¼“å­˜"| Full
    
    style S2 fill:#ccffcc
    style S3 fill:#ccffcc
    style S5 fill:#ccffcc
```

---

## 2. åŸç†ä¸è®¾è®¡ç†å¿µ

### 2.1 HunyuanVideo-1.5 720p_t2v Transformer ç»“æ„

> âš ï¸ **é‡è¦**ï¼šHunyuanVideo-1.5 720p_t2v çš„å®é™…æ¶æ„å¦‚ä¸‹ã€‚

```mermaid
flowchart TB
    subgraph HunyuanVideo["HunyuanVideo-1.5 720p_t2v Transformer"]
        direction TB
        
        Input[Hidden States] --> DB1
        
        subgraph DoubleBlocks["Double Blocks (54å±‚)"]
            DB1[Double Block 1] --> DB2[Double Block 2]
            DB2 --> DB3[...]
            DB3 --> DB54[Double Block 54]
        end
        
        DB54 --> FL[Final Layer]
        FL --> Output[Noise Prediction]
    end
    
    style DoubleBlocks fill:#ffcccc
```

**å®é™…å±‚æ•°ç»Ÿè®¡**ï¼š
- **double_blocks**: 54 å±‚
- **single_blocks**: 0 å±‚
- **final_layer**: 1 å±‚
- **æ€»è®¡**: 55 å±‚

### 2.2 Double Block vs Single Block

**Double Blockï¼ˆMMDoubleStreamBlockï¼‰**ï¼š
- å¤„ç†ä¸¤ä¸ªåˆ†ç¦»çš„æµï¼šimgï¼ˆè§†é¢‘ç‰¹å¾ï¼‰å’Œ txtï¼ˆæ–‡æœ¬ç‰¹å¾ï¼‰
- æ¯ä¸ªæµæœ‰ç‹¬ç«‹çš„ Attention å’Œ MLP
- ä¸¤ä¸ªæµä¹‹é—´é€šè¿‡ Cross-Attention äº¤äº’
- è¾“å…¥: (img, txt)ï¼Œè¾“å‡º: (img, txt)

**Single Blockï¼ˆMMSingleStreamBlockï¼‰**ï¼š
- å°† img å’Œ txt åˆå¹¶ä¸ºå•ä¸€åºåˆ— x = concat(img, txt)
- ç»Ÿä¸€å¤„ç†åå†åˆ†ç¦»
- æ›´è½»é‡ï¼Œé€‚åˆåæœŸå¤„ç†
- 720p_t2v é…ç½®ä¸­ä¸ä½¿ç”¨

### 2.3 ç¼“å­˜ç­–ç•¥

**ç¼“å­˜ç‚¹é€‰æ‹©**ï¼šBlock 52 ä¹‹åã€Block 53 ä¹‹å‰

> âš ï¸ **é‡è¦**ï¼šDeepCache çš„æ­£ç¡®è®¾è®¡æ˜¯è·³è¿‡ block 0-52ï¼ˆå…± 53 å±‚ï¼‰ï¼Œä½†å¿…é¡»è®¡ç®—æœ€åä¸€ä¸ª block 53ã€‚
> è¿™æ˜¯å› ä¸ºæœ€åä¸€ä¸ª block å¯¹ç»†èŠ‚ç”Ÿæˆéå¸¸å…³é”®ï¼Œä¸èƒ½å®Œå…¨è·³è¿‡ã€‚

```mermaid
flowchart LR
    subgraph å®Œæ•´Forward
        A[Input] --> B1[Block 0-52<br/>53å±‚]
        B1 --> C["ç¼“å­˜ç‚¹<br/>(img, txt)"]
        C --> B2[Block 53<br/>1å±‚]
        B2 --> E[Final Layer]
        E --> F[Output]
    end
    
    subgraph ç¼“å­˜Forward
        A2[Input] --> C2["ä½¿ç”¨ç¼“å­˜<br/>(img, txt)"]
        C2 --> B3[Block 53<br/>1å±‚]
        B3 --> E2[Final Layer]
        E2 --> F2[Output]
    end
    
    style B1 fill:#ffcccc
    style C fill:#ffffcc
    style C2 fill:#ffffcc
    style B2 fill:#ccffcc
    style B3 fill:#ccffcc
```

### 2.4 ç†è®ºåŠ é€Ÿæ¯”

| è·¯å¾„ | è®¡ç®—å±‚æ•° | å æ¯” |
|------|----------|------|
| å®Œæ•´ Forward | 54 + 1 = 55 | 100% |
| ç¼“å­˜ Forward | 1 + 1 = 2 | 3.6% |

**ç†è®ºåŠ é€Ÿæ¯”**ï¼šå½“ cache hit ç‡ä¸º 50% æ—¶ï¼š
- å®Œæ•´æ­¥éª¤ï¼š25 Ã— 55 = 1375 å±‚
- ç¼“å­˜æ­¥éª¤ï¼š25 Ã— 2 = 50 å±‚
- æ€»è®¡ï¼š1425 å±‚ vs 2750 å±‚
- **ç†è®ºåŠ é€Ÿæ¯”**ï¼š2750/1425 â‰ˆ **1.93x**

### 2.5 ç¼“å­˜å·¥ä½œæµç¨‹è¯¦è§£

ä»¥ä¸‹æ˜¯ DeepCache çš„è¯¦ç»†å·¥ä½œæµç¨‹ï¼Œä»¥ `cache_step_interval = 4` ä¸ºä¾‹ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DeepCache ç¼“å­˜å·¥ä½œæµç¨‹                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Step 11 (cache_start_step):                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Block 0 â†’ Block 1 â†’ ... â†’ Block 52 â†’ [ä¿å­˜ç¼“å­˜] â†’ Block 53 â†’ FL  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                    â†“                                        â”‚
â”‚                              ç¼“å­˜ = (img, txt)  â† Block 52 çš„è¾“å‡º            â”‚
â”‚                                                                             â”‚
â”‚  Step 12 (cache hit):                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ [è¯»å–ç¼“å­˜] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Block 53 â†’ Final Layer â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚  Step 13 (cache hit):                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ [è¯»å–ç¼“å­˜] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Block 53 â†’ Final Layer â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚  Step 14 (cache hit):                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ [è¯»å–ç¼“å­˜] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Block 53 â†’ Final Layer â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â”‚  Step 15 (cache refresh = cache_start_step + interval):                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Block 0 â†’ Block 1 â†’ ... â†’ Block 52 â†’ [åˆ·æ–°ç¼“å­˜] â†’ Block 53 â†’ FL  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                    â†“                                        â”‚
â”‚                              ç¼“å­˜ = (img, txt)  â† æ–°çš„ Block 52 è¾“å‡º         â”‚
â”‚                                                                             â”‚
â”‚  Step 16-17-18 (cache hit): ä½¿ç”¨ Step 15 çš„ç¼“å­˜...                          â”‚
â”‚                                                                             â”‚
â”‚  ... å¾ªç¯ç›´åˆ° cache_end_step ...                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç†è§£**ï¼š
1. **ç¼“å­˜å†…å®¹**ï¼šBlock 52 çš„è¾“å‡º `(img, txt)`ï¼Œè¿™æ˜¯è¿›å…¥æœ€åä¸€ä¸ª block ä¹‹å‰çš„çŠ¶æ€
2. **ç¼“å­˜ forward æ‰§è¡Œä»€ä¹ˆ**ï¼šBlock 53 + Single Blocksï¼ˆå¦‚æœæœ‰ï¼‰+ Final Layer
3. **ä¸ºä»€ä¹ˆä¿ç•™ Block 53**ï¼šæœ€åä¸€ä¸ª block å¯¹è¾“å‡ºè´¨é‡è‡³å…³é‡è¦ï¼Œå®ƒè´Ÿè´£æ•´åˆæ‰€æœ‰ä¿¡æ¯
4. **1:3 æ¨¡å¼**ï¼šé»˜è®¤ `interval=4` æ„å‘³ç€æ¯ 4 æ­¥åˆ·æ–°ä¸€æ¬¡ï¼Œå³ 1 æ¬¡å®Œæ•´è®¡ç®— + 3 æ¬¡ç¼“å­˜è®¡ç®—

```mermaid
sequenceDiagram
    participant Step11 as Step 11
    participant Step12 as Step 12
    participant Step13 as Step 13
    participant Step14 as Step 14
    participant Step15 as Step 15
    
    Note over Step11: å®Œæ•´è®¡ç®— 54 å±‚
    Step11->>Step11: Block 0-52
    Step11->>Step11: ä¿å­˜ç¼“å­˜
    Step11->>Step11: Block 53 + FL
    
    Note over Step12,Step14: ç¼“å­˜è®¡ç®— 2 å±‚
    Step12->>Step12: ä½¿ç”¨ç¼“å­˜
    Step12->>Step12: Block 53 + FL
    
    Step13->>Step13: ä½¿ç”¨ç¼“å­˜
    Step13->>Step13: Block 53 + FL
    
    Step14->>Step14: ä½¿ç”¨ç¼“å­˜
    Step14->>Step14: Block 53 + FL
    
    Note over Step15: åˆ·æ–°ç¼“å­˜ 54 å±‚
    Step15->>Step15: Block 0-52
    Step15->>Step15: åˆ·æ–°ç¼“å­˜
    Step15->>Step15: Block 53 + FL
```

### 2.6 ç¼“å­˜åˆ·æ–°ç­–ç•¥

ä¸èƒ½æ°¸è¿œä½¿ç”¨æ—§ç¼“å­˜ï¼Œéœ€è¦å‘¨æœŸæ€§åˆ·æ–°ï¼š

```mermaid
flowchart LR
    subgraph åˆ·æ–°ç­–ç•¥
        direction TB
        
        P1["å‰æœŸ (Step 0-10)"]
        P2["ä¸­æœŸ (Step 11-44)"]
        P3["åæœŸ (Step 45-49)"]
        
        P1 --> |"æ¯æ­¥å®Œæ•´è®¡ç®—<br/>ç‰¹å¾å˜åŒ–å¤§"| N1[ä¸ç¼“å­˜]
        P2 --> |"æ¯4æ­¥åˆ·æ–°ä¸€æ¬¡<br/>ç‰¹å¾ç¨³å®š"| N2[ç¼“å­˜+åˆ·æ–°]
        P3 --> |"æ¯æ­¥å®Œæ•´è®¡ç®—<br/>ç»†èŠ‚é‡è¦"| N3[ä¸ç¼“å­˜]
    end
    
    style N1 fill:#ffcccc
    style N2 fill:#ccffcc
    style N3 fill:#ffcccc
```

**å‚æ•°é…ç½®**ï¼š
- `cache_start_step = 11`ï¼šå¼€å§‹ç¼“å­˜çš„æ­¥æ•°
- `cache_end_step = 45`ï¼šåœæ­¢ç¼“å­˜çš„æ­¥æ•°
- `cache_step_interval = 4`ï¼šåˆ·æ–°é—´éš”

---

## 3. GPU ç‰ˆæœ¬å®ç°åˆ†æ

### 3.1 å…¸å‹ GPU DeepCache æ¶æ„

```mermaid
flowchart TB
    subgraph GPU_DeepCache["GPU DeepCache å®ç°"]
        direction TB
        
        A[angelslim åº“] --> B[infer_state ç¼“å­˜ç®¡ç†]
        C[diffusers Pipeline] --> D[register_cache / update_cache]
        
        B --> E{jax.lax.cond é£æ ¼}
        D --> E
        
        E --> |"condition=True"| F[å®Œæ•´ Forward]
        E --> |"condition=False"| G[ç¼“å­˜ Forward]
        
        F --> H[æ›´æ–°ç¼“å­˜]
        G --> I[ä½¿ç”¨ç¼“å­˜]
    end
```

### 3.2 æ ¸å¿ƒæ•°æ®ç»“æ„

```python
# GPU ç‰ˆæœ¬çš„ infer_state
class InferState:
    def __init__(self):
        self.cached_features = {}      # å±‚ç¼“å­˜
        self.step_index = 0            # å½“å‰æ­¥æ•°
        self.no_cache_steps = set()    # ä¸ä½¿ç”¨ç¼“å­˜çš„æ­¥
        
    def should_cache(self, step):
        return step not in self.no_cache_steps
    
    def get_cache(self, layer_name):
        return self.cached_features.get(layer_name)
    
    def set_cache(self, layer_name, features):
        self.cached_features[layer_name] = features
```

### 3.3 Transformer å±‚å†…çš„æ¡ä»¶åˆ†æ”¯

```python
# GPU ç‰ˆæœ¬åœ¨å±‚å†…åšæ¡ä»¶åˆ†æ”¯
class DoubleBlock(nn.Module):
    def forward(self, x, infer_state=None):
        if infer_state and infer_state.should_use_cache(self.layer_idx):
            # ä½¿ç”¨ç¼“å­˜ï¼Œè·³è¿‡è®¡ç®—
            return infer_state.get_cache(self.layer_idx)
        else:
            # æ­£å¸¸è®¡ç®—
            output = self._forward_impl(x)
            if infer_state:
                infer_state.set_cache(self.layer_idx, output)
            return output
```

---

## 4. ä¾èµ–åº“åˆ†æ

### 4.1 angelslim åº“

```mermaid
flowchart TB
    subgraph angelslim["angelslim åº“åŠŸèƒ½"]
        direction TB
        
        A1[CacheManager] --> A2[ç®¡ç†å±‚çº§ç¼“å­˜]
        A1 --> A3[è‡ªåŠ¨ç¼“å­˜æ›´æ–°]
        A1 --> A4[å†…å­˜ä¼˜åŒ–]
        
        B1[InferState] --> B2[çŠ¶æ€è¿½è¸ª]
        B1 --> B3[æ­¥æ•°ç®¡ç†]
        B1 --> B4[æ¡ä»¶åˆ¤æ–­]
        
        C1[PipelineIntegration] --> C2[diffusers é›†æˆ]
        C1 --> C3[è‡ªåŠ¨ hook æ³¨å…¥]
    end
```

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- è‡ªåŠ¨ç®¡ç†å¤šå±‚ç¼“å­˜çš„ç”Ÿå‘½å‘¨æœŸ
- ä¸ diffusers Pipeline æ·±åº¦é›†æˆ
- æä¾›ç®€æ´çš„ API

### 4.2 ä¾èµ–çš„ PyTorch ç‰¹æ€§

```mermaid
flowchart LR
    subgraph PyTorchç‰¹æ€§
        A[torch.compile] --> A1[å›¾æ¨¡å¼ç¼–è¯‘]
        B[åŠ¨æ€æ¡ä»¶åˆ†æ”¯] --> B1[if/else åœ¨è¿è¡Œæ—¶]
        C[in-place æ“ä½œ] --> C1[ç¼“å­˜åŸåœ°æ›´æ–°]
        D[CUDA å†…å­˜ç®¡ç†] --> D1[è‡ªåŠ¨æ˜¾å­˜å›æ”¶]
    end
```

---

## 5. ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨

### 5.1 torchax é™åˆ¶

```mermaid
flowchart TB
    subgraph é™åˆ¶["torchax/JAX é™åˆ¶"]
        direction TB
        
        L1["âŒ åŠ¨æ€æ¡ä»¶åˆ†æ”¯"]
        L2["âŒ è¿è¡Œæ—¶ if/else"]
        L3["âŒ å¯å˜çŠ¶æ€"]
        L4["âŒ å¸ƒå°”ç´¢å¼•"]
        L5["âŒ ConcretizationTypeError"]
    end
    
    subgraph åŸå› ["åŸå› "]
        R1["XLA éœ€è¦é™æ€è®¡ç®—å›¾"]
        R2["JIT ç¼–è¯‘æ—¶éœ€ç¡®å®šæ‰€æœ‰è·¯å¾„"]
        R3["çº¯å‡½æ•°å¼ç¼–ç¨‹æ¨¡å‹"]
    end
    
    L1 --> R1
    L2 --> R2
    L3 --> R3
```

### 5.2 jax.lax.cond çš„é—®é¢˜

GPU ç‰ˆæœ¬ä½¿ç”¨ç±»ä¼¼ `jax.lax.cond` çš„æ¨¡å¼ï¼Œä½†åœ¨ torchax ä¸­ï¼š

```mermaid
flowchart TB
    subgraph jax_condé—®é¢˜["jax.lax.cond åœ¨ torchax ä¸­çš„é—®é¢˜"]
        direction TB
        
        P1["é—®é¢˜1: PyTree ç»“æ„å¿…é¡»åŒ¹é…"]
        P2["é—®é¢˜2: torchax tensor wrapper ä¸é€æ˜"]
        P3["é—®é¢˜3: JAX tracer æ³„æ¼"]
        P4["é—®é¢˜4: è¿”å›å€¼ç»“æ„ä¸ä¸€è‡´"]
        
        P1 --> E1["ä¸¤ä¸ªåˆ†æ”¯è¿”å›ä¸åŒæ•°é‡çš„ tensor"]
        P2 --> E2["æ— æ³•ç›´æ¥æ¯”è¾ƒ PyTree ç»“æ„"]
        P3 --> E3["traced value é€ƒé€¸å‡º JIT èŒƒå›´"]
        P4 --> E4["ç¼–è¯‘å¤±è´¥æˆ–è¿è¡Œæ—¶é”™è¯¯"]
    end
```

### 5.3 å¤±è´¥çš„å°è¯•

```python
# âŒ å°è¯•1ï¼šç›´æ¥åœ¨ JIT å†…åšæ¡ä»¶åˆ†æ”¯
def forward(self, x, use_cache):
    if use_cache:  # ConcretizationTypeError!
        return self.cached_output
    else:
        return self._compute(x)

# âŒ å°è¯•2ï¼šjax.lax.cond å°è£…
def forward(self, x, use_cache):
    return jax.lax.cond(
        use_cache,
        lambda: (self.cached_output, None, None),  # ç»“æ„ä¸åŒ¹é…
        lambda: self._compute_with_cache(x),        # è¿”å› 3 ä¸ªå€¼
    )
```

### 5.4 Tracer æ³„æ¼é—®é¢˜

```mermaid
flowchart TB
    subgraph TracerLeak["Tracer æ³„æ¼ç¤ºæ„"]
        JIT["JIT ç¼–è¯‘èŒƒå›´"]
        
        subgraph Inside["JIT å†…éƒ¨"]
            T1["åˆ›å»º traced tensor"]
            T2["æ¡ä»¶åˆ†æ”¯"]
            T3["è¿”å›ç»“æœ"]
        end
        
        subgraph Outside["JIT å¤–éƒ¨"]
            O1["æ¥æ”¶ç»“æœ"]
            O2["ç¼“å­˜åˆ° Python å¯¹è±¡"]
            O3["ä¸‹æ¬¡è°ƒç”¨ä½¿ç”¨"]
        end
        
        T1 --> T2 --> T3
        T3 --> O1 --> O2 --> O3
        
        O3 -.-> |"tracer æ³„æ¼!"| T2
    end
    
    style O2 fill:#ffcccc
```

å½“æŠŠ JIT å†…éƒ¨çš„ traced tensor ä¿å­˜åˆ°å¤–éƒ¨ Python å¯¹è±¡ï¼ˆå¦‚ cacheï¼‰ï¼Œå†åœ¨ä¸‹æ¬¡ JIT è°ƒç”¨æ—¶ä½¿ç”¨ï¼Œä¼šå¯¼è‡´ tracer æ³„æ¼é”™è¯¯ã€‚

---

## 6. TPU ç‰ˆæœ¬å®ç°

### 6.1 è§£å†³æ–¹æ¡ˆï¼šåˆ†ç¦»æ¨¡å—

```mermaid
flowchart TB
    subgraph è§£å†³æ–¹æ¡ˆ["åˆ†ç¦»æ¨¡å—æ–¹æ¡ˆ"]
        direction TB
        
        M1["FullForwardModule"]
        M2["CachedForwardModule"]
        
        M1 --> |"ç‹¬ç«‹ç¼–è¯‘"| C1["torchax.compile()"]
        M2 --> |"ç‹¬ç«‹ç¼–è¯‘"| C2["torchax.compile()"]
        
        C1 --> R1["å®Œæ•´ Forward<br/>è¿”å› output + cache"]
        C2 --> R2["ç¼“å­˜ Forward<br/>åªç”¨ cache"]
        
        Python["Python å±‚æ¡ä»¶åˆ†æ”¯"] --> |"if use_cache"| Choice
        Choice --> |"True"| M2
        Choice --> |"False"| M1
    end
    
    style Python fill:#ccffcc
```

### 6.2 FullForwardModule å®ç°

```python
class FullForwardModule(torch.nn.Module):
    """å°è£…å®Œæ•´ transformer forwardï¼ˆæ‰§è¡Œæ‰€æœ‰ 54 å±‚ double_blocksï¼‰
    
    ç¼“å­˜ç‚¹ï¼šåœ¨ block 52 ä¹‹åã€block 53 ä¹‹å‰ä¿å­˜ä¸­é—´çŠ¶æ€
    """
    
    def __init__(self, transformer, mask_type, extra_kwargs):
        super().__init__()
        self.transformer = transformer
        self.mask_type = mask_type
        self.extra_kwargs = extra_kwargs
    
    def forward(self, hidden_states, timestep, text_states, ...):
        transformer = self.transformer
        num_double_blocks = len(transformer.double_blocks)
        
        # === è¾“å…¥å¤„ç† ===
        img = transformer.img_in(hidden_states)
        vec = transformer.time_in(timestep)
        txt = transformer.txt_in(text_states)
        
        # === Double Blocks (54å±‚) ===
        img_before_last_block = None
        txt_before_last_block = None
        
        for index, block in enumerate(transformer.double_blocks):
            # ğŸ”‘ åœ¨æœ€åä¸€ä¸ª block ä¹‹å‰ä¿å­˜ç¼“å­˜
            if index == num_double_blocks - 1:
                img_before_last_block = img
                txt_before_last_block = txt
            
            img, txt = block(img=img, txt=txt, vec=vec, ...)
        
        # === Final Layer ===
        img_seq_len = img.shape[1]
        output = transformer.final_layer(img, vec)
        output = transformer.unpatchify(output, ...)
        
        # è¿”å› output + ç¼“å­˜æ•°æ®ï¼ˆblock 52 ä¹‹åçš„çŠ¶æ€ï¼‰
        return (output, img_before_last_block, txt_before_last_block, vec, text_mask, freqs_cis)
```

### 6.3 CachedForwardModule å®ç°

```python
class CachedForwardModule(torch.nn.Module):
    """å°è£…ä½¿ç”¨ç¼“å­˜çš„ forward
    
    è·³è¿‡ block 0-52ï¼Œåªæ‰§è¡Œï¼š
    1. block 53ï¼ˆæœ€åä¸€ä¸ª double_blockï¼‰
    2. single_blocksï¼ˆå¦‚æœæœ‰ï¼‰
    3. final_layer
    """
    
    def __init__(self, transformer, extra_kwargs):
        super().__init__()
        self.transformer = transformer
        self.extra_kwargs = extra_kwargs
    
    def forward(self, hidden_states, timestep, ..., cached_img, cached_txt, cached_freqs_cis, ...):
        transformer = self.transformer
        
        # ğŸ”‘ é‡æ–°è®¡ç®— vecï¼ˆä¾èµ–å½“å‰ timestepï¼‰
        vec = transformer.time_in(timestep)
        if transformer.guidance_embed:
            vec = vec + transformer.guidance_in(guidance)
        ...
        
        # ğŸ”‘ ä½¿ç”¨ç¼“å­˜ï¼ˆblock 52 ä¹‹åçš„çŠ¶æ€ï¼‰
        img = cached_img
        txt = cached_txt
        
        # === æ‰§è¡Œæœ€åä¸€ä¸ª double_block (block 53) ===
        num_double_blocks = len(transformer.double_blocks)
        last_block_index = num_double_blocks - 1
        last_block = transformer.double_blocks[last_block_index]
        
        img, txt = last_block(
            img=img, txt=txt, vec=vec, freqs_cis=cached_freqs_cis, ...
        )
        
        # === Final Layer ===
        output = transformer.final_layer(img, vec)
        output = transformer.unpatchify(output, ...)
        
        return output
```

### 6.4 TPUDeepCache ç¼“å­˜ç®¡ç†

```python
class TPUDeepCache:
    """TPU å‹å¥½çš„ç¼“å­˜ç®¡ç†å™¨
    
    ç¼“å­˜ block 52 ä¹‹åçš„çŠ¶æ€ï¼Œç”¨äºè·³è¿‡ block 0-52
    """
    
    def __init__(self, cache_start_step, cache_end_step, cache_step_interval, total_steps):
        # è®¡ç®—éœ€è¦å®Œæ•´è®¡ç®—çš„æ­¥éª¤
        self.no_cache_steps = set(
            list(range(0, cache_start_step)) +                        # å‰æœŸ
            list(range(cache_start_step, cache_end_step, cache_step_interval)) +  # åˆ·æ–°ç‚¹
            list(range(cache_end_step, total_steps))                  # åæœŸ
        )
        
        # ç¼“å­˜å­˜å‚¨ï¼ˆblock 52 ä¹‹åçš„çŠ¶æ€ï¼‰
        self.cached_img = None
        self.cached_txt = None
        self._cached_vec = None
        self._cached_text_mask = None
        self._cached_freqs_cis = None  # ç”¨äº block 53
    
    def should_use_cache(self, step):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ç¼“å­˜"""
        return step not in self.no_cache_steps and self.cached_img is not None
    
    def update_cache(self, img, txt, vec, text_mask, freqs_cis):
        """æ›´æ–°ç¼“å­˜ï¼ˆä¿å­˜ block 52 ä¹‹åçš„çŠ¶æ€ï¼‰"""
        self.cached_img = img
        self.cached_txt = txt
        self._cached_vec = vec
        self._cached_text_mask = text_mask
        self._cached_freqs_cis = freqs_cis
    
    def get_cache(self):
        """è·å–ç¼“å­˜"""
        return self.cached_img, self.cached_txt, self._cached_vec, self._cached_text_mask, self._cached_freqs_cis
```

### 6.5 æ¨ç†å¾ªç¯é›†æˆ

```mermaid
flowchart TB
    subgraph æ¨ç†å¾ªç¯["æ¨ç†å¾ªç¯"]
        Start[Step i] --> Check{should_use_cache?}
        
        Check --> |"False"| Full["FullForwardModule"]
        Check --> |"True"| Cache["CachedForwardModule"]
        
        Full --> Update["update_cache()"]
        Update --> Output1[noise_pred]
        
        Cache --> Get["get_cache()"]
        Get --> Output2[noise_pred]
        
        Output1 --> Scheduler["scheduler.step()"]
        Output2 --> Scheduler
        
        Scheduler --> Next[Step i+1]
    end
```

```python
# æ¨ç†å¾ªç¯
for i in range(num_steps):
    if deep_cache.should_use_cache(i):
        # ğŸš€ ä½¿ç”¨ç¼“å­˜è·¯å¾„ï¼ˆè·³è¿‡ block 0-52ï¼Œåªæ‰§è¡Œ block 53 + final_layerï¼‰
        cached_img, cached_txt, vec, text_mask, cached_freqs_cis = deep_cache.get_cache()
        noise_pred = cached_forward_fn(
            latent_model_input, timestep,  # éœ€è¦é‡æ–°è®¡ç®— vec
            cached_img, cached_txt,
            transformer._cached_freqs_cos,
            transformer._cached_freqs_sin,
            text_mask,
            cached_freqs_cis,
        )
    else:
        # ğŸ“¦ å®Œæ•´ forward + æ›´æ–°ç¼“å­˜
        output = full_forward_fn(latent_model_input, timestep, ...)
        noise_pred, img_before_last, txt_before_last, vec, text_mask, freqs_cis = output
        # ä¿å­˜ block 52 ä¹‹åçš„çŠ¶æ€ï¼ˆç”¨äºè·³è¿‡ block 0-52ï¼‰
        deep_cache.update_cache(img_before_last, txt_before_last, vec, text_mask, freqs_cis)
    
    # Scheduler step
    latents = scheduler.step(noise_pred, t, latents)[0]
```

### 6.6 å…³é”®è®¾è®¡å†³ç­–

```mermaid
flowchart TB
    subgraph è®¾è®¡å†³ç­–
        D1["ä¸ºä»€ä¹ˆåˆ†ç¦»æ¨¡å—ï¼Ÿ"]
        D2["ä¸ºä»€ä¹ˆ Python å±‚åˆ†æ”¯ï¼Ÿ"]
        D3["ä¸ºä»€ä¹ˆç¼“å­˜ freqsï¼Ÿ"]
        D4["ä¸ºä»€ä¹ˆæ¸…é™¤é¢„çƒ­ç¼“å­˜ï¼Ÿ"]
        
        D1 --> A1["é¿å… JIT å†…æ¡ä»¶åˆ†æ”¯<br/>é¿å… PyTree åŒ¹é…é—®é¢˜"]
        D2 --> A2["Python æ¡ä»¶ä¸å‚ä¸ç¼–è¯‘<br/>å®Œå…¨ç»•è¿‡ XLA é™åˆ¶"]
        D3 --> A3["é¿å… tracer æ³„æ¼<br/>freqs ç‹¬ç«‹äº JIT è¿”å›å€¼"]
        D4 --> A4["warmup æ­¥éª¤çš„ç¼“å­˜æ— æ•ˆ<br/>æ­£å¼æ¨ç†éœ€è¦é‡æ–°å¡«å……"]
    end
```

---

## 7. æ€§èƒ½å¯¹æ¯”

### 7.1 æµ‹è¯•ç»“æœ

| é…ç½® | æ—  DeepCache | æœ‰ DeepCache | åŠ é€Ÿæ¯” |
|------|-------------|-------------|--------|
| 121å¸§, 50æ­¥ | ~350s | ~203s | **1.72x** |
| æ¯æ­¥æ—¶é—´ | ~7.0s | ~4.1s (avg) | - |
| Cache Hit | 0 | 25 (50%) | - |

### 7.2 æ—¶é—´åˆ†å¸ƒ

```mermaid
pie title 50æ­¥æ¨ç†æ—¶é—´åˆ†å¸ƒ (DeepCache)
    "å®Œæ•´ Forward (25æ­¥)" : 50
    "ç¼“å­˜ Forward (25æ­¥)" : 33
    "é¢„çƒ­ç¼–è¯‘" : 17
```

### 7.3 åŠ é€ŸåŸç†åˆ†æ

å®æµ‹åŠ é€Ÿ **1.72x**ï¼Œæ¥è¿‘ç†è®º 1.93xï¼ŒåŸå› ï¼š
- ç¼“å­˜æ­¥éª¤è·³è¿‡ block 0-52ï¼ˆ53å±‚ï¼‰ï¼Œåªæ‰§è¡Œ block 53 + final_layerï¼ˆ2å±‚ vs 55å±‚ï¼‰
- è·³è¿‡äº† 53 å±‚ double_blocks çš„è®¡ç®—
- æ¯ä¸ª double_block åŒ…å«å¤šä¸ª attention + MLP æ“ä½œ
- ä¿ç•™æœ€åä¸€ä¸ª block ç¡®ä¿è¾“å‡ºè´¨é‡

### 7.4 ä½¿ç”¨æ–¹æ³•

```bash
python stage2_transformer_flax_experimental_deepcache.py \
    --enable_cache \
    --cache_start_step 11 \
    --cache_end_step 45 \
    --cache_step_interval 4 \
    --video_length 121 \
    --num_inference_steps 50
```

---

## ğŸ“‹ æ€»ç»“

### å…³é”®å·®å¼‚å¯¹æ¯”

| æ–¹é¢ | GPU ç‰ˆæœ¬ | TPU ç‰ˆæœ¬ |
|------|----------|----------|
| æ¡ä»¶åˆ†æ”¯ | JIT å†… if/else | Python å±‚ if/else |
| æ¨¡å—ç»“æ„ | å•ä¸€æ¨¡å— + çŠ¶æ€ | ä¸¤ä¸ªç‹¬ç«‹æ¨¡å— |
| ç¼“å­˜ç®¡ç† | angelslim åº“ | è‡ªå®šä¹‰ TPUDeepCache |
| ç¼–è¯‘ | torch.compile | torchax.compile Ã— 2 |
| çŠ¶æ€ä¼ é€’ | infer_state å¯¹è±¡ | æ˜¾å¼å‚æ•°ä¼ é€’ |

### æ ¸å¿ƒç»éªŒ

1. **ä¸è¦åœ¨ JIT å†…åšæ¡ä»¶åˆ†æ”¯** - torchax/XLA ä¸æ”¯æŒ
2. **åˆ†ç¦»ç¼–è¯‘æ˜¯å…³é”®** - ä¸¤ä¸ªæ¨¡å—ç‹¬ç«‹ç¼–è¯‘ï¼Œé¿å… PyTree é—®é¢˜
3. **Python å±‚æ§åˆ¶æµ** - æ¡ä»¶åˆ¤æ–­æ”¾åœ¨ç¼–è¯‘èŒƒå›´å¤–
4. **æ˜¾å¼çŠ¶æ€ç®¡ç†** - ä¸ä¾èµ–å¯å˜çŠ¶æ€ï¼Œä½¿ç”¨å‡½æ•°å‚æ•°ä¼ é€’
5. **é¢„è®¡ç®—å¸¸é‡** - freqs ç­‰åœ¨ JIT å¤–é¢„è®¡ç®—å¹¶ç¼“å­˜

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [DeepCache è®ºæ–‡](https://arxiv.org/abs/2312.00858)
- [angelslim GitHub](https://github.com/horseee/DeepCache)
- [HunyuanVideo-1.5](https://github.com/Tencent/HunyuanVideo)
- [JAX JIT æ–‡æ¡£](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)