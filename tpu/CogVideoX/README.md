# CogVideoX TPU åŠ é€Ÿé¡¹ç›®

> **æœ€åæ›´æ–°**ï¼š2025-12-24
>
> ğŸ‰ **æœ€æ–°è¿›å±•**ï¼šStage2 DP=2 ä¼˜åŒ–ï¼æ¨ç†æ—¶é—´ä» 89.67s é™è‡³ **56.63s**ï¼ŒåŠ é€Ÿ **1.58x**ï¼

æœ¬é¡¹ç›®å®ç°äº† CogVideoX è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ Google Cloud TPU ä¸Šçš„é«˜æ€§èƒ½æ¨ç†ï¼Œé€šè¿‡ JAX + TorchAx/Flax å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œå†…å­˜ä¼˜åŒ–ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

CogVideoX æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæœ¬é¡¹ç›®å°†å…¶è¿ç§»åˆ° TPU å¹³å°ï¼Œåˆ©ç”¨ä»¥ä¸‹æŠ€æœ¯å®ç°é«˜æ•ˆæ¨ç†ï¼š

- **ä¸‰é˜¶æ®µæµæ°´çº¿**ï¼šText Encoder â†’ Transformer â†’ VAE Decoderï¼Œæ”¯æŒåˆ†é˜¶æ®µè°ƒè¯•å’Œä¼˜åŒ–
- **Splash Attention**ï¼šTPU åŸç”Ÿçš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒé•¿åºåˆ—å¤„ç†
- **åŒ VAE å®ç°**ï¼šTorchAx VAEï¼ˆå…¼å®¹æ€§å¥½ï¼‰å’Œ Flax VAEï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰
- **æ™ºèƒ½åˆ†ç‰‡**ï¼šTensor Parallel + Data Parallelï¼Œæ”¯æŒå¤š TPU å¹¶è¡Œ
- **BFloat16 ä¼˜åŒ–**ï¼šå…¨æµç¨‹ BF16 è®¡ç®—ï¼Œå‡å°‘å†…å­˜å ç”¨å¹¶æå‡æ€§èƒ½

```mermaid
flowchart LR
    subgraph "Stage 1"
        A[Text Prompt] --> B[T5 Encoder]
        B --> C[Embeddings]
    end
    
    subgraph "Stage 2"
        C --> D[Transformer<br/>+ Splash Attention]
        D --> E[Latents]
    end
    
    subgraph "Stage 3"
        E --> F{VAE Decoder}
        F -->|TorchAx| G1[2.37s]
        F -->|Flax| G2[1.30s ğŸ†]
    end
    
    G1 --> H[Video]
    G2 --> H
```

---

## ğŸš€ Quick Start

### 1. ç¯å¢ƒå®‰è£…

```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install huggingface-hub
pip install -U transformers datasets evaluate accelerate timm flax numpy
pip install torchax
pip install jax[tpu]
pip install tensorflow-cpu

# å®‰è£…è¾…åŠ©å·¥å…·
pip install sentencepiece
sudo apt install ffmpeg -y
pip install imageio[ffmpeg]
pip install tpu-info
pip install matplotlib
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

```bash
# è®¾ç½® Hugging Face ç¼“å­˜ç›®å½•ï¼ˆä½¿ç”¨å…±äº«å†…å­˜åŠ é€Ÿï¼‰
export HF_HOME=/dev/shm

# è®¾ç½® Hugging Face Token
export HF_TOKEN=<your HF_TOKEN>

# JAX ç¼–è¯‘ç¼“å­˜ï¼ˆåŠ é€Ÿé‡å¤è¿è¡Œï¼‰
export JAX_COMPILATION_CACHE_DIR=/dev/shm/jax_cache
```

### 3. å…‹éš†å¹¶å®‰è£…é¡¹ç›®

```bash
# å…‹éš† diffusers-tpu é¡¹ç›®ï¼ˆåŒ…å« TorchAx/Flax VAE å®ç°ï¼‰
git clone https://github.com/yangwhale/diffusers-tpu.git
cd diffusers-tpu
pip install -e .

# å…‹éš†æœ¬é¡¹ç›®
git clone https://github.com/yangwhale/gpu-tpu-pedia.git
cd gpu-tpu-pedia/tpu/CogVideoX/
```

### 4. è¿è¡Œè§†é¢‘ç”Ÿæˆ

**æ¨èï¼šä½¿ç”¨ä¸‰é˜¶æ®µæµæ°´çº¿**

```bash
cd generate_diffusers_torchax_staged

# é˜¶æ®µ1ï¼šæ–‡æœ¬ç¼–ç 
python stage1_text_encoder.py --prompt "A panda playing guitar"

# é˜¶æ®µ2ï¼šTransformer æ¨ç†
python stage2_transformer.py --num_inference_steps 10

# é˜¶æ®µ3ï¼šVAE è§£ç ï¼ˆæ¨è Flax ç‰ˆæœ¬ï¼‰
python stage3_vae_decoder_flax.py
```

**æˆ–è€…ï¼šä¸€ä½“åŒ–è„šæœ¬**

```bash
python generate_torchax.py
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
CogVideoX/
â”œâ”€â”€ README.md                           # æœ¬æ–‡æ¡£
â”œâ”€â”€ generate_torchax.py                 # ä¸€ä½“åŒ–ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ custom_splash_attention.py          # TPU Splash Attention å®ç°
â”‚
â””â”€â”€ generate_diffusers_torchax_staged/  # â­ ä¸‰é˜¶æ®µæµæ°´çº¿ï¼ˆæ¨èï¼‰
    â”œâ”€â”€ README.md                       # è¯¦ç»†ä½¿ç”¨è¯´æ˜
    â”œâ”€â”€ utils.py                        # å…±äº«å·¥å…·æ¨¡å—
    â”œâ”€â”€ stage1_text_encoder.py          # é˜¶æ®µ1ï¼šæ–‡æœ¬ç¼–ç 
    â”œâ”€â”€ stage2_transformer.py           # é˜¶æ®µ2ï¼šTransformer æ¨ç†
    â”œâ”€â”€ stage3_vae_decoder.py           # é˜¶æ®µ3ï¼šVAE (TorchAx)
    â”œâ”€â”€ stage3_vae_decoder_flax.py      # é˜¶æ®µ3ï¼šVAE (Flax) ğŸ†
    â”œâ”€â”€ TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md  # VAE æ·±åº¦å¯¹æ¯”æ–‡æ¡£
    â””â”€â”€ stage_outputs/                  # ä¸­é—´æ–‡ä»¶å­˜å‚¨
        â”œâ”€â”€ generation_config.json
        â”œâ”€â”€ stage1_embeddings.safetensors
        â”œâ”€â”€ stage2_latents.safetensors
        â””â”€â”€ output_video.mp4
```

---

## ğŸ¯ æ€§èƒ½åŸºå‡†

### æœ€æ–°æµ‹è¯•ç»“æœï¼ˆ2025-12-24ï¼‰

**æµ‹è¯•ç¯å¢ƒ**ï¼šTPU v6e-8 (8 chips)ï¼ŒCogVideoX1.5-5Bï¼Œ768Ã—1360Ã—81 å¸§

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         æ€§èƒ½å¯¹æ¯”æ€»è§ˆ                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       é˜¶æ®µ        â”‚  é¦–æ¬¡è¿è¡Œ  â”‚  åç»­è¿è¡Œ   â”‚  JIT ç¼–è¯‘  â”‚    å¤‡æ³¨      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stage 1: T5       â”‚    ~2s     â”‚    ~2s     â”‚     -      â”‚ CPU è¿è¡Œ     â”‚
â”‚ Stage 2: Trans    â”‚   ~194s    â”‚   ~57s     â”‚   ~137s    â”‚ 50æ­¥ DP=2 ğŸ† â”‚
â”‚ Stage 3: TorchAx  â”‚   ~129s    â”‚   0.65s    â”‚   ~126s    â”‚ VAE è§£ç      â”‚
â”‚ Stage 3: Flax     â”‚   ~245s    â”‚   1.30s    â”‚   ~244s    â”‚ VAE è§£ç      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ€»è®¡ (TorchAx)    â”‚   ~325s    â”‚   ~60s ğŸ†  â”‚     -      â”‚ æ¨èï¼       â”‚
â”‚ æ€»è®¡ (Flax)       â”‚   ~441s    â”‚   ~60s     â”‚     -      â”‚ å¤‡é€‰         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage 2 Transformer é…ç½®å¯¹æ¯”ï¼ˆ50 æ­¥æ¨ç†ï¼‰

| é…ç½® | Mesh | æ¯æ­¥æ—¶é—´ | 50æ­¥æ€»æ—¶é—´ | åŠ é€Ÿæ¯” |
|------|------|----------|-----------|--------|
| DP=1 | `dp=1, tp=8` | 1.79s | 89.67s | 1.0x |
| **DP=2** ğŸ† | `dp=2, tp=4` | **1.13s** | **56.63s** | **1.58x** |

> ğŸ’¡ **DP=2 æ›´å¿«çš„åŸå› **ï¼šCFG æ¨¡å¼ä¸‹æ­£å‘+è´Ÿå‘ prompt å¯å¹¶è¡Œå¤„ç†

### VAE è§£ç å™¨å¯¹æ¯”

| å®ç° | è§£ç æ—¶é—´ | ç‰¹ç‚¹ |
|------|----------|------|
| **TorchAx VAE** ğŸ† | **0.65s** | PyTorch å…¼å®¹ï¼Œç§»æ¤å®¹æ˜“ï¼Œç¼“å­˜åæœ€å¿« |
| Flax VAE | 1.30s | çº¯ JAXï¼Œé¦–æ¬¡ç¼–è¯‘åç¨³å®š |
| Flax VAE (æœªä¼˜åŒ–) | ~12s | æ—  JIT/åˆ†ç‰‡ |

> ğŸ“– è¯¦ç»†å¯¹æ¯”åˆ†æè¯·å‚é˜…ï¼š[`TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md`](generate_diffusers_torchax_staged/TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md)

### Stage 2 å†å²ä¼˜åŒ–è®°å½•ï¼ˆ10 æ­¥åŸºå‡†æµ‹è¯•ï¼‰

| # | ä¼˜åŒ–é¡¹ | DP | Block Size | æ¯æ­¥æ—¶é—´ | 10æ­¥æ€»æ—¶é—´ | ç›¸å¯¹åŸºçº¿ |
|---|--------|-----|------------|----------|-----------|---------|
| 1 | åŸºçº¿ç‰ˆæœ¬ | âœ— | åŸå§‹ | 4.04s | 40.54s | - |
| 2 | +sharding constraint | âœ— | åŸå§‹ | 3.93s | 39.43s | +2.7% |
| 3 | +DP | âœ“ | åŸå§‹ | 3.08s | 33.90s | +16.4% |
| 4 | +mesh é¡ºåºä¼˜åŒ– | âœ“ | åŸå§‹ | 2.75s | 30.26s | +25.4% |
| 5 | **æœ€ä¼˜é…ç½®** | âœ“ | Wan2.1 | **2.31s** | **25.36s** | **+37.4%** |

---

## ğŸ”§ ä¸‰é˜¶æ®µæµæ°´çº¿è¯¦è§£

### é˜¶æ®µ1ï¼šæ–‡æœ¬ç¼–ç 

```bash
python stage1_text_encoder.py \
  --prompt "A panda playing guitar in a bamboo forest" \
  --output_dir ./stage_outputs
```

**å‚æ•°**ï¼š
- `--prompt`: æ­£é¢æç¤ºè¯
- `--negative_prompt`: è´Ÿé¢æç¤ºè¯ï¼ˆå¯é€‰ï¼‰
- `--model_id`: æ¨¡å‹ IDï¼ˆé»˜è®¤ `zai-org/CogVideoX1.5-5B`ï¼‰

**è¾“å‡º**ï¼š
- `stage1_embeddings.safetensors`: prompt embeddings
- `generation_config.json`: ç”Ÿæˆé…ç½®

### é˜¶æ®µ2ï¼šTransformer æ¨ç†

```bash
python stage2_transformer.py \
  --input_dir ./stage_outputs \
  --num_inference_steps 10 \
  --height 768 \
  --width 1360 \
  --frames 81
```

**å‚æ•°**ï¼š
- `--num_inference_steps`: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 10ï¼‰
- `--height/width`: è§†é¢‘å°ºå¯¸ï¼ˆé»˜è®¤ 768Ã—1360ï¼‰
- `--frames`: è§†é¢‘å¸§æ•°ï¼ˆé»˜è®¤ 81ï¼‰
- `--guidance_scale`: CFG å¼•å¯¼å°ºåº¦ï¼ˆé»˜è®¤ 6.0ï¼‰

**è¾“å‡º**ï¼š
- `stage2_latents.safetensors`: ç”Ÿæˆçš„ latents

### é˜¶æ®µ3ï¼šVAE è§£ç 

**ğŸ† æ¨èï¼šFlax VAEï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰**

```bash
python stage3_vae_decoder_flax.py \
  --input_dir ./stage_outputs \
  --output_video ./stage_outputs/output_video.mp4 \
  --dp 1
```

**å¤‡é€‰ï¼šTorchAx VAEï¼ˆå…¼å®¹æ€§å¥½ï¼‰**

```bash
python stage3_vae_decoder.py \
  --input_dir ./stage_outputs \
  --output_video ./stage_outputs/output_video.mp4
```

**è¾“å‡º**ï¼š
- `output_video.mp4`: æœ€ç»ˆè§†é¢‘

---

## âš™ï¸ æ ¸å¿ƒæŠ€æœ¯

### 1. Splash Attention

TPU ä¸“ç”¨çš„é«˜æ•ˆæ³¨æ„åŠ›å®ç°ï¼Œæ”¯æŒé•¿åºåˆ—å¤„ç†ï¼š

```python
# é…ç½®å‚æ•°ï¼ˆWan2.1 ä¼˜åŒ–é…ç½®ï¼‰
BQSIZE = 3328           # Query å—å¤§å°
BKVSIZE = 2816          # Key/Value å—å¤§å°
BKVCOMPUTESIZE = 256    # KV è®¡ç®—å—å¤§å°
USE_K_SMOOTH = True     # K-smooth ä¼˜åŒ–
```

**ç‰¹æ€§**ï¼š
- å—çŠ¶è®¡ç®—ï¼Œé¿å… VMEM æº¢å‡º
- æ”¯æŒå±€éƒ¨çª—å£æ³¨æ„åŠ›
- K-smooth æŠ€æœ¯æå‡æ•°å€¼ç¨³å®šæ€§

### 2. æ™ºèƒ½æƒé‡åˆ†ç‰‡

æ”¯æŒå¤šç§å¹¶è¡Œæ¨¡å¼ï¼š

```mermaid
graph TB
    subgraph "åˆ†ç‰‡ç­–ç•¥"
        A[8 TPU Chips] --> B{åˆ†ç‰‡æ¨¡å¼}
        B --> C["TP only<br/>(tp=8, dp=1)"]
        B --> D["TP + DP<br/>(tp=4, dp=2)"]
        B --> E["DP only<br/>(tp=1, dp=8)"]
    end
```

```python
# Mesh é…ç½®
mesh = Mesh(devices, ("dp", "sp", "tp"))

# æƒé‡åˆ†ç‰‡
P(None, None, None, ("dp", "tp"), None)  # Width ç»´åº¦åˆ†ç‰‡
```

### 3. Flax VAE ä¼˜åŒ–

å…³é”®ä¼˜åŒ–ç‚¹ï¼š

1. **JIT ç¼–è¯‘**ï¼š`nnx.jit(flax_vae.decoder)` 
2. **TPU åˆ†ç‰‡**ï¼š`jax.lax.with_sharding_constraint()`
3. **Mesh ä¸Šä¸‹æ–‡**ï¼š`with mesh: decode(...)`
4. **å†…å­˜ä¼˜åŒ– GroupNorm**ï¼šä½¿ç”¨ `jnp.var()` æ›¿ä»£ `lax.square()`

```python
# æ ¸å¿ƒä¼˜åŒ–ä»£ç 
def _apply_sharding_constraint(inputs, is_nthwc=True):
    """åœ¨ Width ç»´åº¦åˆ†ç‰‡åˆ°å¤š TPU"""
    if is_nthwc:
        # Flax: (B, T, H, W, C) - W at index 3
        spec = P(None, None, None, ("dp", "tp"), None)
    else:
        # TorchAx: (B, C, T, H, W) - W at index 4
        spec = P(None, None, None, None, ("dp", "tp"))
    return jax.lax.with_sharding_constraint(inputs, spec)
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### 1. VMEM æº¢å‡º

**ç—‡çŠ¶**ï¼š`RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem`

**è§£å†³**ï¼šå‡å° Splash Attention å—å¤§å°
```python
BQSIZE = 1024        # ä» 3328 å‡å°
BKVSIZE = 512        # ä» 2816 å‡å°
```

### 2. HBM OOM

**ç—‡çŠ¶**ï¼š`Attempting to allocate X.XXG. That was not possible.`

**è§£å†³**ï¼š
1. æ£€æŸ¥æ˜¯å¦æ·»åŠ äº† TPU åˆ†ç‰‡çº¦æŸ
2. å‡å°‘è§†é¢‘å¸§æ•°æˆ–åˆ†è¾¨ç‡
3. ä½¿ç”¨ Data Parallel æ¨¡å¼åˆ†æ•£å†…å­˜

### 3. JIT ç¼–è¯‘æ…¢

**è§£å†³**ï¼šå¯ç”¨æŒä¹…åŒ–ç¼–è¯‘ç¼“å­˜
```python
jax.config.update("jax_compilation_cache_dir", "/dev/shm/jax_cache")
jax.config.update("jax_persistent_cache_min_entry_size_bytes", -1)
jax.config.update("jax_persistent_cache_min_compile_time_secs", 0)
```

### 4. ç»´åº¦ä¸åŒ¹é…

**æ³¨æ„**ï¼šTorchAx å’Œ Flax ä½¿ç”¨ä¸åŒçš„ç»´åº¦é¡ºåºï¼š
- TorchAx: `NCTHW` (Width @ index 4)
- Flax: `NTHWC` (Width @ index 3)

åˆ†ç‰‡ PartitionSpec å¿…é¡»ç›¸åº”è°ƒæ•´ï¼

---

## ğŸ“Š æ•°æ®æ ¼å¼è¯´æ˜

### Latents ç»´åº¦æ ¼å¼

| é˜¶æ®µ | æ ¼å¼ | è¯´æ˜ |
|------|------|------|
| Pipeline è¾“å‡º | `[B, T, C, H, W]` | Diffusers åŸå§‹æ ¼å¼ |
| Stage2 ä¿å­˜ | `[B, C, T, H, W]` | PyTorch æ ‡å‡†æ ¼å¼ |
| TorchAx VAE | `[B, C, T, H, W]` | NCTHW |
| Flax VAE | `[B, T, H, W, C]` | NTHWC |

### è§†é¢‘è¾“å‡ºæ ¼å¼

- `List[np.ndarray]`ï¼šæ¯å¸§ä¸º `float32`ï¼ŒèŒƒå›´ `[0, 1]`
- ä½¿ç”¨ `diffusers.utils.export_to_video` ä¿å­˜

---

##  ç›¸å…³èµ„æº

- **æ·±åº¦å¯¹æ¯”æ–‡æ¡£**ï¼š[`TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md`](generate_diffusers_torchax_staged/TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md)
- **ä¸‰é˜¶æ®µè¯¦ç»†è¯´æ˜**ï¼š[`generate_diffusers_torchax_staged/README.md`](generate_diffusers_torchax_staged/README.md)
- [CogVideoX å®˜æ–¹ä»“åº“](https://github.com/THUDM/CogVideo)
- [Diffusers TPU ç‰ˆæœ¬](https://github.com/yangwhale/diffusers-tpu)
- [JAX æ–‡æ¡£](https://jax.readthedocs.io/)
- [Flax æ–‡æ¡£](https://flax.readthedocs.io/)
- [TPU å¼€å‘æŒ‡å—](https://cloud.google.com/tpu/docs)

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªåŸå§‹é¡¹ç›®çš„è®¸å¯è¯ã€‚

## ğŸ™ è‡´è°¢

- CogVideoX / THUDM å›¢é˜Ÿ
- Hugging Face Diffusers å›¢é˜Ÿ
- Google JAX/Flax å›¢é˜Ÿ
- TPU ç¤¾åŒº

---

## ğŸ“ˆ æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | æ›´æ–°å†…å®¹ |
|------|----------|
| 2025-12-20 | Flax VAE ä¼˜åŒ–å®Œæˆï¼Œè§£ç æ—¶é—´ 12s â†’ 1.30sï¼Œæ¯” TorchAx å¿« 82% |
| 2025-12-18 | æ–°å¢ TorchAx VAE å®ç°ï¼Œè§£ç æ—¶é—´ 90s â†’ 2.4s |
| 2025-12-12 | Stage 2 ä¼˜åŒ–å®Œæˆï¼Œæ¨ç†æ—¶é—´é™ä½ 37.4% |
| 2025-11-04 | åˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒåŸºç¡€ TPU æ¨ç† |
