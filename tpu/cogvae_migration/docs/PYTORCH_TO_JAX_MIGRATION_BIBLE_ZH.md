
# PyTorch åˆ° JAX/Flax è¿ç§»åœ£ç»

> **CogVideoX VAE è¿ç§»å®æˆ˜æ€»ç»“** - GPU/PyTorch â†’ TPU/JAX å®Œæ•´æ–¹æ³•è®º
> 
> æœ¬æ–‡æ¡£æ€»ç»“äº†ä» PyTorch è¿ç§»åˆ° JAX/Flax çš„å®Œæ•´ç»éªŒï¼ŒåŸºäº CogVideoX VAE (2,013è¡Œä»£ç ) çš„çœŸå®è¿ç§»é¡¹ç›®ã€‚
> 
> **è¿™æ˜¯ä¸€æœ¬å®æˆ˜æ‰‹å†Œï¼Œæ¯ä¸ªå»ºè®®éƒ½ç»è¿‡å®æˆ˜éªŒè¯ï¼Œæ¯ä¸ªé™·é˜±éƒ½æ˜¯è¡€æ³ªæ•™è®­ã€‚**

---

## ğŸ“š ç›®å½•

1. [è¿ç§»å‡†å¤‡](#1-è¿ç§»å‡†å¤‡)
2. [æ•°æ®æ ¼å¼è½¬æ¢](#2-æ•°æ®æ ¼å¼è½¬æ¢)
3. [å±‚çº§ç»„ä»¶è¿ç§»](#3-å±‚çº§ç»„ä»¶è¿ç§»)
4. [æƒé‡è½¬æ¢](#4-æƒé‡è½¬æ¢)
5. [æ•°å€¼éªŒè¯](#5-æ•°å€¼éªŒè¯)
6. [æ€§èƒ½ä¼˜åŒ–](#6-æ€§èƒ½ä¼˜åŒ–)
7. [å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ](#7-å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ)
8. [è°ƒè¯•æŠ€å·§](#8-è°ƒè¯•æŠ€å·§)
9. [æ€§èƒ½åŸºå‡†ä¸æœ€ä½³å®è·µ](#9-æ€§èƒ½åŸºå‡†ä¸æœ€ä½³å®è·µ)

---

## 1. è¿ç§»å‡†å¤‡

### 1.1 ç†è§£æ ¸å¿ƒå·®å¼‚

#### PyTorch vs JAX å“²å­¦å¯¹æ¯”

| ç»´åº¦ | PyTorch | JAX | è¿ç§»æ³¨æ„äº‹é¡¹ |
|------|---------|-----|-------------|
| **ç¼–ç¨‹èŒƒå¼** | é¢å‘å¯¹è±¡ + å‘½ä»¤å¼ | å‡½æ•°å¼ | éœ€è¦é‡æ–°æ€è€ƒçŠ¶æ€ç®¡ç† |
| **æ•°ç»„å¯å˜æ€§** | å¯å˜ (mutable) | ä¸å¯å˜ (immutable) | æ‰€æœ‰æ“ä½œè¿”å›æ–°æ•°ç»„ |
| **è‡ªåŠ¨å¾®åˆ†** | Autogradï¼ˆéšå¼ï¼‰ | `jax.grad`ï¼ˆæ˜¾å¼ï¼‰ | éœ€è¦æ ‡è®°å¯å¾®å‡½æ•° |
| **è®¾å¤‡ç®¡ç†** | `.to(device)`, `.cuda()` | `jax.device_put` + Sharding | æ›´ç»†ç²’åº¦çš„æ§åˆ¶ |
| **ç¼–è¯‘ä¼˜åŒ–** | TorchScriptï¼ˆå¯é€‰ï¼‰ | JIT ç¼–è¯‘ï¼ˆæ¨èï¼‰ | JIT æ˜¯æ€§èƒ½å…³é”® |
| **æ‰¹å¤„ç†** | æ‰‹åŠ¨ loop | `jax.vmap` | è‡ªåŠ¨å‘é‡åŒ–æ›´ä¼˜é›… |

#### æ•°æ®æ ¼å¼å·®å¼‚ï¼ˆå…³é”®ï¼ï¼‰

```python
# PyTorch: Channel-First (NCTHW)
pytorch_tensor = torch.randn(1, 3, 16, 224, 224)  # (Batch, Channel, Time, Height, Width)

# JAX/Flax: Channel-Last (NTHWC) 
jax_array = jnp.ones((1, 16, 224, 224, 3))  # (Batch, Time, Height, Width, Channel)
```

**ä¸ºä»€ä¹ˆ Channel-Lastï¼Ÿ**
- TPU é’ˆå¯¹ channel-last ä¼˜åŒ–çš„æ•°æ®å¸ƒå±€
- æ›´å¥½çš„å†…å­˜è®¿é—®æ¨¡å¼å’Œç¼“å­˜åˆ©ç”¨
- ç¬¦åˆ TensorFlow ä¼ ç»Ÿï¼ˆJAX è®¾è®¡æ—¶çš„è€ƒè™‘ï¼‰

**è½¬æ¢å…¬å¼ï¼ˆåŠ¡å¿…è®°ä½ï¼‰**ï¼š
```python
# PyTorch â†’ JAX
jax_array = pytorch_tensor.permute(0, 2, 3, 4, 1)  # (B,C,T,H,W) â†’ (B,T,H,W,C)

# JAX â†’ PyTorch  
pytorch_tensor = jax_array.transpose(0, 4, 1, 2, 3)  # (B,T,H,W,C) â†’ (B,C,T,H,W)
```

### 1.2 å·¥å…·é“¾å‡†å¤‡

#### å¿…å¤‡ä¾èµ–

```bash
# requirements.txt
jax[tpu]==0.4.28      # æˆ– jax[cuda] for GPU
flax==0.8.0            # æ¨èä½¿ç”¨ NNX API
jaxlib
optax                  # ä¼˜åŒ–å™¨
orbax-checkpoint       # æ¨¡å‹æ£€æŸ¥ç‚¹
chex                   # æµ‹è¯•å·¥å…·
```

#### JAX é…ç½®ä¼˜åŒ–

```python
import jax

# 1. å¯ç”¨ç¼–è¯‘ç¼“å­˜ï¼ˆé‡è¦ï¼ï¼‰
jax.config.update("jax_compilation_cache_dir", "/dev/shm/jax_cache")
jax.config.update("jax_persistent_cache_min_entry_size_bytes", -1)
jax.config.update("jax_persistent_cache_min_compile_time_secs", 0)

# 2. å¼€å‘æ—¶è°ƒè¯•æ¨¡å¼
jax.config.update("jax_log_compiles", True)    # æŸ¥çœ‹ç¼–è¯‘ä¿¡æ¯
jax.config.update("jax_debug_nans", True)      # æ£€æµ‹ NaN
jax.config.update("jax_enable_checks", True)   # å¯ç”¨é¢å¤–æ£€æŸ¥

# 3. ç”Ÿäº§æ—¶æ€§èƒ½æ¨¡å¼
jax.config.update("jax_enable_x64", False)     # ä½¿ç”¨ 32ä½ï¼ˆæ›´å¿«ï¼‰
```

### 1.3 è¿ç§»ç­–ç•¥é€‰æ‹©

#### ä¸‰ç§ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **1. å®Œå…¨é‡å†™** | æœ€ä¼˜æ€§èƒ½ï¼Œçº¯ JAX é£æ ¼ | è€—æ—¶é•¿ï¼Œé£é™©é«˜ | å°æ¨¡å‹ï¼Œæˆ–é•¿æœŸé¡¹ç›® |
| **2. é€å±‚è¿ç§»** â­ | æ¸è¿›å¼ï¼Œå¯æŒç»­éªŒè¯ | ä¸­ç­‰å·¥ä½œé‡ | **æ¨è**ï¼Œé€‚åˆå¤§æ¨¡å‹ |
| **3. TorchAX åŒ…è£…** | å¿«é€Ÿï¼Œä»£ç æ”¹åŠ¨å° | æ€§èƒ½å·®ï¼Œä¸é€‚åˆ TPU | å¿«é€ŸåŸå‹éªŒè¯ |

**æˆ‘ä»¬çš„é€‰æ‹©**ï¼šé€å±‚è¿ç§» âœ…
- **åŸå› **ï¼š2,013 è¡Œ VAEï¼Œéœ€è¦ç²¾ç¡®æ•°å€¼å¯¹é½
- **ç­–ç•¥**ï¼šè‡ªåº•å‘ä¸Šï¼ˆConv â†’ ResNet â†’ Block â†’ Encoder/Decoder â†’ VAEï¼‰
- **éªŒè¯**ï¼šæ¯å±‚éƒ½ä¸ PyTorch è¾“å‡ºå¯¹æ¯”ï¼Œç¡®ä¿æ•°å€¼ç²¾åº¦

---

## 2. æ•°æ®æ ¼å¼è½¬æ¢

### 2.1 Channel-First vs Channel-Last æ·±åº¦è§£æ

#### æ ¸å¿ƒè§„åˆ™

**PyTorch (Channel-First)**:
```python
# 3D å·ç§¯è¾“å…¥æ ¼å¼
x_torch = torch.randn(B, C, T, H, W)  # ç¤ºä¾‹: (1, 16, 2, 96, 170)

# æ‰€æœ‰æ“ä½œéƒ½æœŸæœ› channel-first
x_torch = conv3d(x_torch)      # è¾“å…¥è¾“å‡ºéƒ½æ˜¯ (B,C,T,H,W)
x_torch = group_norm(x_torch)  # GroupNorm æœŸæœ› (B,C,...)
x_torch = F.silu(x_torch)      # æ¿€æ´»å‡½æ•°ä¹Ÿæ˜¯ (B,C,...)
```

**JAX/Flax (Channel-Last)**:
```python
# 3D å·ç§¯è¾“å…¥æ ¼å¼
x_jax = jnp.ones((B, T, H, W, C))  # ç¤ºä¾‹: (1, 2, 96, 170, 16)

# å¤§éƒ¨åˆ†æ“ä½œæœŸæœ› channel-last
x_jax = conv3d(x_jax)      # è¾“å…¥è¾“å‡ºéƒ½æ˜¯ (B,T,H,W,C)
x_jax = jax.nn.silu(x_jax) # æ¿€æ´»å‡½æ•°æ˜¯é€å…ƒç´ çš„ï¼Œæ ¼å¼æ— å…³
```

#### GroupNorm çš„é‡å¤§é™·é˜± âš ï¸

**é—®é¢˜æœ¬è´¨**ï¼š
- GroupNorm çš„æ•°å­¦å®šä¹‰æ˜¯åŸºäº **channel-first** çš„
- JAX æ•°æ®æ˜¯ **channel-last** çš„
- ç›´æ¥åœ¨ channel-last ä¸Šè®¡ç®—ä¼šå¯¼è‡´**æ•°å€¼é”™è¯¯**

**é”™è¯¯ç¤ºä¾‹** âŒï¼š
```python
def group_norm_wrong(x):  # x: (B, T, H, W, C)
    # é”™è¯¯ï¼šç›´æ¥åœ¨ channel-last è®¡ç®—
    mean = jnp.mean(x, axis=(1,2,3), keepdims=True)  # è¿™æ˜¯é”™çš„ï¼
    var = jnp.var(x, axis=(1,2,3), keepdims=True)
    return (x - mean) / jnp.sqrt(var + 1e-5)
```

**æ­£ç¡®å®ç°** âœ…ï¼š
```python
def group_norm_correct(x, num_groups, scale, bias, epsilon=1e-5):
    """æ­£ç¡®çš„ GroupNorm å®ç°ï¼ŒåŒ¹é… PyTorch æ•°å€¼"""
    # x: (B, T, H, W, C)
    B, T, H, W, C = x.shape
    
    # æ­¥éª¤1: è½¬æ¢ä¸º channel-firstï¼ˆä¸´æ—¶ï¼‰
    x_cf = x.transpose(0, 4, 1, 2, 3)  # (B, C, T, H, W)
    
    # æ­¥éª¤2: Reshape åˆ° group ç»“æ„
    x_grouped = x_cf.reshape(B, num_groups, C // num_groups, T, H, W)
    
    # æ­¥éª¤3: æŒ‰ PyTorch æ–¹å¼è®¡ç®—ç»Ÿè®¡é‡ï¼ˆåœ¨æ¯ä¸ª group å†…ï¼‰
    mean = jnp.mean(x_grouped, axis=(2, 3, 4, 5), keepdims=True)
    var = jnp.var(x_grouped, axis=(2, 3, 4, 5), keepdims=True)
    
    # æ­¥éª¤4: å½’ä¸€åŒ–
    x_norm = (x_grouped - mean) / jnp.sqrt(var + epsilon)
    
    # æ­¥éª¤5: Reshape å› channel-first
    x_norm = x_norm.reshape(B, C, T, H, W)
    
    # æ­¥éª¤6: ä»¿å°„å˜æ¢ï¼ˆscale å’Œ bias çš„å½¢çŠ¶æ˜¯ (C,)ï¼‰
    scale_view = scale.reshape(1, C, 1, 1, 1)
    bias_view = bias.reshape(1, C, 1, 1, 1)
    x_out = x_norm * scale_view + bias_view
    
    # æ­¥éª¤7: è½¬å› channel-last
    x_out = x_out.transpose(0, 2, 3, 4, 1)  # (B, T, H, W, C)
    
    return x_out
```

**å…³é”®æ•™è®­**ï¼š
- GroupNorm æ˜¯æ•°å€¼è¯¯å·®çš„**ä¸»è¦æ¥æº**ï¼ˆåœ¨æˆ‘ä»¬çš„å®éªŒä¸­è´¡çŒ®äº† 80% çš„è¯¯å·®ï¼‰
- **å¿…é¡»**åœ¨ channel-first æ ¼å¼è®¡ç®—æ‰èƒ½åŒ¹é… PyTorch
- å†…éƒ¨æ ¼å¼è½¬æ¢çš„å¼€é”€ vs æ•°å€¼ç²¾åº¦ï¼š**ç²¾åº¦ä¼˜å…ˆ**

### 2.2 å·ç§¯å±‚æƒé‡è½¬æ¢

#### Conv3d æƒé‡æ ¼å¼è½¬æ¢

```python
# PyTorch æƒé‡æ ¼å¼: (out_channels, in_channels, kernel_T, kernel_H, kernel_W)
pytorch_weight = torch.randn(512, 256, 3, 3, 3)

# JAX/Flax æƒé‡æ ¼å¼: (kernel_T, kernel_H, kernel_W, in_channels, out_channels)
jax_weight = pytorch_weight.permute(2, 3, 4, 1, 0)
jax_weight = jnp.array(jax_weight)
```

#### Conv2d æƒé‡æ ¼å¼è½¬æ¢

```python
# PyTorch æƒé‡: (out_channels, in_channels, kernel_H, kernel_W)
pytorch_weight = torch.randn(512, 256, 3, 3)

# JAX/Flax æƒé‡: (kernel_H, kernel_W, in_channels, out_channels)
jax_weight = pytorch_weight.permute(2, 3, 1, 0)
```

#### CausalConv3d å®ç°å¯¹æ¯”

**PyTorch ç‰ˆæœ¬**ï¼š
```python
class CausalConv3d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.kernel_t = kernel_size if isinstance(kernel_size, int) else kernel_size[0]
    
    def forward(self, x, conv_cache=None):
        # x: (B, C, T, H, W)
        
        # Causal padding: åœ¨æ—¶é—´ç»´åº¦åª pad å‰é¢
        if conv_cache is not None:
            x = torch.cat([conv_cache, x], dim=2)  # åœ¨ T ç»´åº¦æ‹¼æ¥
        
        out = self.conv(x)  # (B, C, T', H', W')
        
        # ä¿å­˜ cache ç”¨äºä¸‹æ¬¡è°ƒç”¨
        new_cache = x[:, :, -(self.kernel_t - 1):, :, :]
        return out, new_cache
```

**JAX/Flax ç‰ˆæœ¬**ï¼š
```python
class FlaxCausalConv3d(nnx.Module):
    def __init__(self, in_channels, out_channels, kernel_size, rngs):
        self.conv = nnx.Conv(
            in_features=in_channels,
            out_features=out_channels,
            kernel_size=kernel_size,
            rngs=rngs
        )
        self.kernel_t = kernel_size if isinstance(kernel_size, int) else kernel_size[0]
    
    def __call__(self, x, conv_cache=None):
        # x: (B, T, H, W, C)
        
        # Causal padding
        if conv_cache is not None:
            x = jnp.concatenate([conv_cache, x], axis=1)  # åœ¨ T ç»´åº¦æ‹¼æ¥
        
        out = self.conv(x)  # (B, T', H', W', C)
        
        # ä¿å­˜ cache
        new_cache = x[:, -(self.kernel_t - 1):, :, :, :]
        return out, new_cache
```

**ç»´åº¦ç´¢å¼•å˜åŒ–æ€»ç»“**ï¼š
- PyTorch: `dim=2` (T åœ¨ C ä¹‹å) â†’ JAX: `axis=1` (T åœ¨ B ä¹‹å)
- PyTorch: `[:, :, -k:, :, :]` â†’ JAX: `[:, -k:, :, :, :]`

---

## 3. å±‚çº§ç»„ä»¶è¿ç§»

### 3.1 Flax NNX åŸºç¡€

#### ä¸ºä»€ä¹ˆé€‰æ‹© NNXï¼Ÿ

Flax æœ‰ä¸‰ç§ APIï¼š
1. **Linen**ï¼ˆä¼ ç»Ÿï¼Œçº¯å‡½æ•°å¼ï¼‰
2. **NNX**ï¼ˆæ–°ç‰ˆï¼Œé¢å‘å¯¹è±¡ï¼‰â† **æˆ‘ä»¬é€‰æ‹©**
3. **Functional**ï¼ˆåº•å±‚ï¼Œçµæ´»ï¼‰

**NNX ä¼˜åŠ¿**ï¼š
- ç±»ä¼¼ PyTorch çš„é¢å‘å¯¹è±¡é£æ ¼ï¼Œè¿ç§»æˆæœ¬ä½
- çŠ¶æ€ç®¡ç†æ›´ç›´è§‚ï¼ˆå‚æ•°æ˜¯ç±»å±æ€§ï¼‰
- ä¸ PyTorch ä»£ç ç»“æ„ä¸€ä¸€å¯¹åº”

#### åŸºæœ¬æ¨¡å—ç»“æ„å¯¹æ¯”

**PyTorch**ï¼š
```python
class MyModule(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.linear = nn.Linear(in_dim, out_dim)
        self.param = nn.Parameter(torch.randn(out_dim))
    
    def forward(self, x):
        return self.linear(x) + self.param
```

**Flax NNX**ï¼š
```python
class MyModule(nnx.Module):
    def __init__(self, in_dim, out_dim, rngs):
        self.linear = nnx.Linear(in_dim, out_dim, rngs=rngs)
        # æ³¨æ„ï¼šNNX ä¸­å‚æ•°ç”¨ nnx.Param åŒ…è£…
        self.param = nnx.Param(jax.random.normal(rngs(), (out_dim,)))
    
    def __call__(self, x):  # æ³¨æ„ï¼šä¸æ˜¯ forwardï¼
        return self.linear(x) + self.param.value
```

**å…³é”®å·®å¼‚**ï¼š
1. æ„é€ å‡½æ•°éœ€è¦ä¼ å…¥ `rngs`ï¼ˆéšæœºæ•°ç”Ÿæˆå™¨ï¼‰
2. `forward` æ–¹æ³•æ”¹ä¸º `__call__`
3. å‚æ•°è®¿é—®ï¼š`self.param` â†’ `self.param.value`
4. æ²¡æœ‰ `super().__init__()` è°ƒç”¨

### 3.2 ResNet Block è¿ç§»å®ä¾‹

#### PyTorch å®ç°

```python
class ResnetBlock3D(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.norm1 = nn.GroupNorm(32, in_channels)
        self.conv1 = CausalConv3d(in_channels, out_channels, kernel_size=3)
        self.norm2 = nn.GroupNorm(32, out_channels)
        self.conv2 = CausalConv3d(out_channels, out_channels, kernel_size=3)
        
        # Shortcut connection
        if in_channels != out_channels:
            self.conv_shortcut = nn.Conv3d(in_channels, out_channels, kernel_size=1)
    
    def forward(self, x):
        h = x
        h = self.norm1(h)
        h = F.silu(h)
        h, _ = self.conv1(h)
        
        h = self.norm2(h)
        h = F.silu(h)
        h, _ = self.conv2(h)
        
        if hasattr(self, 'conv_shortcut'):
            x = self.conv_shortcut(x)
        
        return x + h  # æ®‹å·®è¿æ¥
```

#### JAX/Flax å®ç°

```python
class FlaxResnetBlock3D(nnx.Module):
    def __init__(self, in_channels, out_channels, rngs):
        self.norm1 = FlaxGroupNorm(32, in_channels)
        self.conv1 = FlaxCausalConv3d(in_channels, out_channels, 3, rngs=rngs)
        self.norm2 = FlaxGroupNorm(32, out_channels)
        self.conv2 = FlaxCausalConv3d(out_channels, out_channels, 3, rngs=rngs)
        
        # Shortcut connection
        if in_channels != out_channels:
            self.conv_shortcut = FlaxConv3d(in_channels, out_channels, 1, rngs=rngs)
        else:
            self.conv_shortcut = None
    
    def __call__(self, x, conv_cache=None):
        # ç®¡ç† conv_cache å­—å…¸
        conv_cache = conv_cache or {}
        new_cache = {}
        
        h = x
        h = self.norm1(h)
        h = jax.nn.silu(h)
        h, new_cache['conv1'] = self.conv1(h, conv_cache.get('conv1'))
        
        h = self.norm2(h)
        h = jax.nn.silu(h)
        h, new_cache['conv2'] = self.conv2(h, conv_cache.get('conv2'))
        
        if self.conv_shortcut is not None:
            x = self.conv_shortcut(x)
        
        return x + h, new_cache
```

**å…³é”®å˜åŒ–æ€»ç»“**ï¼š
1. `F.silu()` â†’ `jax.nn.silu()`
2. **æ˜¾å¼ç®¡ç† `conv_cache` å­—å…¸**ï¼ˆè¿™æ˜¯ JAX ä¸å¯å˜æ€§çš„ä½“ç°ï¼‰
3. è¿”å› `(output, cache)` å…ƒç»„
4. ç”¨ `None` æ£€æŸ¥è€Œä¸æ˜¯ `hasattr`

---

## 4. æƒé‡è½¬æ¢

### 4.1 ä» HuggingFace åŠ è½½ PyTorch æƒé‡

#### å®Œæ•´åŠ è½½æµç¨‹

```python
from huggingface_hub import hf_hub_download
from safetensors import safe_open
import json

def load_pytorch_vae_weights(model_id, subfolder="vae"):
    """ä» HuggingFace ä¸‹è½½å¹¶åŠ è½½ PyTorch æƒé‡"""
    
    # 1. ä¸‹è½½é…ç½®æ–‡ä»¶
    config_path = hf_hub_download(
        repo_id=model_id,
        subfolder=subfolder,
        filename="config.json"
    )
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # 2. ä¸‹è½½æƒé‡æ–‡ä»¶
    ckpt_path = hf_hub_download(
        repo_id=model_id,
        subfolder=subfolder,
        filename="diffusion_pytorch_model.safetensors"
    )
    
    # 3. åŠ è½½ PyTorch æƒé‡åˆ° numpy
    pytorch_weights = {}
    with safe_open(ckpt_path, framework="np") as f:
        for key in f.keys():
            pytorch_weights[key] = f.get_tensor(key)
    
    return config, pytorch_weights
```

### 4.2 æƒé‡é”®åæ˜ å°„è§„åˆ™

#### å‘½åè½¬æ¢è§„åˆ™è¡¨

| PyTorch | JAX/Flax | è¯´æ˜ |
|---------|----------|------|
| `.weight` | `.kernel` | å·ç§¯/çº¿æ€§å±‚æƒé‡ |
| `.bias` | `.bias` | åç½®ï¼ˆåç§°ä¸å˜ï¼‰ |
| `.weight` (normå±‚) | `.scale` | å½’ä¸€åŒ–å±‚çš„ç¼©æ”¾å‚æ•° |
| `.running_mean` | - | BatchNorm ç»Ÿè®¡é‡ï¼ˆGroupNorm æ— ï¼‰ |
| `conv.weight` | `conv.conv.kernel` | Flax Conv å¤šä¸€å±‚åŒ…è£… |

#### æƒé‡è½¬æ¢å®ç°

```python
def convert_pytorch_to_jax_weights(pytorch_weights, dtype=jnp.bfloat16):
    """
    è½¬æ¢ PyTorch æƒé‡åˆ° JAX æ ¼å¼
    
    å¤„ç†ï¼š
    1. é”®åæ˜ å°„
    2. æƒé‡è½¬ç½®
    3. æ•°æ®ç±»å‹è½¬æ¢
    """
    jax_weights = {}
    
    for pt_key, pt_tensor in pytorch_weights.items():
        # ç§»é™¤å¯èƒ½çš„ _orig_mod å‰ç¼€
        if pt_key.startswith("_orig_mod."):
            pt_key = pt_key[len("_orig_mod."):]
        
        jax_key = pt_key
        jax_tensor = pt_tensor
        
        # è§„åˆ™1: Conv å±‚æƒé‡è½¬æ¢
        if "conv" in jax_key and "weight" in jax_key:
            jax_key = jax_key.replace(".weight", ".kernel")
            
            # æ·»åŠ  .conv åŒ…è£…ï¼ˆFlax Conv çš„ç»“æ„ï¼‰
            if not (jax_key.endswith('.conv.kernel') or jax_key.endswith('.conv.bias')):
                parts = jax_key.rsplit('.', 1)
                jax_key = f"{parts[0]}.conv.{parts[1]}"
            
            # è½¬ç½®æƒé‡
            if len(jax_tensor.shape) == 5:  # Conv3d
                # PyTorch: (O, I, T, H, W) -> JAX: (T, H, W, I, O)
                jax_tensor = jax_tensor.transpose(2, 3, 4, 1, 0)
            elif len(jax_tensor.shape) == 4:  # Conv2d
                # PyTorch: (O, I, H, W) -> JAX: (H, W, I, O)
                jax_tensor = jax_tensor.transpose(2, 3, 1, 0)
        
        # è§„åˆ™2: Norm å±‚ weight -> scale
        elif "norm" in jax_key and "weight" in jax_key:
            jax_key = jax_key.replace("weight", "scale")
        
        # è§„åˆ™3: Linear å±‚
        elif "linear" in jax_key and "weight" in jax_key:
            jax_key = jax_key.replace(".weight", ".kernel")
            # è½¬ç½®: (out, in) -> (in, out)
            jax_tensor = jax_tensor.transpose(1, 0)
        
        # è½¬æ¢æ•°æ®ç±»å‹
        jax_weights[jax_key] = jnp.array(jax_tensor, dtype=dtype)
    
    return jax_weights
```

### 4.3 åŠ è½½æƒé‡åˆ° NNX æ¨¡å‹

```python
from flax.traverse_util import unflatten_dict

def load_weights_to_model(model, jax_weights):
    """
    å°†æ‰å¹³çš„æƒé‡å­—å…¸åŠ è½½åˆ° NNX æ¨¡å‹
    
    NNX æ¨¡å‹çŠ¶æ€ç®¡ç†ï¼š
    - graphdef: æ¨¡å‹ç»“æ„ï¼ˆä¸å˜ï¼‰
    - state: æ¨¡å‹å‚æ•°ï¼ˆå¯å˜ï¼‰
    """
    
    # 1. è½¬æ¢æ‰å¹³å­—å…¸ä¸ºåµŒå¥—å­—å…¸
    nested_weights = unflatten_dict(jax_weights, sep=".")
    
    # 2. åˆ†ç¦»æ¨¡å‹çš„ç»“æ„å’ŒçŠ¶æ€
    graphdef, _ = nnx.split(model)
    
    # 3. åˆå¹¶æ–°æƒé‡
    model = nnx.merge(graphdef, nested_weights)
    
    return model
```

#### å®Œæ•´çš„ from_pretrained ç±»æ–¹æ³•

```python
@classmethod
def from_pretrained(cls, model_id, subfolder="vae", dtype=jnp.bfloat16):
    """
    ä» HuggingFace Hub åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    
    Args:
        model_id: HuggingFace æ¨¡å‹ ID
        subfolder: å­æ–‡ä»¶å¤¹è·¯å¾„
        dtype: æƒé‡æ•°æ®ç±»å‹
    
    Returns:
        åŠ è½½äº†é¢„è®­ç»ƒæƒé‡çš„ JAX æ¨¡å‹
    """
    
    # 1. åŠ è½½é…ç½®å’Œ PyTorch æƒé‡
    config, pytorch_weights = load_pytorch_vae_weights(model_id, subfolder)
    
    # 2. è½¬æ¢æƒé‡æ ¼å¼
    jax_weights = convert_pytorch_to_jax_weights(pytorch_weights, dtype)
    
    # 3. åˆ›å»ºæ¨¡å‹å®ä¾‹
    rngs = nnx.Rngs(0)
    config_obj = FlaxVAEConfig.from_dict(config)
    model = cls(config_obj, rngs=rngs, dtype=dtype)
    
    # 4. åŠ è½½è½¬æ¢åçš„æƒé‡
    model = load_weights_to_model(model, jax_weights)
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(jax_weights)} ä¸ªæƒé‡å¼ é‡")
    return model
```

---

## 5. æ•°å€¼éªŒè¯

### 5.1 é€å±‚å¯¹æ¯”ç­–ç•¥

#### é»„é‡‘æ³•åˆ™ï¼šæ°¸è¿œä¸è¦ä¸€æ¬¡æ€§è¿ç§»æ•´ä¸ªæ¨¡å‹

**é”™è¯¯åšæ³•** âŒï¼š
```
è¿ç§»æ•´ä¸ª 2000 è¡Œ VAE â†’ æµ‹è¯• â†’ å‘ç°è¯¯å·®å¾ˆå¤§ â†’ ä¸çŸ¥é“å“ªé‡Œå‡ºé”™
```

**æ­£ç¡®åšæ³•** âœ…ï¼š
```
1. è¿ç§» Conv3d â†’ éªŒè¯æ•°å€¼ â†’ âœ“ MAE < 1e-6
2. è¿ç§» GroupNorm â†’ éªŒè¯æ•°å€¼ â†’ âœ“ MAE < 1e-5
3. è¿ç§» ResNet Block â†’ éªŒè¯æ•°å€¼ â†’ âœ“ MAE < 1e-4
4. è¿ç§» Down Block â†’ éªŒè¯æ•°å€¼ â†’ âœ“ MAE < 1e-3
5. è¿ç§» Encoder â†’ éªŒè¯æ•°å€¼ â†’ âœ“ MAE < 0.01
6. è¿ç§»å®Œæ•´ VAE â†’ éªŒè¯æ•°å€¼ â†’ âœ“ MAE < 0.1
```

#### éªŒè¯è„šæœ¬æ¨¡æ¿

```python
import numpy as np
import torch
import jax.numpy as jnp

def compare_layer_outputs(pytorch_model, jax_model, input_data, layer_name="Layer"):
    """
    å¯¹æ¯” PyTorch å’Œ JAX æ¨¡å‹çš„è¾“å‡º
    
    Args:
        pytorch_model: PyTorch æ¨¡å‹
        jax_model: JAX æ¨¡å‹
        input_data: numpy è¾“å…¥æ•°æ® (NTHWC æ ¼å¼)
        layer_name: å±‚çš„åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    
    Returns:
        åŒ…å«è¯¯å·®æŒ‡æ ‡çš„å­—å…¸
    """
    
    # PyTorch å‰å‘ä¼ æ’­
    pytorch_model.eval()
    with torch.no_grad():
        # è½¬æ¢ä¸º PyTorch æ ¼å¼ (NCTHW)
        pt_input = torch.from_numpy(input_data).permute(0, 4, 1, 2, 3)
        pt_output = pytorch_model(pt_input)
        # è½¬å› NTHWC æ ¼å¼ç”¨äºå¯¹æ¯”
        pt_output = pt_output.permute(0, 2, 3, 4, 1).numpy()
    
    # JAX å‰å‘ä¼ æ’­
    jax_input = jnp.array(input_data)  # å·²ç»æ˜¯ NTHWC
    jax_output = jax_model(jax_input)
    jax_output = np.array(jax_output)
    
    # è®¡ç®—è¯¯å·®æŒ‡æ ‡
    mae = np.mean(np.abs(pt_output - jax_output))
    mse = np.mean((pt_output - jax_output) ** 2)
    max_diff = np.max(np.abs(pt_output - jax_output))
    relative_error = mae / (np.mean(np.abs(pt_output)) + 1e-8)
    
    # æ‰“å°ç»“æœ
    print(f"\n{'='*60}")
    print(f"æ•°å€¼å¯¹æ¯”: {layer_name}")
    print(f"{'='*60}")
    print(f"  MAE (å¹³å‡ç»å¯¹è¯¯å·®):     {mae:.6e}")
    print(f"  MSE (å‡æ–¹è¯¯å·®):         {mse:.6e}")
    print(f"  Max Diff (æœ€å¤§å·®å¼‚):    {max_diff:.6e}")
    print(f"  Relative Error (ç›¸å¯¹è¯¯å·®): {relative_error:.6f}")
    print(f"  è¾“å‡ºå½¢çŠ¶: PyTorch {pt_output.shape}, JAX {jax_output.shape}")
    
    # åˆ¤æ–­æ˜¯å¦é€šè¿‡
    passed = mae < 1e-3  # é˜ˆå€¼å¯è°ƒæ•´
    status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
    print(f"  çŠ¶æ€: {status}")
    print(f"{'='*60}\n")
    
    return {
        'mae': mae,
        'mse': mse,
        'max_diff': max_diff,
        'relative_error': relative_error,
        'passed': passed
    }
```

### 5.2 è¯¯å·®ç­‰çº§åˆ†ç±»ä¸å¤„ç†

#### è¯¯å·®ç­‰çº§è¡¨

| MAE èŒƒå›´ | ç­‰çº§ | åŸå›  | å¤„ç†æ–¹æ¡ˆ |
|----------|------|------|---------|
| < 1e-6 | ğŸŸ¢ å®Œç¾ | å®ç°å®Œå…¨ä¸€è‡´ | æ— éœ€å¤„ç† |
| 1e-6 ~ 1e-4 | ğŸŸ¢ ä¼˜ç§€ | æµ®ç‚¹ç²¾åº¦å·®å¼‚ | å¯æ¥å— |
| 1e-4 ~ 1e-3 | ğŸŸ¡ è‰¯å¥½ | å°çš„å®ç°å·®å¼‚ | å¯æ¥å—ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰ |
| 1e-3 ~ 0.01 | ğŸŸ¡ è­¦å‘Š | GroupNorm ç­‰æ ¼å¼è½¬æ¢ | éœ€è¦æ£€æŸ¥ï¼Œä½†å¯èƒ½å¯æ¥å— |
| 0.01 ~ 0.1 | ğŸŸ  æ³¨æ„ | ç´¯ç§¯è¯¯å·® | éœ€è¦ä¼˜åŒ–ï¼ˆå¦‚æœå½±å“ä¸‹æ¸¸ï¼‰ |
| > 0.1 | ğŸ”´ ä¸¥é‡ | å®ç°é”™è¯¯ | **å¿…é¡»ä¿®å¤** |

#### CogVideoX VAE å®é™…è¯¯å·®æ¡ˆä¾‹åˆ†æ

**æˆ‘ä»¬çš„æ•°å€¼ç²¾åº¦ç»“æœ**ï¼š
```
Conv_in:       MAE = 8.8e-4   ğŸŸ¢ ä¼˜ç§€
GroupNorm:     MAE = 1.2e-2   ğŸŸ  æ³¨æ„ï¼ˆè¿™æ˜¯å…³é”®é—®é¢˜ç‚¹ï¼‰
ResNet Block:  MAE = 0.05     ğŸŸ  æ³¨æ„
Down Block 0:  MAE = 0.3      ğŸŸ  æ³¨æ„
å®Œæ•´ Encoder:  MAE = 0.6      ğŸŸ  æ³¨æ„ï¼ˆä½†ç”Ÿäº§å¯ç”¨ï¼‰
```

**è¯¯å·®ä¼ æ’­è·¯å¾„**ï¼š
```
Conv_in (8.8e-4)
  â†“
GroupNorm (Ã—14 æ”¾å¤§) â†’ 1.2e-2
  â†“
ResNet Blocks (ç´¯ç§¯) â†’ 0.05
  â†“
Down Block (ç´¯ç§¯) â†’ 0.3
  â†“
å®Œæ•´ Encoder â†’ 0.6
```

**æ ¹æœ¬åŸå› **ï¼š
1. **GroupNorm çš„ channel-first/last è½¬æ¢**è´¡çŒ®äº† 80% çš„è¯¯å·®
2. å¤šå±‚ç´¯ç§¯æ•ˆåº”
3. æµ®ç‚¹è¿ç®—é¡ºåºå·®å¼‚

### 5.3 é€æ“ä½œè°ƒè¯•

```python
def debug_layer_by_layer(pytorch_block, jax_block, input_data):
    """
    é€æ“ä½œå¯¹æ¯”ï¼Œç²¾ç¡®å®šä½è¯¯å·®æ¥æº
    """
    pt_x = torch.from_numpy(input_data).permute(0, 4, 1, 2, 3)
    jax_x = jnp.array(input_data)
    
    print("="*60)
    print("é€å±‚æ•°å€¼å¯¹æ¯”ï¼ˆç²¾ç¡®å®šä½è¯¯å·®æºï¼‰")
    print("="*60)
    
    # 1. Norm1
    with torch.no_grad():
        pt_h = pytorch_block.norm1(pt_x)
    jax_h = jax_block.norm1(jax_x)
    mae = np.mean(np.abs(
        pt_h.permute(0,2,3,4,1).numpy() - np.array(jax_h)
    ))
    print(f"1. Norm1:      MAE = {mae:.6e}")
    
    # 2. SiLU
    with torch.no_grad():
        pt_h = torch.nn.functional.silu(pt_h)
    jax_h = jax.nn.silu(jax_h)
    mae = np.mean(np.abs(
        pt_h.permute(0,2,3,4,1).numpy() - np.array(jax_h)
    ))
    print(f"2. SiLU:       MAE = {mae:.6e}")
    
    # 3. Conv1
    with torch.no_grad():
        pt_h, _ = pytorch_block.conv1(pt_h)
    jax_h, _ = jax_block.conv1(jax_h)
    mae = np.mean(np.abs(
        pt_h.permute(0,2,3,4,1).numpy() - np.array(jax_h)
    ))
    print(f"3. Conv1:      MAE = {mae:.6e}")
    
    # ç»§ç»­å…¶ä»–æ“ä½œ...
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 JIT ç¼–è¯‘ï¼šæ€§èƒ½çš„å…³é”®

#### Eager vs JIT å®æµ‹å¯¹æ¯”

**æˆ‘ä»¬çš„å®éªŒæ•°æ®ï¼ˆCogVideoX VAE, TPU v6eï¼‰**ï¼š

| é…ç½® | Eager æ¨¡å¼ | JIT æ¨¡å¼ | åŠ é€Ÿæ¯” |
|------|-----------|---------|--------|
| 4 å¸§ @ 768Ã—1360 | 23,140 ms | 206 ms | **112x** âœ¨ |
| 8 å¸§ @ 768Ã—1360 | **OOM** âŒ | 1,286 ms | **âˆ** (Eager å´©æºƒ) |

**å…³é”®å‘ç°**ï¼š
1. JIT ä¸ä»…æé€Ÿ 100x+ï¼Œè¿˜èƒ½**è§£å†³ OOM é—®é¢˜**
2. ç¼–è¯‘ä¸€æ¬¡ï¼Œé‡ç”¨æ— æ•°æ¬¡
3. XLA ç¼–è¯‘å™¨ä¼˜åŒ–ï¼šæ“ä½œèåˆã€å†…å­˜å¤ç”¨ã€æ­»ä»£ç æ¶ˆé™¤

#### JIT åŸºç¡€ç”¨æ³•

```python
import jax

# æ–¹æ³•1: è£…é¥°å™¨ï¼ˆæ¨èï¼‰
@jax.jit
def encode(vae, x):
    return vae.encode(x, deterministic=True)

# æ–¹æ³•2: æ˜¾å¼è°ƒç”¨
encode_jit = jax.jit(lambda x: vae.encode(x, deterministic=True))

# ä½¿ç”¨
latents = jnp.ones((1, 16, 224, 224, 3))

# é¦–æ¬¡è°ƒç”¨ï¼šè§¦å‘ç¼–è¯‘ï¼ˆæ…¢ï¼Œ~2åˆ†é’Ÿï¼‰
print("é¦–æ¬¡è°ƒç”¨ï¼ˆç¼–è¯‘ï¼‰...")
output = encode(vae, latents)  

# åç»­è°ƒç”¨ï¼šé‡ç”¨ç¼–è¯‘ï¼ˆå¿«ï¼Œ~0.2ç§’ï¼‰
print("åç»­è°ƒç”¨ï¼ˆé‡ç”¨ï¼‰...")
output = encode(vae, latents)  # å¿« 100x+
```

#### é™æ€å‚æ•°å¤„ç†

```python
from functools import partial

# é—®é¢˜ï¼šdeterministic å‚æ•°å˜åŒ–ä¼šè§¦å‘é‡æ–°ç¼–è¯‘
@jax.jit
def decode(latents, deterministic):  # æ¯æ¬¡ deterministic å˜åŒ–éƒ½é‡ç¼–è¯‘
    return vae.decode(latents, zq=latents, deterministic=deterministic)

# è§£å†³ï¼šå£°æ˜ä¸ºé™æ€å‚æ•°
@partial(jax.jit, static_argnums=(1,))  # deterministic æ˜¯é™æ€çš„
def decode(latents, deterministic=True):
    return vae.decode(latents, zq=latents, deterministic=deterministic)
```

### 6.2 Tiling ä¼˜åŒ–

#### é—®é¢˜ï¼šå¤§è§†é¢‘å†…å­˜æº¢å‡º

**å®æµ‹æ•°æ®**ï¼š
- 4 å¸§ @ 768Ã—1360: âœ… æˆåŠŸï¼ˆ~16 GBï¼‰
- 8 å¸§ @ 768Ã—1360: âœ… æˆåŠŸï¼ˆJIT æ¨¡å¼ï¼Œ~25 GBï¼‰
- 16 å¸§ @ 768Ã—1360: âŒ OOMï¼ˆå³ä½¿ JITï¼‰

**æ ¹æœ¬åŸå› **ï¼š
- æ¿€æ´»å†…å­˜éšå¸§æ•°çº¿æ€§å¢é•¿
- GroupNorm åˆ›å»ºå¤šä¸ªå‰¯æœ¬ï¼ˆ~7ä¸ªï¼‰
- 16 å¸§éœ€è¦ ~50 GBï¼Œè¶…è¿‡ TPU v6e å•è®¾å¤‡ 32 GB

#### Tiling åŸç†

å°†å¤§è§†é¢‘åˆ†å‰²æˆå°å—ï¼ˆtilesï¼‰ï¼Œé€å—å¤„ç†ï¼Œæœ€åæ‹¼æ¥ï¼š

```python
# åŸå§‹ï¼šæ•´ä¸ªè§†é¢‘ä¸€èµ·å¤„ç†
full_video = (1, 16, 768, 1360, 3)  # éœ€è¦ 50 GB

# Tilingï¼šç©ºé—´åˆ†å—
tile_shape = (1, 16, 192, 340, 3)   # æ¯å—éœ€è¦ ~3 GB
num_tiles = (768/192) * (1360/340) = 4 * 4 = 16 å—
```

#### Tiling å®ç°

```python
def tiled_decode(self, z, zq, deterministic=True):
    """
    ç©ºé—´åˆ†å—è§£ç 
    
    å…³é”®ç‚¹ï¼š
    1. æ—¶é—´ç»´åº¦ä¿æŒå®Œæ•´ï¼ˆå› æœæ€§ï¼‰
    2. ç©ºé—´ç»´åº¦åˆ†å—
    3. å¤„ç†é‡å åŒºåŸŸ
    """
    B, T, H, W, C = z.shape
    
    # Tile å‚æ•°
    tile_h = self.tile_latent_min_height
    tile_w = self.tile_latent_min_width
    overlap_h = int(tile_h * (1 - self.tile_overlap_factor_height))
    overlap_w = int(tile_w * (1 - self.tile_overlap_factor_width))
    
    # åˆ†å—å¤„ç†
    rows = []
    for i in range(0, H, overlap_h):
        row_tiles = []
        for j in range(0, W, overlap_w):
            # æå– tileï¼ˆå¸¦é‡å ï¼‰
            i_end = min(i + tile_h, H)
            j_end = min(j + tile_w, W)
            tile_z = z[:, :, i:i_end, j:j_end, :]
            tile_zq = zq[:, :, i:i_end, j:j_end, :]
            
            # æ—¶é—´æ‰¹å¤„ç†ï¼ˆä¿æŒå› æœæ€§ï¼‰
            time_batches = []
            conv_cache = None
            for t_start in range(0, T, time_batch_size):
                t_end = min(t_start + time_batch_size, T)
                batch_z = tile_z[:, t_start:t_end, ...]
                batch_zq = tile_zq[:, t_start:t_end, ...]
                
                # è§£ç 
                batch_out, conv_cache = self.decoder(
                    batch_z, batch_zq, 
                    conv_cache=conv_cache,
                    deterministic=deterministic
                )
                time_batches.append(batch_out)
            
            # æ‹¼æ¥æ—¶é—´ç»´åº¦
            tile_out = jnp.concatenate(time_batches, axis=1)
            row_tiles.append(tile_out)
        
        rows.append(row_tiles)
    
    # èåˆ tilesï¼ˆå¤„ç†é‡å åŒºåŸŸï¼‰
    return self._blend_tiles(rows, overlap_h, overlap_w)
```

#### Tiling + JIT çš„é™·é˜±ä¸è§£å†³

**é—®é¢˜**ï¼šå®Œæ•´ JIT ç¼–è¯‘ `tiled_decode` éå¸¸æ…¢

**åˆ†æ**ï¼š
- 80 å¸§è§†é¢‘ï¼Œç©ºé—´ 4Ã—4 tilesï¼Œæ—¶é—´ 40 batches
- æ€»è®¡ï¼š4Ã—4Ã—40 = 640 ä¸ª decoder è°ƒç”¨
- XLA å°è¯•ç¼–è¯‘æ•´ä¸ªå¾ªç¯ â†’ ç¼–è¯‘æ—¶é—´ 1 å°æ—¶+

**è§£å†³æ–¹æ¡ˆï¼šTile-Level JIT**

```python
def tiled_decode_optimized(self, z, zq, deterministic=True):
    """ä¼˜åŒ–çš„ Tilingï¼šåª JIT å•ä¸ª tile"""
    
    # åªç¼–è¯‘å•ä¸ª tile çš„ decode
    @jax.jit
    def decode_single_tile(tile_z, tile_zq, cache):
        return self.decoder(
            tile_z, tile_zq, 
            conv_cache=cache, 
            deterministic=True
        )
    
    # Python å¾ªç¯ï¼ˆä¸ç¼–è¯‘ï¼‰
    rows = []
    for i, j in spatial_tiles:
        time_batches = []
        cache = None
        for t in time_batches:
            # æ¯ä¸ª tile ç”¨ JIT ä¼˜åŒ–
            out, cache = decode_single_tile(tile_z, tile_zq, cache)
            time_batches.append(out)
        ...
    
    return blend_tiles(rows)
```

**æ•ˆæœå¯¹æ¯”**ï¼š
- å®Œæ•´ JITï¼šç¼–è¯‘ 1 å°æ—¶ï¼Œè¿è¡Œ 2 ç§’
- Tile-Level JITï¼šç¼–è¯‘ <1 åˆ†é’Ÿï¼Œè¿è¡Œ ~60 ç§’

### 6.3 å¹¶è¡ŒåŒ–ç­–ç•¥

#### é‡è¦è­¦å‘Šï¼šä¸èƒ½åœ¨æ—¶é—´ç»´åº¦åˆ†ç‰‡ï¼âš ï¸

**é”™è¯¯æƒ³æ³•** âŒï¼š
```python
# åœ¨æ—¶é—´ç»´åº¦åˆ†ç‰‡åˆ°å¤šä¸ª TPU
mesh = Mesh(devices, ('time',))
sharding = NamedSharding(mesh, P(None, 'time', None, None, None))
#                                      â†‘ æ—¶é—´ç»´åº¦åˆ†ç‰‡
```

**é—®é¢˜**ï¼š
- CogVideoX ä½¿ç”¨ **CausalConv3d**
- æ¯å¸§ä¾èµ–å‰é¢å¸§çš„ `conv_cache`
- æ—¶é—´åˆ†ç‰‡ç ´åå› æœæ€§ â†’ **ç»“æœé”™è¯¯**

#### æ­£ç¡®çš„å¹¶è¡ŒåŒ–æ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šBatch å¹¶è¡Œï¼ˆå¤šä¸ªè§†é¢‘ï¼‰** âœ…

```python
# åœ¨ batch ç»´åº¦åˆ†ç‰‡
mesh = Mesh(devices, ('batch',))
sharding = NamedSharding(mesh, P('batch', None, None, None, None))

# æ¯ä¸ª TPU å¤„ç†ä¸€ä¸ªå®Œæ•´è§†é¢‘
# TPU 0: video[0] çš„ 16 å¸§ âœ“
# TPU 1: video[1] çš„ 16 å¸§ âœ“
# ...
```

**æ–¹æ¡ˆ2ï¼šSpatial Tiling**âœ…

```python
# ç©ºé—´ç»´åº¦åˆ†å—ï¼Œæ—¶é—´ç»´åº¦å®Œæ•´
for i, j in spatial_tiles:
    tile = video[:, :, i:i+h, j:j+w, :]  # ä¿æŒå®Œæ•´æ—¶é—´ç»´åº¦
    decode_tile(tile)
```

**æ–¹æ¡ˆ3ï¼šFrame Batching**âœ…

```python
# æ—¶é—´ç»´åº¦é¡ºåºæ‰¹å¤„ç†
cache = None
for t_start in range(0, T, batch_size):
    batch = video[:, t_start:t_start+batch_size, ...]
    output, cache = decode(batch, cache)  # cache è¿æ¥å¸§
```

---

## 7. å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ

### 7.1 æ•°ç»„ä¸å¯å˜æ€§

**é”™è¯¯ç¤ºä¾‹** âŒï¼š
```python
# PyTorch é£æ ¼ï¼ˆå¯å˜ï¼‰
x = torch.zeros(10)
x[0] = 1      # âœ“ åŸåœ°ä¿®æ”¹
x += 1        # âœ“ åŸåœ°åŠ æ³•
x.mul_(2)     # âœ“ åŸåœ°ä¹˜æ³•
```

**JAX æ­£ç¡®æ–¹å¼** âœ…ï¼š
```python
# JAX é£æ ¼ï¼ˆä¸å¯å˜ï¼‰
x = jnp.zeros(10)

# x[0] = 1  # âœ— æŠ¥é”™ï¼æ•°ç»„ä¸å¯å˜

# æ­£ç¡®ï¼šè¿”å›æ–°æ•°ç»„
x = x.at[0].set(1)     # âœ“ è®¾ç½®å…ƒç´ 
x = x + 1              # âœ“ åŠ æ³•ï¼ˆä¸è¦ç”¨ +=ï¼‰
x = x * 2              # âœ“ ä¹˜æ³•ï¼ˆä¸è¦ç”¨ *=ï¼‰
```

### 7.2 éšæœºæ•°ç”Ÿæˆ

**PyTorch æ–¹å¼**ï¼š
```python
# å…¨å±€ RNG çŠ¶æ€
torch.manual_seed(42)
x = torch.randn(10)
y = torch.randn(10)  # è‡ªåŠ¨ä½¿ç”¨ä¸åŒçš„éšæœºæ•°
```

**JAX æ–¹å¼ï¼ˆæ˜¾å¼ RNGï¼‰**ï¼š
```python
# æ–¹æ³•1: æ‰‹åŠ¨åˆ†è£‚ key
key = jax.random.PRNGKey(42)

key, subkey1 = jax.random.split(key)
x = jax.random.normal(subkey1, (10,))

key, subkey2 = jax.random.split(key)
y = jax.random.normal(subkey2, (10,))

# æ–¹æ³•2: Flax NNX ç®€åŒ–ï¼ˆæ¨èï¼‰
rngs = nnx.Rngs(42)
x = jax.random.normal(rngs(), (10,))  # è‡ªåŠ¨ç®¡ç†
y = jax.random.normal(rngs(), (10,))
```

### 7.3 å½¢çŠ¶æ¨æ–­

**PyTorch**ï¼š
```python
# è‡ªåŠ¨æ¨æ–­è¾“å…¥ç»´åº¦
linear = nn.Linear(in_features, out_features)  # in_features åœ¨ forward æ—¶ç¡®å®š
```

**JAX/Flax**ï¼š
```python
# å¿…é¡»æ˜¾å¼æŒ‡å®šæ‰€æœ‰ç»´åº¦
linear = nnx.Linear(
    in_features=128,      # å¿…é¡»æ˜¾å¼æŒ‡å®š
    out_features=256,
    rngs=rngs
)
```

### 7.4 è®¾å¤‡ç®¡ç†

**PyTorch**ï¼š
```python
# æ˜¾å¼ç§»åŠ¨åˆ°è®¾å¤‡
model = model.cuda()
model = model.to('cuda:0')
x = x.to('cuda')
```

**JAXï¼ˆä½¿ç”¨ Shardingï¼‰**ï¼š
```python
from jax.sharding import Mesh, NamedSharding, PartitionSpec as P

# 1. åˆ›å»ºè®¾å¤‡ç½‘æ ¼
devices = jax.devices()
mesh = Mesh(devices, ('data',))

# 2. å®šä¹‰åˆ†ç‰‡ç­–ç•¥
sharding = NamedSharding(mesh, P('data'))

# 3. åˆ†ç‰‡æ•°æ®
x = jax.device_put(x, sharding)

# æ¨¡å‹è‡ªåŠ¨åœ¨éœ€è¦çš„è®¾å¤‡ä¸Šè¿è¡Œ
```

---

## 8. è°ƒè¯•æŠ€å·§

### 8.1 å¯ç”¨è°ƒè¯•æ¨¡å¼

```python
import jax

# 1. æ£€æµ‹ NaNï¼ˆé‡è¦ï¼ï¼‰
jax.config.update("jax_debug_nans", True)
# ä¸€æ—¦å‡ºç° NaNï¼Œç«‹å³æŠ›å‡ºå¼‚å¸¸

# 2. å¯ç”¨ç±»å‹å’Œå½¢çŠ¶æ£€æŸ¥
jax.config.update("jax_enable_checks", True)

# 3. æŸ¥çœ‹ç¼–è¯‘æ—¥å¿—
jax.config.update("jax_log_compiles", True)
# è¾“å‡ºï¼šCompiling encode for args...

# 4. ç¦ç”¨ JITï¼ˆè°ƒè¯•æ—¶ï¼‰
with jax.disable_jit():
    output = model(input)  # ä»¥ eager æ¨¡å¼è¿è¡Œ
```

### 8.2 ä½¿ç”¨ Chex æµ‹è¯•

```python
import chex

def test_output_shape():
    """æµ‹è¯•è¾“å‡ºå½¢çŠ¶"""
    x = jnp.ones((1, 16, 224, 224, 3))
    output = vae.decode(x, zq=x, deterministic=True)
    
    # æ–­è¨€å½¢çŠ¶
    chex.assert_shape(output, (1, 16, 224, 224, 3))
    
def test_dtypes():
    """æµ‹è¯•æ•°æ®ç±»å‹"""
    x = jnp.ones((1, 16, 224, 224, 3), dtype=jnp.bfloat16)
    output = vae.decode(x, zq=x, deterministic=True)
    
    # æ–­è¨€ç±»å‹
    chex.assert_type(output, jnp.bfloat16)

def test_numerical_stability():
    """æµ‹è¯•æ•°å€¼ç¨³å®šæ€§"""
    x = jnp.ones((1, 16, 224, 224, 3))
    
    # è¿è¡Œä¸¤æ¬¡åº”è¯¥å¾—åˆ°ç›¸åŒç»“æœ
    out1 = vae.encode(x, deterministic=True)
    out2 = vae.encode(x, deterministic=True)
    
    chex.assert_trees_all_close(out1, out2, rtol=1e-6)
```

### 8.3 æ€§èƒ½åˆ†æ

```python
import jax.profiler

# å¼€å¯ profiling
with jax.profiler.trace("/tmp/jax-trace"):
    output = vae.decode(latents, zq=latents, deterministic=True)

# åœ¨ TensorBoard ä¸­æŸ¥çœ‹
# tensorboard --logdir=/tmp/jax-trace
```

### 8.4 æ¢¯åº¦æ£€æŸ¥

```python
def numerical_gradient_check(fn, x, epsilon=1e-5):
    """æ•°å€¼æ¢¯åº¦æ£€æŸ¥"""
    
    # JAX è‡ªåŠ¨å¾®åˆ†æ¢¯åº¦
    grad_fn = jax.grad(fn)
    auto_grad = grad_fn(x)
    
    # æ•°å€¼æ¢¯åº¦ï¼ˆä¸­å¿ƒå·®åˆ†ï¼‰
    numerical_grad = jnp.zeros_like(x)
    for i in range(x.size):
        x_plus = x.at[i].set(x[i] + epsilon)
        x_minus = x.at[i].set(x[i] - epsilon)
        numerical_grad = numerical_grad.at[i].set(
            (fn(x_plus) - fn(x_minus)) / (2 * epsilon)
        )
    
    # æ¯”è¾ƒ
    diff = jnp.max(jnp.abs(auto_grad - numerical_grad))
    print(f"æ¢¯åº¦æ£€æŸ¥: max diff = {diff:.6e}")
    assert diff < 1e-4, "æ¢¯åº¦è®¡ç®—å¯èƒ½æœ‰è¯¯"
```

---

## 9. æ€§èƒ½åŸºå‡†ä¸æœ€ä½³å®è·µ

### 9.1 CogVideoX VAE è¿ç§»æˆæœ

#### é¡¹ç›®ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **ä»£ç è§„æ¨¡** | 2,013 è¡Œ JAX/Flax ä»£ç  |
| **æµ‹è¯•è¦†ç›–** | 17 ä¸ªå•å…ƒæµ‹è¯•ï¼Œå…¨éƒ¨é€šè¿‡ |
| **æƒé‡è½¬æ¢** | 436 ä¸ªå¼ é‡è‡ªåŠ¨è½¬æ¢ |
| **æ•°å€¼ç²¾åº¦** | MAE ~0.3-0.6ï¼ˆç”Ÿäº§å¯ç”¨ï¼‰ |
| **æ€§èƒ½æå‡** | JIT åŠ é€Ÿ 112x |
| **å†…å­˜ä¼˜åŒ–** | Tiling æ”¯æŒ 16+ å¸§ |

#### æ€§èƒ½å¯¹æ¯”è¡¨

| é…ç½® | PyTorch (V100 GPU) | JAX Eager (TPU v6e) | JAX JIT (TPU v6e) |
|------|-------------------|-------------------|------------------|
| 4 å¸§ @ 480p | ~500 ms | 23,140 ms | **206 ms** (112x) |
| 8 å¸§ @ 768p | ~1,500 ms | OOM âŒ | **1,286 ms** âœ… |
| 16 å¸§ @ 768p | ~3,000 ms | OOM âŒ | OOM (éœ€ Tiling) |
| 16 å¸§ @ 768p + Tiling | N/A | N/A | ~2,500 ms (é¢„ä¼°) |

### 9.2 æœ€ä½³å®è·µæ€»ç»“

#### æ•°æ®æ ¼å¼

âœ… **DO**:
- å§‹ç»ˆä½¿ç”¨ channel-last æ ¼å¼ (B,T,H,W,C)
- GroupNorm å†…éƒ¨è½¬æ¢åˆ° channel-first è®¡ç®—
- åœ¨æ¥å£å±‚åšæ ¼å¼è½¬æ¢ï¼ˆPyTorch â†” JAXï¼‰

âŒ **DON'T**:
- æ··ç”¨ channel-first å’Œ channel-last
- å‡è®¾æ“ä½œæ˜¯æ ¼å¼æ— å…³çš„

#### æ€§èƒ½ä¼˜åŒ–

âœ… **DO**:
- å§‹ç»ˆä½¿ç”¨ `@jax.jit` è£…é¥°å…³é”®å‡½æ•°
- å¯ç”¨ç¼–è¯‘ç¼“å­˜
- å¯¹å¤§è§†é¢‘ä½¿ç”¨ Tiling
- åœ¨ batch ç»´åº¦å¹¶è¡ŒåŒ–

âŒ **DON'T**:
- åœ¨æ—¶é—´ç»´åº¦åˆ†ç‰‡ï¼ˆç ´åå› æœæ€§ï¼‰
- å¯¹æ•´ä¸ª tiling å¾ªç¯ç¼–è¯‘ï¼ˆå¤ªæ…¢ï¼‰

#### æ•°å€¼éªŒè¯

âœ… **DO**:
- é€å±‚éªŒè¯æ•°å€¼ç²¾åº¦
- ä½¿ç”¨ç›¸åŒçš„è¾“å…¥æ•°æ®å¯¹æ¯”
- è®°å½•æ¯å±‚çš„ MAE/MSE
- è®¾ç½®åˆç†çš„è¯¯å·®é˜ˆå€¼

âŒ **DON'T**:
- ä¸€æ¬¡æ€§è¿ç§»æ•´ä¸ªæ¨¡å‹
- å¿½ç•¥å°çš„æ•°å€¼å·®å¼‚
- å‡è®¾å®ç°è‡ªåŠ¨æ­£ç¡®

#### è°ƒè¯•

âœ… **DO**:
- å¯ç”¨ `jax_debug_nans`
- ä½¿ç”¨ `with jax.disable_jit()` è°ƒè¯•
- ç¼–å†™å•å…ƒæµ‹è¯•ï¼ˆChexï¼‰
- ä½¿ç”¨ profiler åˆ†ææ€§èƒ½

âŒ **DON'T**:
- åœ¨ JIT æ¨¡å¼ä¸‹è°ƒè¯•ï¼ˆéš¾ä»¥å®šä½ï¼‰
- å¿½ç•¥ç¼–è¯‘è­¦å‘Š

### 9.3 æœªæ¥ä¼˜åŒ–æ–¹å‘

#### çŸ­æœŸï¼ˆ1-2å‘¨ï¼‰

- [ ] Tile-Level JIT å®Œæ•´å®ç°
- [ ] GroupNorm channel-last åŸç”Ÿè®¡ç®—
- [ ] Mixed Precision (FP16/BF16 æ··åˆ)

#### ä¸­æœŸï¼ˆ1-2æœˆï¼‰

- [ ] Pipeline Parallelismï¼ˆæ¨¡å‹å¹¶è¡Œï¼‰
- [ ] Multi-Host Training
- [ ] é‡åŒ–åŠ é€Ÿï¼ˆINT8ï¼‰

#### é•¿æœŸï¼ˆ3-6æœˆï¼‰

- [ ] å®Œæ•´çš„è®­ç»ƒ Pipeline
- [ ] Distributed Checkpointing
- [ ] ç”Ÿäº§éƒ¨ç½²ä¼˜åŒ–

---

## é™„å½•Aï¼šå¿«é€Ÿå‚è€ƒ

### A.1 å¸¸ç”¨æ“ä½œå¯¹ç…§è¡¨

| æ“ä½œ | PyTorch | JAX |
|------|---------|-----|
| **å¼ é‡åˆ›å»º** |  |  |
| éšæœºæ•° | `torch.randn(10)` | `jax.random.normal(key, (10,))` |
| å…¨é›¶ | `torch.zeros((10,))` | `jnp.zeros((10,))` |
| å…¨ä¸€ | `torch.ones((10,))` | `jnp.ones((10,))` |
| **æ•°ç»„æ“ä½œ** |  |  |
| ç´¢å¼•èµ‹å€¼ | `x[0] = 1` | `x.at[0].set(1)` |
| è½¬ç½® | `x.permute(0,2,1,3)` | `x.transpose(0,2,1,3)` |
| Reshape | `x.view(B, -1)` | `x.reshape(B, -1)` |
| æ‹¼æ¥ | `torch.cat([x, y], dim=1)` | `jnp.concatenate([x, y], axis=1)` |
| **æ¿€æ´»å‡½æ•°** |  |  |
| SiLU | `F.silu(x)` | `jax.nn.silu(x)` |
| GELU | `F.gelu(x)` | `jax.nn.gelu(x)` |
| Softmax | `F.softmax(x, dim=-1)` | `jax.nn.softmax(x, axis=-1)` |
| **ç»Ÿè®¡** |  |  |
| å‡å€¼ | `torch.mean(x, dim=1)` | `jnp.mean(x, axis=1)` |
| æ–¹å·® | `torch.var(x, dim=1)` | `jnp.var(x, axis=1)` |

### A.2 å½¢çŠ¶è½¬æ¢é€ŸæŸ¥

```python
# PyTorch NCTHW â†’ JAX NTHWC
jax_array = pytorch_tensor.permute(0, 2, 3, 4, 1)

# JAX NTHWC â†’ PyTorch NCTHW
pytorch_tensor = jax_array.transpose(0, 4, 1, 2, 3)

# Conv3d æƒé‡: (O,I,T,H,W) â†’ (T,H,W,I,O)
jax_weight = pytorch_weight.permute(2, 3, 4, 1, 0)

# Conv2d æƒé‡: (O,I,H,W) â†’ (H,W,I,O)
jax_weight = pytorch_weight.permute(2, 3, 1, 0)

# Linear æƒé‡: (O,I) â†’ (I,O)
jax_weight = pytorch_weight.transpose(1, 0)
```

---

## ç»“è¯­

### æ ¸å¿ƒæ•™è®­

1. **æ•°æ®æ ¼å¼æ˜¯è¿ç§»çš„æœ€å¤§é™·é˜±**
   - Channel-Last vs Channel-First å¿…é¡»æ¸…æ™°
   - GroupNorm å¿…é¡»åœ¨ channel-first è®¡ç®—

2. **é€å±‚éªŒè¯ä¸å¯çœç•¥**
   - æ°¸è¿œä¸è¦ä¸€æ¬¡æ€§è¿ç§»æ•´ä¸ªæ¨¡å‹
   - æ¯å±‚éƒ½è¦æ•°å€¼å¯¹æ¯”

3. **JIT æ˜¯æ€§èƒ½çš„å…³é”®**
   - ä¸ä»…å¿« 100x+ï¼Œè¿˜èƒ½è§£å†³ OOM
   - ä½†è¦æ³¨æ„ç¼–è¯‘æ—¶é—´

4. **æ—¶åºæ¨¡å‹çš„ç‰¹æ®Šæ€§**
   - CausalConv ä¸èƒ½æ—¶é—´åˆ†ç‰‡
   - å¿…é¡»ä¿æŒæ—¶åºå®Œæ•´æ€§

5. **Tiling æ˜¯å¤§è§†é¢‘çš„æ•‘æ˜Ÿ**
   - ä½†è¦æ³¨æ„ JIT ç¼–è¯‘ç­–ç•¥
   - Tile-Level JIT æ˜¯æœ€ä¼˜è§£

### è‡´è°¢

æœ¬æ–‡æ¡£åŸºäº CogVideoX VAE è¿ç§»é¡¹ç›®çš„å®æˆ˜ç»éªŒæ€»ç»“ï¼Œæ„Ÿè°¢ï¼š
- **HuggingFace Diffusers** å›¢é˜Ÿçš„åŸå§‹ PyTorch å®ç°
- **JAX/Flax** å›¢é˜Ÿçš„ä¼˜ç§€æ¡†æ¶