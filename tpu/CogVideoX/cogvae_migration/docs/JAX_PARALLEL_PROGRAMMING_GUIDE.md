# JAX å¹¶è¡Œç¼–ç¨‹è¯¦è§£ï¼šshard_mapã€vmapã€pmap å®Œå…¨æŒ‡å—

> **å‰ç½®çŸ¥è¯†**ï¼šå¦‚æœä½ æ˜¯ JAX åˆå­¦è€…ï¼Œå»ºè®®å…ˆé˜…è¯» [PyTorch åˆ° JAX å…¥é—¨æ•™ç¨‹](../../torch_to_jax_jumpstart/)ï¼Œä»åŸºç¡€å¼€å§‹å­¦ä¹  JAX å’Œ HuggingFace æ¨¡å‹çš„ä½¿ç”¨ã€‚

## ç›®å½•

- [1. shard_map å·¥ä½œåŸç†](#1-shard_map-å·¥ä½œåŸç†)
- [2. shard_map vs ä¼ ç»Ÿæ–¹æ³•](#2-shard_map-vs-ä¼ ç»Ÿæ–¹æ³•)
- [3. è·¨åˆ†ç‰‡ä¾èµ–é—®é¢˜](#3-è·¨åˆ†ç‰‡ä¾èµ–é—®é¢˜)
- [4. JAX Map å‡½æ•°å®¶æ—](#4-jax-map-å‡½æ•°å®¶æ—)
- [5. vmap vs pmap è¯¦è§£](#5-vmap-vs-pmap-è¯¦è§£)
- [6. Mesh çš„é‡è¦æ€§](#6-mesh-çš„é‡è¦æ€§)
- [7. å®è·µå»ºè®®](#7-å®è·µå»ºè®®)

---

## 1. shard_map å·¥ä½œåŸç†

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

`shard_map` æ˜¯ JAX ä¸­ç”¨äº**æ˜¾å¼æ§åˆ¶æ•°æ®åœ¨å¤šè®¾å¤‡ä¸Šçš„åˆ†ç‰‡å’Œè®¡ç®—**çš„é«˜çº§å·¥å…·ã€‚

```python
from jax.experimental.shard_map import shard_map
from jax.sharding import PartitionSpec as P, NamedSharding, Mesh

sharded_decode = shard_map(
    f=decode_on_slice,                      # åœ¨å•ä¸ªåˆ‡ç‰‡ä¸Šæ‰§è¡Œçš„å‡½æ•°
    mesh=mesh,                              # è®¾å¤‡ç½‘æ ¼ï¼ˆå¦‚ 8ä¸ªTPUï¼‰
    in_specs=P(None, None, 'tp', None, None),   # è¾“å…¥åˆ†ç‰‡è§„æ ¼
    out_specs=P(None, None, 'tp', None, None)   # è¾“å‡ºåˆ†ç‰‡è§„æ ¼
)
```

### 1.2 å·¥ä½œæµç¨‹

```
å®Œæ•´è¾“å…¥æ•°æ®
    â†“ (è‡ªåŠ¨åˆ†ç‰‡)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Device 0 â”‚Device 1 â”‚Device N â”‚
â”‚åˆ‡ç‰‡ 0   â”‚åˆ‡ç‰‡ 1   â”‚åˆ‡ç‰‡ N   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (å¹¶è¡Œæ‰§è¡Œ decode_on_slice)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚è¾“å‡ºåˆ‡ç‰‡0â”‚è¾“å‡ºåˆ‡ç‰‡1â”‚è¾“å‡ºåˆ‡ç‰‡Nâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (è‡ªåŠ¨åˆå¹¶)
å®Œæ•´è¾“å‡ºæ•°æ®
```

### 1.3 å…³é”®å‚æ•°è§£æ

**`in_specs` å’Œ `out_specs`ï¼š**

```python
P(None, None, 'tp', None, None)
```

å¯¹åº”å¼ é‡å½¢çŠ¶ `(batch, channels, height, width, depth)`ï¼š
- ç¬¬3ä¸ªç»´åº¦ï¼ˆ`height`ï¼‰æ ‡è®°ä¸º `'tp'`ï¼šåœ¨ mesh çš„ `'tp'` è½´ä¸Šåˆ†ç‰‡
- å…¶ä»–ç»´åº¦æ ‡è®°ä¸º `None`ï¼šåœ¨æ‰€æœ‰è®¾å¤‡ä¸Šå¤åˆ¶ï¼ˆä¸åˆ†ç‰‡ï¼‰

**ç¤ºä¾‹ï¼š**
```python
# è¾“å…¥: (1, 16, 1024, 64, 64)
# mesh æœ‰ 8 ä¸ªè®¾å¤‡åœ¨ 'tp' è½´ä¸Š
# æ¯ä¸ªè®¾å¤‡å¾—åˆ°: (1, 16, 128, 64, 64)  # heightä»1024åˆ†æˆ8ä»½
```

---

## 2. shard_map vs ä¼ ç»Ÿæ–¹æ³•

### 2.1 åªç”¨ device_putï¼ˆâŒ ä¼šå¯¼è‡´é—®é¢˜ï¼‰

```python
# æ•°æ®åˆ†ç‰‡
sharding = NamedSharding(mesh, P(None, None, 'tp', None, None))
latents_jax = jax.device_put(latents_np, sharding)

# ç›´æ¥è°ƒç”¨æ¨¡å‹ï¼ˆé—®é¢˜æ‰€åœ¨ï¼ï¼‰
with env:
    latents_torch = env.j2t_iso(latents_jax)  
    output = vae.decode(latents_torch)  # âš ï¸ è§¦å‘ all-gatherï¼
```

**å‘ç”Ÿçš„äº‹æƒ…ï¼š**

1. âœ… æ•°æ®ç¡®å®è¢«åˆ†ç‰‡äº†
2. âŒ ä½†**è®¡ç®—æ²¡æœ‰è¢«åˆ†ç‰‡**
3. âŒ JAX ä¼šè‡ªåŠ¨æ‰§è¡Œ **all-gather** å°†æ•°æ®æ”¶é›†åˆ°ä¸€ä¸ªè®¾å¤‡
4. âŒ å•ä¸ªè®¾å¤‡å¤„ç†å…¨éƒ¨æ•°æ® â†’ **OOMï¼**

**ä¸ºä»€ä¹ˆä¼š all-gatherï¼Ÿ**

å› ä¸º `vae.decode()` å‡½æ•°ä¸çŸ¥é“å¦‚ä½•å¤„ç†åˆ†ç‰‡æ•°æ®ï¼Œå®ƒæœŸæœ›å®Œæ•´çš„è¾“å…¥å¼ é‡ã€‚JAX ä¸ºäº†æ»¡è¶³è¿™ä¸ªè¦æ±‚ï¼Œä¼šè‡ªåŠ¨æ”¶é›†æ•°æ®ã€‚

### 2.2 ä½¿ç”¨ shard_mapï¼ˆâœ… æ­£ç¡®ï¼‰

```python
def decode_on_slice(latents_slice):  # åªå¤„ç†ä¸€ä¸ªåˆ‡ç‰‡
    with env:
        latents_torchax_slice = env.j2t_iso(latents_slice)
        decoded_output = vae.decode(latents_torchax_slice)
        return decoded_output.sample.jax()

sharded_decode = shard_map(
    f=decode_on_slice,
    mesh=mesh,
    in_specs=P(None, None, 'tp', None, None),
    out_specs=P(None, None, 'tp', None, None)
)

output = sharded_decode(latents_jax)
# âœ… æ¯ä¸ªè®¾å¤‡å¤„ç† 1/8 æ•°æ®ï¼Œæ²¡æœ‰ all-gather
```

### 2.3 å†…å­˜ä½¿ç”¨å¯¹æ¯”

å‡è®¾æ€»æ•°æ® 8GBï¼š

```
åªç”¨ device_putï¼š
  Device 0: 1GBï¼ˆåˆ†ç‰‡æ•°æ®ï¼‰â†’ all-gather â†’ 8GBï¼ˆå®Œæ•´æ•°æ®ï¼‰ğŸ’¥ OOM
  Device 1: 1GBï¼ˆåˆ†ç‰‡æ•°æ®ï¼‰â†’ ç©ºé—²
  ...
  Device 7: 1GBï¼ˆåˆ†ç‰‡æ•°æ®ï¼‰â†’ ç©ºé—²

ç”¨ shard_mapï¼š
  Device 0: 1GBï¼ˆåˆ†ç‰‡æ•°æ®ï¼‰â†’ å¤„ç† â†’ 1GBï¼ˆè¾“å‡ºï¼‰âœ…
  Device 1: 1GBï¼ˆåˆ†ç‰‡æ•°æ®ï¼‰â†’ å¤„ç† â†’ 1GBï¼ˆè¾“å‡ºï¼‰âœ…
  ...
  Device 7: 1GBï¼ˆåˆ†ç‰‡æ•°æ®ï¼‰â†’ å¤„ç† â†’ 1GBï¼ˆè¾“å‡ºï¼‰âœ…
```

### 2.4 å…³äº device_put with shard

**ç»“è®ºï¼šä½¿ç”¨ `shard_map` åï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨ `device_put`ï¼**

```python
# âŒ ä¸éœ€è¦è¿™æ ·åšï¼š
latents_sharded = jax.device_put(
    latents, 
    NamedSharding(mesh, P(None, None, 'tp', None, None))
)
output = sharded_decode(latents_sharded)

# âœ… ç›´æ¥è¿™æ ·å°±å¯ä»¥ï¼š
output = sharded_decode(latents)  # shard_map ä¼šè‡ªåŠ¨å¤„ç†åˆ†ç‰‡
```

**ä¾‹å¤–æƒ…å†µï¼š** å¦‚æœæ•°æ®**å·²ç»åœ¨æ­£ç¡®çš„åˆ†ç‰‡çŠ¶æ€**ï¼Œæ‰‹åŠ¨ `device_put` å¯ä»¥é¿å…ä¸å¿…è¦çš„æ•°æ®ç§»åŠ¨ã€‚

---

## 3. è·¨åˆ†ç‰‡ä¾èµ–é—®é¢˜

### 3.1 é—®é¢˜æè¿°

åœ¨è§†é¢‘è§£ç ç­‰åœºæ™¯ä¸­ï¼Œå¯èƒ½å­˜åœ¨å¸§é—´ä¾èµ–ï¼š

```
Latent Frame 0 â†’ è§£ç  â†’ Output Frames 0,1,2,3
Latent Frame 1 â†’ è§£ç  â†’ Output Frames 4,5,6,7 (éœ€è¦ Frames 2,3)
Latent Frame 2 â†’ è§£ç  â†’ Output Frames 8,9,10,11 (éœ€è¦ Frames 6,7)
```

**é—®é¢˜ï¼š**
- âœ… **é¡ºåºä¾èµ–**ï¼šå¿…é¡»å…ˆè§£ç  Frame 0ï¼Œæ‰èƒ½è§£ç  Frame 1
- âœ… **è·¨è®¾å¤‡ä¾èµ–**ï¼šChip1 éœ€è¦ Chip0 çš„è¾“å‡º
- âŒ **å½“å‰ç®€å• shard_map**ï¼šå„è®¾å¤‡å¹¶è¡Œç‹¬ç«‹è§£ç 

### 3.2 ç©ºé—´ä¾èµ–é—®é¢˜

å³ä½¿æ”¹ä¸ºç©ºé—´åˆ†ç‰‡ï¼Œå·ç§¯æ“ä½œå¯¹ç›¸é‚»åƒç´ ä¹Ÿæœ‰ä¾èµ–ï¼Œå¯èƒ½äº§ç”Ÿ"åˆ†å‰²çº¿"é—®é¢˜ã€‚

### 3.3 è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆAï¼šä½¿ç”¨ Halo Exchange

```python
from jax import lax

def decode_on_slice_with_halo(latents_slice):
    """å¤„ç†ç©ºé—´è¾¹ç•Œä¾èµ–"""
    
    # 1. ä»ç›¸é‚»è®¾å¤‡è·å–è¾¹ç•Œæ•°æ®ï¼ˆhalo exchangeï¼‰
    axis_index = lax.axis_index('tp')
    
    # å®šä¹‰é€šä¿¡æ¨¡å¼
    perm = [(i, (i+1) % mesh.shape['tp']) for i in range(mesh.shape['tp'])]
    
    # è·å–å·¦å³é‚»å±…çš„è¾¹ç•Œ
    left_halo = lax.ppermute(prev_boundary, 'tp', perm=perm)
    
    # 2. æ‹¼æ¥ halo åŒºåŸŸ
    extended_slice = jnp.concatenate([left_halo, latents_slice, right_halo], axis=2)
    
    # 3. è§£ç æ‰©å±•çš„åˆ‡ç‰‡
    output = vae.decode(extended_slice)
    
    # 4. è£å‰ªæ‰ halo éƒ¨åˆ†
    return output[..., halo_size:-halo_size, :, :]
```

**ç¼ºç‚¹ï¼š**
- âŒ éœ€è¦è®¾å¤‡é—´é€šä¿¡
- âŒ ä»£ç å¤æ‚
- âŒ æ€§èƒ½å¼€é”€

#### æ–¹æ¡ˆBï¼šä½¿ç”¨ scan å¤„ç†é¡ºåºä¾èµ–

```python
from jax import lax

def sequential_decode(latents_all):
    """é¡ºåºè§£ç ï¼Œå¤„ç†æ—¶é—´ä¾èµ–"""
    
    def decode_step(carry, latent_frame):
        prev_frames = carry  # å‰é¢å·²è§£ç çš„å¸§
        
        if prev_frames is None:
            # ç¬¬ä¸€å¸§
            output = vae.decode(latent_frame)
        else:
            # ä½¿ç”¨å‰ 2 å¸§ä½œä¸º context
            context = prev_frames[:, :, -2:, :, :]
            extended = jnp.concatenate([context, latent_frame], axis=2)
            output = vae.decode(extended)
            output = output[:, :, 2:, :, :]  # è£å‰ª
        
        # æ›´æ–° carry
        new_carry = output if prev_frames is None else \
                    jnp.concatenate([prev_frames, output], axis=2)
        
        return new_carry, output
    
    # ä½¿ç”¨ scan é¡ºåºå¤„ç†
    _, outputs = lax.scan(decode_step, init=None, xs=latents_all)
    
    return jnp.concatenate(outputs, axis=2)
```

**ç¼ºç‚¹ï¼š**
- âŒ å¤±å»å¹¶è¡Œæ€§ - å®Œå…¨é¡ºåºæ‰§è¡Œ
- âŒ æ— æ³•åˆ©ç”¨å¤šè®¾å¤‡

#### æ–¹æ¡ˆCï¼šæ”¹å˜åˆ†ç‰‡ç»´åº¦ï¼ˆæ¨èï¼‰

```python
# åœ¨ç©ºé—´ç»´åº¦åˆ†ç‰‡ï¼Œä¿æŒæ—¶é—´ç»´åº¦å®Œæ•´
in_specs=P(None, None, None, 'tp', None)  # H ç»´åº¦

# æ¯ä¸ªè®¾å¤‡ï¼š
# - å¤„ç†å®Œæ•´çš„æ—¶é—´åºåˆ—ï¼ˆæ‰€æœ‰å¸§ï¼‰
# - åªå¤„ç†éƒ¨åˆ†é«˜åº¦
# - ä½¿ç”¨ halo exchange å¤„ç†ç©ºé—´è¾¹ç•Œ
```

### 3.4 shard_map èƒ½å¤„ç†ä¾èµ–å—ï¼Ÿ

**ç­”æ¡ˆï¼šå¯ä»¥ï¼Œä½†éœ€è¦æ˜¾å¼ç¼–ç¨‹ã€‚**

`shard_map` **ä¸ä¼šè‡ªåŠ¨**å¤„ç†è·¨åˆ†ç‰‡ä¾èµ–ï¼Œä½†å®ƒæä¾›äº†å·¥å…·ï¼ˆå¦‚ `lax.ppermute`ï¼‰è®©ä½ **æ‰‹åŠ¨å®ç°**ã€‚

**å…³é”®é™åˆ¶ï¼š** `shard_map` ä¸èƒ½è¡¨è¾¾é¡ºåºä¾èµ–ï¼Œå› ä¸ºæ‰€æœ‰è®¾å¤‡å¿…é¡»æ‰§è¡Œç›¸åŒçš„ç¨‹åºï¼ˆSPMD - Single Program Multiple Dataï¼‰ã€‚

---

## 4. JAX Map å‡½æ•°å®¶æ—

### 4.1 å®Œæ•´åˆ—è¡¨

| Map ç±»å‹ | ç”¨é€” | æ‰§è¡Œæ–¹å¼ | è®¾å¤‡ | å…¸å‹åœºæ™¯ |
|---------|------|---------|------|---------|
| **vmap** | å‘é‡åŒ– | å¹¶è¡Œï¼ˆå‘é‡åŒ–ï¼‰ | å•è®¾å¤‡ | æ‰¹å¤„ç† |
| **pmap** | æ•°æ®å¹¶è¡Œ | å¹¶è¡Œï¼ˆå¤šè®¾å¤‡ï¼‰ | å¤šè®¾å¤‡ | ç®€å•å¤šGPU |
| **shard_map** | åˆ†ç‰‡å¹¶è¡Œ | å¹¶è¡Œï¼ˆå¯æ§ï¼‰ | å¤šè®¾å¤‡ | å¤æ‚åˆ†ç‰‡ |
| **scan** | é¡ºåºå¾ªç¯ | é¡ºåº | å•/å¤šè®¾å¤‡ | æ—¶é—´åºåˆ— |
| **tree_map** | ç»“æ„æ˜ å°„ | - | - | åµŒå¥—æ•°æ® |

### 4.2 vmap - å‘é‡åŒ–æ˜ å°„

```python
# åŸå§‹å‡½æ•°ï¼šå¤„ç†å•ä¸ªæ ·æœ¬
def process_single(x):
    return x ** 2 + 1

# ä½¿ç”¨ vmapï¼ˆé«˜æ•ˆï¼‰
process_batch = jax.vmap(process_single)

# ä½¿ç”¨
batch = jnp.array([[1, 2], [3, 4], [5, 6]])  # (3, 2)
output = process_batch(batch)
```

**ç‰¹ç‚¹ï¼š**
- âœ… è‡ªåŠ¨å‘é‡åŒ–
- âœ… å•è®¾å¤‡ï¼Œæ‰€æœ‰æ•°æ®åœ¨åŒä¸€è®¾å¤‡
- âœ… ç®€å•æ˜“ç”¨
- âŒ å—å•è®¾å¤‡å†…å­˜é™åˆ¶

### 4.3 pmap - å¹¶è¡Œæ˜ å°„

```python
# pmap å¤šè®¾å¤‡
pmap_square = pmap(square)

# ä½¿ç”¨ï¼ˆå‡è®¾4ä¸ªGPUï¼‰
data = jnp.array([1, 2, 3, 4])  # (4,)
result = pmap_square(data)

# æ¯ä¸ªGPUå¤„ç†ä¸€ä¸ªå…ƒç´ 
# GPU:0 å¤„ç† data[0] = 1
# GPU:1 å¤„ç† data[1] = 2
# ...
```

**ç‰¹ç‚¹ï¼š**
- âœ… è‡ªåŠ¨æ•°æ®å¹¶è¡Œ
- âœ… ç®€å•æ˜“ç”¨
- âŒ ç¬¬ä¸€ç»´å¿…é¡»ç­‰äºè®¾å¤‡æ•°
- âŒ ä¸æ”¯æŒå¤æ‚åˆ†ç‰‡æ¨¡å¼
- âš ï¸ æ­£åœ¨è¢« `shard_map` æ›¿ä»£

### 4.4 scan - é¡ºåºæ˜ å°„

```python
from jax import lax

def cumulative_sum(arr):
    def step(carry, x):
        new_sum = carry + x
        return new_sum, new_sum  # (æ–°carry, è¾“å‡º)
    
    init = 0
    final_carry, outputs = lax.scan(step, init, arr)
    return outputs

# ç¤ºä¾‹
arr = jnp.array([1, 2, 3, 4, 5])
result = cumulative_sum(arr)  # [1, 3, 6, 10, 15]
```

**ç‰¹ç‚¹ï¼š**
- âœ… å¯å¾®åˆ†çš„å¾ªç¯
- âœ… æ”¯æŒç´¯ç§¯çŠ¶æ€
- âœ… é¡ºåºä¾èµ–çš„æœ€ä½³é€‰æ‹©
- âŒ é¡ºåºæ‰§è¡Œï¼ˆä¸å¹¶è¡Œï¼‰

**ä½¿ç”¨åœºæ™¯ï¼š**
- RNN/LSTM çš„æ—¶é—´æ­¥è¿­ä»£
- å¸§é—´ä¾èµ–çš„è§†é¢‘è§£ç 
- ä»»ä½•éœ€è¦ç´¯ç§¯çŠ¶æ€çš„å¾ªç¯

### 4.5 tree_map - æ ‘ç»“æ„æ˜ å°„

```python
from jax import tree_util

# å¤„ç†åµŒå¥—æ•°æ®ç»“æ„
data = {
    'weights': jnp.array([1, 2, 3]),
    'biases': jnp.array([0.1, 0.2]),
    'nested': {
        'layer1': jnp.array([5, 6])
    }
}

# å¯¹æ‰€æœ‰å¶å­èŠ‚ç‚¹åº”ç”¨å‡½æ•°
doubled = tree_util.tree_map(lambda x: x * 2, data)
```

**ç‰¹ç‚¹ï¼š**
- âœ… å¤„ç†ä»»æ„åµŒå¥—ç»“æ„
- âœ… é€‚åˆæ¨¡å‹å‚æ•°ç­‰å¤æ‚ç»“æ„
- âœ… ä¿æŒç»“æ„ä¸å˜

---

## 5. vmap vs pmap è¯¦è§£

### 5.1 æ ¸å¿ƒåŒºåˆ«

è™½ç„¶çœ‹èµ·æ¥ç›¸ä¼¼ï¼Œä½†**æœ¬è´¨å®Œå…¨ä¸åŒ**ï¼š

#### vmap: å•è®¾å¤‡å‘é‡åŒ–

```python
def square(x):
    return x ** 2

vmap_square = vmap(square)

data = jnp.array([1, 2, 3, 4])  # (4,)
result = vmap_square(data)

# å†…éƒ¨ï¼š
# âœ… æ‰€æœ‰æ•°æ®åœ¨å•ä¸ªè®¾å¤‡ï¼ˆå¦‚ GPU:0ï¼‰
# âœ… JAX å°†å¾ªç¯ä¼˜åŒ–ä¸ºå‘é‡åŒ–æ“ä½œ
# âœ… åœ¨å•ä¸ªè®¾å¤‡ä¸Šå¹¶å‘æ‰§è¡Œï¼ˆå‘é‡åŒ–ï¼‰
```

**å†…å­˜å¸ƒå±€ï¼š**
```
è®¾å¤‡ GPU:0:  [1, 2, 3, 4] â†’ å‘é‡åŒ–è®¡ç®— â†’ [1, 4, 9, 16]
```

#### pmap: å¤šè®¾å¤‡å¹¶è¡Œ

```python
pmap_square = pmap(square)

data = jnp.array([1, 2, 3, 4])  # (4,)
result = pmap_square(data)

# å†…éƒ¨ï¼š
# GPU:0 å¤„ç† data[0] = 1
# GPU:1 å¤„ç† data[1] = 2
# GPU:2 å¤„ç† data[2] = 3
# GPU:3 å¤„ç† data[3] = 4
```

**å†…å­˜å¸ƒå±€ï¼š**
```
è®¾å¤‡ GPU:0:  [1] â†’ è®¡ç®— â†’ [1]
è®¾å¤‡ GPU:1:  [2] â†’ è®¡ç®— â†’ [4]
è®¾å¤‡ GPU:2:  [3] â†’ è®¡ç®— â†’ [9]
è®¾å¤‡ GPU:3:  [4] â†’ è®¡ç®— â†’ [16]
```

### 5.2 è¯¦ç»†å¯¹æ¯”è¡¨

| ç‰¹æ€§ | vmap | pmap |
|------|------|------|
| **è®¾å¤‡æ•°é‡** | å•è®¾å¤‡ | å¤šè®¾å¤‡ |
| **å†…å­˜ä½ç½®** | æ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªè®¾å¤‡ | æ•°æ®åˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ |
| **ç¬¬ä¸€ç»´è¦æ±‚** | ä»»æ„å¤§å° | å¿…é¡»ç­‰äºè®¾å¤‡æ•° |
| **é€‚ç”¨åœºæ™¯** | æ‰¹å¤„ç†ï¼ˆæ•°æ®å°ï¼‰ | æ•°æ®å¹¶è¡Œï¼ˆæ•°æ®å¤§ï¼‰ |
| **é€šä¿¡å¼€é”€** | æ—  | æœ‰ï¼ˆè·¨è®¾å¤‡ï¼‰ |
| **æ‰©å±•æ€§** | å—å•è®¾å¤‡å†…å­˜é™åˆ¶ | å¯æ‰©å±•åˆ°å¤šè®¾å¤‡ |

### 5.3 å®é™…ä½¿ç”¨åœºæ™¯

#### åœºæ™¯1ï¼šæ•°æ®é‡å°ï¼ˆå‡ ç™¾MBï¼‰

```python
# ç”¨ vmap - ç®€å•é«˜æ•ˆ
batch_data = jnp.ones((1000, 224, 224, 3))
result = vmap(model.forward)(batch_data)  # å•GPUå°±å¤Ÿäº†
```

#### åœºæ™¯2ï¼šæ•°æ®é‡å¤§ï¼ˆå‡ åGBï¼‰

```python
# ç”¨ pmap - åˆ†å¸ƒåˆ°å¤šGPU
num_gpus = jax.device_count()
batch_data = jnp.ones((num_gpus, 128, 224, 224, 3))
result = pmap(vmap(model.forward))(batch_data)
```

#### åœºæ™¯3ï¼šè¶…å¤§æ•°æ® + çµæ´»åˆ†ç‰‡

```python
# ç”¨ shard_map - æœ€çµæ´»
batch_data = jnp.ones((1024, 224, 224, 3))  # ä»»æ„å¤§å°
result = shard_map(
    vmap(model.forward),
    mesh=mesh,
    in_specs=P('devices', None, None, None),
    out_specs=P('devices', None, None, None)
)(batch_data)
```

---

## 6. Mesh çš„é‡è¦æ€§

### 6.1 ä¸ºä»€ä¹ˆ pmap æ²¡æœ‰ meshï¼Ÿ

`pmap` æ˜¯**æ—©æœŸè®¾è®¡**ï¼ˆ2018-2020ï¼‰ï¼Œé‡‡ç”¨**éšå¼è®¾å¤‡ç®¡ç†**ï¼š

```python
# pmap è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰å¯ç”¨è®¾å¤‡
parallel_fn = pmap(my_function)

# å†…éƒ¨ï¼š
# âœ… è‡ªåŠ¨å‘ç°è®¾å¤‡ï¼šjax.devices()
# âœ… ç®€å•ä¸€ç»´æ’åˆ—ï¼š[GPU:0, GPU:1, GPU:2, ...]
# âœ… ç¬¬ä¸€ç»´è‡ªåŠ¨åˆ†é…åˆ°è®¾å¤‡
```

### 6.2 pmap çš„é™åˆ¶

```python
# âŒ é™åˆ¶1ï¼šç¬¬ä¸€ç»´å¿…é¡»ç­‰äºè®¾å¤‡æ•°
data = jnp.ones((8, 100))  # å¦‚æœåªæœ‰4ä¸ªGPU â†’ é”™è¯¯ï¼

# âŒ é™åˆ¶2ï¼šåªæ”¯æŒä¸€ç»´è®¾å¤‡æ’åˆ—
# ä¸èƒ½è¡¨è¾¾ (2Ã—4) çš„è®¾å¤‡ç½‘æ ¼

# âŒ é™åˆ¶3ï¼šåˆ†ç‰‡æ¨¡å¼å›ºå®š
# åªèƒ½åœ¨ç¬¬ä¸€ç»´åˆ†ç‰‡

# âŒ é™åˆ¶4ï¼šéš¾ä»¥è¡¨è¾¾å¤æ‚å¹¶è¡Œæ¨¡å¼
# å¦‚æ¨¡å‹å¹¶è¡Œ + æ•°æ®å¹¶è¡Œ
```

### 6.3 shard_map çš„æ˜¾å¼ mesh

```python
from jax.sharding import Mesh
from jax.experimental import mesh_utils

# æ˜¾å¼åˆ›å»ºè®¾å¤‡ç½‘æ ¼
devices = mesh_utils.create_device_mesh((2, 4))  # 2Ã—4ç½‘æ ¼
mesh = Mesh(devices, ('data', 'model'))

# æ˜¾å¼æŒ‡å®šåˆ†ç‰‡æ¨¡å¼
sharded_fn = shard_map(
    fn,
    mesh=mesh,
    in_specs=P('data', 'model'),  # æ˜ç¡®è¯´æ˜å¦‚ä½•åˆ†ç‰‡
    out_specs=P('data', 'model')
)
```

**ä¼˜åŠ¿ï¼š**
```python
# âœ… æ”¯æŒä»»æ„å½¢çŠ¶æ•°æ®
data = jnp.ones((100, 512))  # ä¸éœ€è¦ç¬¬ä¸€ç»´ç­‰äºè®¾å¤‡æ•°

# âœ… æ”¯æŒå¤šç»´è®¾å¤‡ç½‘æ ¼
mesh = Mesh(devices, ('dp', 'mp', 'pp'))  # 3ç»´å¹¶è¡Œ

# âœ… çµæ´»çš„åˆ†ç‰‡æ¨¡å¼
in_specs=P('dp', None)      # åªåœ¨æ•°æ®å¹¶è¡Œç»´åº¦åˆ†ç‰‡
in_specs=P(None, 'mp')      # åªåœ¨æ¨¡å‹å¹¶è¡Œç»´åº¦åˆ†ç‰‡
in_specs=P('dp', 'mp')      # ä¸¤ä¸ªç»´åº¦éƒ½åˆ†ç‰‡
```

### 6.4 å¤æ‚å¹¶è¡Œæ¨¡å¼å¯¹æ¯”

#### pmap å®ç° 2D å¹¶è¡Œï¼ˆç¹çï¼‰

```python
# pmap åµŒå¥—å®ç°æ•°æ® + æ¨¡å‹å¹¶è¡Œ
data_parallel = pmap(fn, axis_name='dp')
model_parallel = pmap(data_parallel, axis_name='mp')

# éœ€è¦æ‰‹åŠ¨é‡å¡‘æ•°æ®
data = data.reshape(2, 4, ...)  # å‡è®¾ 2Ã—4 è®¾å¤‡
result = model_parallel(data)
```

#### shard_map å®ç° 2D å¹¶è¡Œï¼ˆæ¸…æ™°ï¼‰

```python
# shard_map ç›´æ¥è¡¨è¾¾ 2D å¹¶è¡Œ
devices = mesh_utils.create_device_mesh((2, 4))
mesh = Mesh(devices, ('dp', 'mp'))

sharded_fn = shard_map(
    fn,
    mesh=mesh,
    in_specs=P('dp', 'mp'),  # æ¸…æ™°çš„2Dåˆ†ç‰‡
    out_specs=P('dp', 'mp')
)

data = jnp.ones((64, 1024))  # ä»»æ„å½¢çŠ¶
result = sharded_fn(data)
```

### 6.5 JAX å¹¶è¡Œç¼–ç¨‹æ¼”è¿›

```
2018-2020: pmap æ—¶ä»£
  - ç®€å•éšå¼
  - å•ä¸€ç»´åº¦
  - æ˜“äºå…¥é—¨

2021: xmap å®éªŒ
  - å¼•å…¥å‘½åè½´
  - å¤æ‚ä½†çµæ´»

2022-2023: mesh + shard_map
  - æ˜¾å¼è®¾å¤‡æ‹“æ‰‘
  - ç»Ÿä¸€ API
  - ç”Ÿäº§å°±ç»ª

2024+: æ¨èä½¿ç”¨
  - shard_map + mesh
  - pmap é€æ­¥æ·˜æ±°
```

---

## 7. å®è·µå»ºè®®

### 7.1 é€‰æ‹©å†³ç­–æ ‘

```
éœ€è¦å¤„ç†æ•°æ®
    â”‚
    â”œâ”€ æ•°æ® < å•è®¾å¤‡å†…å­˜ï¼Ÿ
    â”‚   â”œâ”€ æ˜¯ â†’ æœ‰é¡ºåºä¾èµ–ï¼Ÿ
    â”‚   â”‚   â”œâ”€ æ—  â†’ vmap
    â”‚   â”‚   â””â”€ æœ‰ â†’ scan
    â”‚   â”‚
    â”‚   â””â”€ å¦ â†’ ä¾èµ–æ¨¡å¼ï¼Ÿ
    â”‚       â”œâ”€ ç®€å•æ•°æ®å¹¶è¡Œ â†’ pmap
    â”‚       â”œâ”€ å¤æ‚åˆ†ç‰‡ â†’ shard_map
    â”‚       â””â”€ æœ‰é¡ºåºä¾èµ– â†’ shard_map + scan
```

### 7.2 å¸¸è§æ¨¡å¼

#### æ¨¡å¼1ï¼šæ‰¹é‡å¤„ç†ï¼ˆå•è®¾å¤‡ï¼‰

```python
# ä½¿ç”¨ vmap
process_batch = jax.vmap(process_single)
results = process_batch(batch_data)
```

#### æ¨¡å¼2ï¼šç®€å•å¤šè®¾å¤‡ï¼ˆè¢«æ·˜æ±°ï¼‰

```python
# ä½¿ç”¨ pmapï¼ˆä¸æ¨èï¼Œä½†è¿˜èƒ½ç”¨ï¼‰
process_parallel = jax.pmap(process_single)
results = process_parallel(reshaped_data)
```

#### æ¨¡å¼3ï¼šå¤æ‚åˆ†ç‰‡ï¼ˆæ¨èï¼‰

```python
# ä½¿ç”¨ shard_map + mesh
mesh = Mesh(devices, ('dp', 'mp'))
process_sharded = shard_map(
    process_single,
    mesh=mesh,
    in_specs=P('dp', None),
    out_specs=P('dp', None)
)
results = process_sharded(data)
```

#### æ¨¡å¼4ï¼šé¡ºåºä¾èµ–

```python
# ä½¿ç”¨ scan
def step(carry, x):
    new_state = f(carry, x)
    return new_state, output

_, results = lax.scan(step, init, sequence)
```

#### æ¨¡å¼5ï¼šç»„åˆä½¿ç”¨

```python
# shard_map + vmapï¼šæ¯ä¸ªè®¾å¤‡å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡
sharded_batch = shard_map(
    jax.vmap(process_single),
    mesh=mesh,
    in_specs=P('devices', None),
    out_specs=P('devices', None)
)
```

### 7.3 æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä¼˜å…ˆä½¿ç”¨ vmap**ï¼šå¦‚æœæ•°æ®é€‚åˆå•è®¾å¤‡ï¼Œvmap æœ€ç®€å•é«˜æ•ˆ
2. **é¿å… all-gather**ï¼šç¡®ä¿ä½¿ç”¨ shard_map åŒ…è£…è®¡ç®—
3. **åˆç†åˆ†ç‰‡ç»´åº¦**ï¼š
   - æ— ä¾èµ–ï¼šä»»æ„ç»´åº¦
   - ç©ºé—´ä¾èµ–ï¼šè€ƒè™‘ halo exchange
   - æ—¶é—´ä¾èµ–ï¼šä½¿ç”¨ scan æˆ–æ”¹ä¸ºç©ºé—´åˆ†ç‰‡
4. **æ˜¾å¼ä¼˜äºéšå¼**ï¼šä½¿ç”¨ shard_map + mesh è€Œä¸æ˜¯ pmap
5. **æµ‹è¯•éªŒè¯**ï¼šå¯¹æ¯”åˆ†ç‰‡å’Œéåˆ†ç‰‡ç»“æœï¼Œç¡®ä¿æ•°å€¼ä¸€è‡´æ€§

### 7.4 è°ƒè¯•å»ºè®®

1. **æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦ç”Ÿæ•ˆ**ï¼š
```python
print(f"Input sharding: {jax.device_get(input.sharding)}")
print(f"Output sharding: {jax.device_get(output.sharding)}")
```

2. **ç›‘æ§å†…å­˜ä½¿ç”¨**ï¼š
```python
# åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šæ£€æŸ¥å†…å­˜
for device in jax.devices():
    print(f"{device}: {device.memory_stats()}")
```

3. **éªŒè¯æ•°å€¼æ­£ç¡®æ€§**ï¼š
```python
# å¯¹æ¯”åˆ†ç‰‡å’Œéåˆ†ç‰‡ç»“æœ
result_sharded = sharded_fn(data)
result_baseline = baseline_fn(data)
assert jnp.allclose(result_sharded, result_baseline, rtol=1e-5)
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **shard_map**ï¼šæ˜¾å¼åˆ†ç‰‡å¹¶è¡Œçš„ç°ä»£å·¥å…·
   - éœ€è¦ mesh å®šä¹‰è®¾å¤‡æ‹“æ‰‘
   - éœ€è¦ in_specs/out_specs å®šä¹‰åˆ†ç‰‡è§„åˆ™
   - ä¸è‡ªåŠ¨å¤„ç†è·¨åˆ†ç‰‡ä¾èµ–

2. **vmap vs pmap**ï¼š
   - vmap = å•è®¾å¤‡å‘é‡åŒ–
   - pmap = å¤šè®¾å¤‡æ•°æ®å¹¶è¡Œ
   - ä¸¤è€…éƒ½æœ‰ç”¨ï¼Œä½†åœºæ™¯ä¸åŒ

3. **ä¾èµ–å¤„ç†**ï¼š
   - ç©ºé—´ä¾èµ– â†’ halo exchange
   - æ—¶é—´ä¾èµ– â†’ scan æˆ–æ”¹åˆ†ç‰‡ç»´åº¦
   - shard_map æä¾›å·¥å…·ä½†éœ€æ‰‹åŠ¨å®ç°

4. **mesh çš„é‡è¦æ€§**ï¼š
   - æ˜¾å¼è®¾å¤‡æ‹“æ‰‘
   - æ”¯æŒå¤æ‚å¹¶è¡Œæ¨¡å¼
   - ç°ä»£ JAX çš„æ ¸å¿ƒ

### å¿«é€Ÿå‚è€ƒ

| éœ€æ±‚ | æ¨èå·¥å…· |
|------|---------|
| æ‰¹å¤„ç†ï¼ˆå•è®¾å¤‡ï¼‰ | vmap |
| ç®€å•å¤šè®¾å¤‡ | pmapï¼ˆç»´æŠ¤æ¨¡å¼ï¼‰ |
| å¤æ‚åˆ†ç‰‡ | shard_map |
| é¡ºåºå¾ªç¯ | scan |
| åµŒå¥—ç»“æ„ | tree_map |
| å¤šç»´å¹¶è¡Œ | shard_map + mesh |

---

## å‚è€ƒèµ„æº

- [JAX å®˜æ–¹æ–‡æ¡£](https://jax.readthedocs.io/)
- [shard_map æ•™ç¨‹](https://jax.readthedocs.io/en/latest/notebooks/shard_map.html)
- [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)
- [Parallel evaluation in JAX](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html)

---

*æ–‡æ¡£åˆ›å»ºæ—¶é—´ï¼š2025-11-04*
*åŸºäº JAX 0.4+ å’Œ torchax æœ€æ–°ç‰ˆæœ¬*