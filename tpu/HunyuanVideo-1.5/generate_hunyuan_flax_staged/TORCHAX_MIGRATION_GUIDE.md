# PyTorch GPU â†’ torchax TPU å®Œæ•´è¿ç§»æŒ‡å—

æœ¬æ–‡æ¡£è®°å½•äº†å°† HunyuanVideo-1.5 Transformer ä» GPU PyTorch è¿ç§»åˆ° TPU torchax çš„å®Œæ•´è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é—®é¢˜åˆ†æã€è§£å†³æ–¹æ¡ˆå’Œæœ€ä½³å®è·µã€‚

---

## ğŸ“š ç›®å½•

1. [è¿ç§»æ¦‚è§ˆ](#1-è¿ç§»æ¦‚è§ˆ)
2. [æ¶æ„å¯¹æ¯”](#2-æ¶æ„å¯¹æ¯”)
3. [æ ¸å¿ƒä¿®å¤è¯¦è§£](#3-æ ¸å¿ƒä¿®å¤è¯¦è§£)
4. [å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ](#4-å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ)
5. [å®Œæ•´è¿ç§»æµç¨‹](#5-å®Œæ•´è¿ç§»æµç¨‹)
6. [ä»£ç æ¨¡æ¿](#6-ä»£ç æ¨¡æ¿)
7. [æ€§èƒ½ä¼˜åŒ–](#7-æ€§èƒ½ä¼˜åŒ–)
8. [è°ƒè¯•æŠ€å·§](#8-è°ƒè¯•æŠ€å·§)

---

## 1. è¿ç§»æ¦‚è§ˆ

### 1.1 æ•´ä½“è¿ç§»æµç¨‹

```mermaid
flowchart TB
    subgraph GPU["ğŸ® GPU ç‰ˆæœ¬"]
        G1[PyTorch æ¨¡å‹] --> G2[Flash Attention]
        G2 --> G3[CUDA Kernels]
        G3 --> G4[NCCL åˆ†å¸ƒå¼]
    end
    
    subgraph TPU["â˜ï¸ TPU ç‰ˆæœ¬"]
        T1[PyTorch æ¨¡å‹] --> T2[torchax æ¡¥æ¥]
        T2 --> T3[Splash Attention]
        T3 --> T4[XLA ç¼–è¯‘]
        T4 --> T5[GSPMD åˆ†å¸ƒå¼]
    end
    
    GPU --> |è¿ç§»| TPU
    
    style GPU fill:#ffcccc
    style TPU fill:#ccffcc
```

### 1.2 å…³é”®æŠ€æœ¯æ ˆå¯¹æ¯”

| æŠ€æœ¯å±‚ | GPU ç‰ˆæœ¬ | TPU ç‰ˆæœ¬ |
|--------|----------|----------|
| è¿è¡Œæ¡†æ¶ | PyTorch | torchax (PyTorch â†’ JAX) |
| Attention | Flash Attention 2/3 | Splash Attention (Pallas) |
| JIT ç¼–è¯‘ | torch.compile | XLA JIT |
| åˆ†å¸ƒå¼ | NCCL + æ‰‹åŠ¨ SP/TP | GSPMD (è‡ªåŠ¨åˆ†ç‰‡) |
| æ•°æ®ç±»å‹ | fp16 / fp32 | bf16 (åŸç”Ÿæ”¯æŒ) |
| è®¾å¤‡ç®¡ç† | CUDA | JAX Device Mesh |

---

## 2. æ¶æ„å¯¹æ¯”

### 2.1 æ•°æ®æµå¯¹æ¯”

```mermaid
flowchart LR
    subgraph GPU["GPU æ•°æ®æµ"]
        direction TB
        GA[CPU Tensor] --> GB[.cuda]
        GB --> GC[GPU Tensor]
        GC --> GD[Model Forward]
        GD --> GE[Output Tensor]
    end
    
    subgraph TPU["TPU æ•°æ®æµ"]
        direction TB
        TA[CPU Tensor] --> TB[.to jax]
        TB --> TC[env.to_xla]
        TC --> TD[XLA Tensor]
        TD --> TE[JIT Compiled Model]
        TE --> TF[Output Tensor]
    end
    
    style GPU fill:#f9f
    style TPU fill:#9ff
```

### 2.2 Attention å®ç°å¯¹æ¯”

```mermaid
flowchart TB
    subgraph Flash["Flash Attention (GPU)"]
        F1[Q, K, V] --> F2[Variable Length<br/>Packed Sequences]
        F2 --> F3[flash_attn_no_pad]
        F3 --> F4[CUDA Kernel<br/>åˆ†å—è®¡ç®—]
        F4 --> F5[Output]
    end
    
    subgraph Splash["Splash Attention (TPU)"]
        S1[Q, K, V] --> S2[Pad to Block Size]
        S2 --> S3[shard_map åˆ†ç‰‡]
        S3 --> S4[Pallas Kernel<br/>åˆ†å—è®¡ç®—]
        S4 --> S5[Trim Padding]
        S5 --> S6[Output]
    end
    
    style Flash fill:#ffcccc
    style Splash fill:#ccffcc
```

---

## 3. æ ¸å¿ƒä¿®å¤è¯¦è§£

åœ¨å°† HunyuanVideo-1.5 è¿ç§»åˆ° TPU æ—¶ï¼Œé‡åˆ°äº†å¤šä¸ªå¯¼è‡´ç”Ÿæˆè´¨é‡é—®é¢˜çš„å…³é”®å·®å¼‚ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†åˆ†æå’Œä¿®å¤è¿‡ç¨‹ã€‚

### 3.1 ä¿®å¤ #1: ByT5 Embeddings ç²¾åº¦é—®é¢˜

#### é—®é¢˜åˆ†æ

```mermaid
flowchart LR
    subgraph GPU["GPU: ByT5 å¤„ç†"]
        G1[ByT5 Output<br/>float32] --> G2[ä¿æŒ float32]
        G2 --> G3[Transformer<br/>æ··åˆç²¾åº¦]
    end
    
    subgraph TPU_BAD["TPU (é”™è¯¯): ByT5 å¤„ç†"]
        T1[ByT5 Output<br/>float32] --> T2[è½¬æ¢ä¸º bf16]
        T2 --> T3[ç²¾åº¦æŸå¤±!]
        T3 --> T4[ç”Ÿæˆè´¨é‡ä¸‹é™]
    end
    
    subgraph TPU_GOOD["TPU (ä¿®å¤): ByT5 å¤„ç†"]
        F1[ByT5 Output<br/>float32] --> F2[ä¿æŒ float32]
        F2 --> F3[è´¨é‡æ­£å¸¸]
    end
    
    style GPU fill:#ccffcc
    style TPU_BAD fill:#ffcccc
    style TPU_GOOD fill:#ccffcc
```

#### é”™è¯¯ä»£ç 

```python
# âŒ é”™è¯¯ï¼šä½¿ç”¨ bf16 å¯¼è‡´ç²¾åº¦æŸå¤±
prompt_embeds_2 = prompt_embeds_2.to(dtype=target_dtype).to('jax')  # bf16
```

#### ä¿®å¤ä»£ç 

```python
# âœ… æ­£ç¡®ï¼šä¿æŒ float32
prompt_embeds_2 = prompt_embeds_2.to(dtype=torch.float32).to('jax')
```

#### ä¸ºä»€ä¹ˆ ByT5 éœ€è¦ float32ï¼Ÿ

ByT5 æ˜¯å­—èŠ‚çº§åˆ«çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œå…¶è¾“å‡ºç”¨äºç»†ç²’åº¦çš„æ–‡æœ¬æ¡ä»¶æ§åˆ¶ã€‚bf16 çš„ç²¾åº¦ä¸è¶³ä»¥ä¿ç•™ç»†å¾®çš„æ–‡æœ¬è¯­ä¹‰å·®å¼‚ï¼Œå¯¼è‡´ç”Ÿæˆè§†é¢‘æ— æ³•å‡†ç¡®è·Ÿéšæç¤ºè¯ã€‚

---

### 3.2 ä¿®å¤ #2: Attention Mask å¤„ç†

#### é—®é¢˜åˆ†æ

è¿™æ˜¯æœ€å…³é”®çš„ä¿®å¤ã€‚GPU ç‰ˆæœ¬ä½¿ç”¨ `flex_attention` é…åˆ `score_mod` å‡½æ•°æ¥å±è”½ padding tokensï¼Œè€Œæˆ‘ä»¬çš„åˆå§‹ TPU ç‰ˆæœ¬å®Œå…¨å¿½ç•¥äº†è¿™ä¸ª maskã€‚

```mermaid
flowchart TB
    subgraph GPU["GPU: Attention Mask å¤„ç†"]
        G1[text_mask] --> G2[F.pad æ‰©å±•åˆ°å®Œæ•´åºåˆ—]
        G2 --> G3[flex_attention<br/>score_mod å‡½æ•°]
        G3 --> G4[Padding ä½ç½® â†’ -inf]
        G4 --> G5[Softmax åæƒé‡ = 0]
    end
    
    subgraph TPU_BAD["TPU (é”™è¯¯): æ—  Mask"]
        T1[text_mask] --> T2[å¿½ç•¥!]
        T2 --> T3[Splash Attention<br/>æ—  mask]
        T3 --> T4[Padding å‚ä¸è®¡ç®—]
        T4 --> T5[æ³¨æ„åŠ›æ±¡æŸ“]
    end
    
    subgraph TPU_GOOD["TPU (ä¿®å¤): K/V ç½®é›¶è¿‘ä¼¼"]
        F1[text_mask] --> F2[æ‰©å±• mask ç»´åº¦]
        F2 --> F3[K *= mask<br/>V *= mask]
        F3 --> F4[Padding ä½ç½® K/V = 0]
        F4 --> F5[QK^T å¯¹åº”ä½ç½® â‰ˆ 0]
        F5 --> F6[Softmax åæƒé‡å¾ˆä½]
    end
    
    style GPU fill:#ccffcc
    style TPU_BAD fill:#ffcccc
    style TPU_GOOD fill:#ccffcc
```

#### åŸå§‹ GPU ä»£ç  (attention.py)

```python
# GPU ä½¿ç”¨ flex_attention + score_mod
if text_mask is not None:
    attn_mask = F.pad(text_mask, (sequence_length, 0), value=True)

def score_mod(score, b, h, q_idx, kv_idx):
    return torch.where(attn_mask[b, q_idx] & attn_mask[b, kv_idx], score, float('-inf'))

hidden_states = flex_attention(query, key, value, score_mod=score_mod)
```

#### é”™è¯¯çš„ TPU ä»£ç 

```python
# âŒ é”™è¯¯ï¼šå®Œå…¨å¿½ç•¥ text_mask
attn_mask = None  # å¼ºåˆ¶ä½¿ç”¨ Splash Attentionï¼Œä½†æ²¡æœ‰ maskï¼
hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attn_mask)
```

#### ä¿®å¤åçš„ TPU ä»£ç 

```python
# âœ… æ­£ç¡®ï¼šå°† padding ä½ç½®çš„ K/V è®¾ä¸ºé›¶
if text_mask is not None:
    # text_mask: [B, text_len], 1=æœ‰æ•ˆ, 0=padding
    text_mask_expanded = text_mask.unsqueeze(-1).unsqueeze(-1).to(encoder_key.dtype)
    encoder_key = encoder_key * text_mask_expanded    # Padding ä½ç½® â†’ 0
    encoder_value = encoder_value * text_mask_expanded  # Padding ä½ç½® â†’ 0

# åˆå¹¶ image å’Œ text tokens
query = torch.cat([query, encoder_query], dim=1)
key = torch.cat([key, encoder_key], dim=1)     # text padding éƒ¨åˆ†æ˜¯ 0
value = torch.cat([value, encoder_value], dim=1)  # text padding éƒ¨åˆ†æ˜¯ 0

# Splash Attentionï¼ˆæ— éœ€æ˜¾å¼ maskï¼‰
hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=None)
```

#### ä¸ºä»€ä¹ˆ K/V ç½®é›¶æœ‰æ•ˆï¼Ÿ

```mermaid
flowchart LR
    subgraph æ•°å­¦åŸç†
        A["Q @ K^T"] --> B["å½“ K[i]=0 æ—¶<br/>score[i] â‰ˆ 0"]
        B --> C["Softmax å<br/>weight[i] å¾ˆå°"]
        C --> D["V[i] è´¡çŒ®<br/>æ¥è¿‘äº 0"]
    end
```

è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼æ–¹æ¡ˆï¼š
- ç²¾ç¡®æ–¹æ¡ˆï¼šå°† score è®¾ä¸º `-inf`ï¼Œsoftmax åæƒé‡ = 0
- è¿‘ä¼¼æ–¹æ¡ˆï¼šå°† K/V è®¾ä¸º 0ï¼Œscore â‰ˆ 0ï¼Œsoftmax åæƒé‡å¾ˆå°

è¿‘ä¼¼æ–¹æ¡ˆè¶³å¤Ÿæœ‰æ•ˆï¼Œå› ä¸º padding tokens çš„å½±å“è¢«å¤§å¹…é™ä½ã€‚

---

### 3.3 ä¿®å¤ #3: vision_states å¤„ç†

#### é—®é¢˜åˆ†æ

```mermaid
flowchart LR
    subgraph GPU["GPU: t2v æ¨¡å¼"]
        G1["vision_states = zeros(...)"] --> G2["torch.all(x==0) æ£€æŸ¥"]
        G2 --> G3["extra_attention_mask = 0"]
    end
    
    subgraph TPU["TPU: t2v æ¨¡å¼"]
        T1["vision_states = None"] --> T2["è·³è¿‡ vision_in åˆ†æ”¯"]
        T2 --> T3["ç­‰æ•ˆæ•ˆæœ"]
    end
    
    style GPU fill:#ccffcc
    style TPU fill:#ccffcc
```

#### ä¸ºä»€ä¹ˆä½¿ç”¨ None è€Œéé›¶å‘é‡ï¼Ÿ

Transformer ä»£ç ä¸­æœ‰è¿™æ ·çš„æ£€æŸ¥ï¼š

```python
if mask_type == "t2v" and torch.all(vision_states == 0):
    ...
```

`torch.all()` åœ¨ JIT ç¼–è¯‘æ—¶ä¼šå¯¼è‡´ ConcretizationTypeErrorï¼Œå› ä¸ºå®ƒéœ€è¦å…·ä½“çš„å¸ƒå°”å€¼ã€‚ä½¿ç”¨ `None` å¯ä»¥å®Œå…¨è·³è¿‡è¿™ä¸ªåˆ†æ”¯ï¼Œé¿å…é—®é¢˜ã€‚

---

## 4. å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ

### 4.1 ConcretizationTypeError

```mermaid
flowchart TB
    A["JIT ç¼–è¯‘æ¨¡å‹"] --> B{"é‡åˆ°æ¡ä»¶åˆ¤æ–­?"}
    B -->|"if tensor.max() > 1"| C["ConcretizationTypeError!"]
    B -->|"if static_arg == 'value'"| D["æ­£å¸¸ç¼–è¯‘"]
    C --> E["è§£å†³æ–¹æ¡ˆ"]
    E --> E1["1. é¢„è®¡ç®—ç§»åˆ° JIT å¤–"]
    E --> E2["2. Monkey-patch ç§»é™¤æ£€æŸ¥"]
    E --> E3["3. ä½¿ç”¨ static_argnames"]
```

**å¸¸è§è§¦å‘åœºæ™¯ï¼š**

| ä»£ç æ¨¡å¼ | é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|----------|------|----------|
| `if tensor.max() > 1:` | éœ€è¦å…·ä½“å€¼ | ç§»åˆ° JIT å¤–æˆ–ç§»é™¤ |
| `assert tensor.min() >= 0` | æ–­è¨€éœ€è¦å…·ä½“å€¼ | Monkey-patch ç§»é™¤ |
| `torch.all(x == 0)` | éœ€è¦å…·ä½“å¸ƒå°”å€¼ | ä¼ å…¥ None è·³è¿‡åˆ†æ”¯ |
| `tensor.item()` | éœ€è¦æ ‡é‡å€¼ | ä½¿ç”¨ tensor è¿ç®—ä»£æ›¿ |

### 4.2 å¸ƒå°”ç´¢å¼•ä¸æ”¯æŒ

```mermaid
flowchart LR
    A["tensor[bool_mask]"] --> B["torchax ä¸æ”¯æŒ!"]
    B --> C["è§£å†³æ–¹æ¡ˆ"]
    C --> C1["torch.where()"]
    C --> C2["* mask ä¹˜æ³•"]
    C --> C3["ç®€åŒ–é€»è¾‘é¿å…"]
```

**ç¤ºä¾‹ä¿®å¤ï¼š**

```python
# âŒ é”™è¯¯
selected = tensor[~mask]

# âœ… æ–¹æ¡ˆ 1: torch.where
selected = torch.where(mask.unsqueeze(-1), tensor, torch.zeros_like(tensor))

# âœ… æ–¹æ¡ˆ 2: ä¹˜æ³•
selected = tensor * mask.unsqueeze(-1).float()
```

### 4.3 åŠ¨æ€ Tensor åˆ›å»º

```mermaid
flowchart TB
    A["JIT å†…éƒ¨è°ƒç”¨<br/>torch.arange()"] --> B["æ¯æ¬¡é‡æ–°ç¼–è¯‘!"]
    B --> C["æ€§èƒ½æå·®"]
    C --> D["è§£å†³æ–¹æ¡ˆï¼šé¢„è®¡ç®—"]
    D --> E["åœ¨ JIT å¤–è®¡ç®—ä¸€æ¬¡"]
    E --> F["ç¼“å­˜åˆ°æ¨¡å‹å±æ€§"]
    F --> G["JIT å†…ä½¿ç”¨ç¼“å­˜"]
```

**ç¤ºä¾‹ï¼šRotary Position Embeddings**

```python
# åœ¨ JIT ç¼–è¯‘å‰é¢„è®¡ç®—
with torch.no_grad():
    freqs_cos, freqs_sin = model.get_rotary_pos_embed((t, h, w))
    with env:
        model._cached_freqs_cos = freqs_cos.to('jax')
        model._cached_freqs_sin = freqs_sin.to('jax')

# Monkey-patch ä½¿ç”¨ç¼“å­˜
def cached_get_rotary_pos_embed(self, latent_size):
    return self._cached_freqs_cos, self._cached_freqs_sin
model.get_rotary_pos_embed = types.MethodType(cached_get_rotary_pos_embed, model)
```

### 4.4 Scheduler dtype é—®é¢˜

```mermaid
flowchart LR
    A["latents<br/>bf16"] --> B["scheduler.step()"]
    B --> C["å†…éƒ¨è½¬ fp32<br/>ç²¾åº¦ä¿æŠ¤"]
    C --> D["è¾“å‡º fp32"]
    D --> E["éœ€è¦è½¬å› bf16!"]
    E --> F["latents.to(bf16)"]
```

```python
# æ¯æ¬¡ scheduler.step åè½¬å› bf16
latents = scheduler.step(noise_pred, t, latents)[0]
latents = latents.to(target_dtype)  # è½¬å› bf16
```

### 4.5 OOM é—®é¢˜

```mermaid
flowchart TB
    A["åˆ›å»º attention mask<br/>[B, H, S, S]"] --> B{"S = 26456?"}
    B -->|Yes| C["çŸ©é˜µå¤ªå¤§<br/>26456 x 26456 x 2 x 64"]
    C --> D["OOM!"]
    B -->|No| E["æ­£å¸¸"]
    
    D --> F["è§£å†³æ–¹æ¡ˆ"]
    F --> F1["ä½¿ç”¨ Splash Attention<br/>åˆ†å—è®¡ç®—"]
    F --> F2["ä¸åˆ›å»ºå®Œæ•´ mask<br/>K/V ç½®é›¶è¿‘ä¼¼"]
```

---

## 5. å®Œæ•´è¿ç§»æµç¨‹

```mermaid
flowchart TB
    subgraph PREP["ğŸ“‹ å‡†å¤‡é˜¶æ®µ"]
        P1[åˆ†æ GPU ä»£ç ] --> P2[è¯†åˆ«ä¸å…¼å®¹æ¨¡å¼]
        P2 --> P3[è®¾è®¡è§£å†³æ–¹æ¡ˆ]
    end
    
    subgraph SETUP["âš™ï¸ ç¯å¢ƒè®¾ç½®"]
        S1[åˆ›å»º JAX Mesh] --> S2[åˆ›å»º torchax ç¯å¢ƒ]
        S2 --> S3[æ³¨å†Œ Splash Attention]
    end
    
    subgraph PATCH["ğŸ”§ ä»£ç é€‚é…"]
        M1[Mock GPU åˆ†å¸ƒå¼çŠ¶æ€] --> M2[Patch ä¸å…¼å®¹å‡½æ•°]
        M2 --> M3[å¯¼å…¥æ¨¡å‹]
    end
    
    subgraph LOAD["ğŸ“¦ æ¨¡å‹åŠ è½½"]
        L1[åŠ è½½æ¨¡å‹] --> L2[è½¬æ¢æƒé‡åˆ° XLA]
        L2 --> L3[æƒé‡åˆ†ç‰‡]
        L3 --> L4[é¢„è®¡ç®—åŠ¨æ€ Tensor]
    end
    
    subgraph COMPILE["ğŸš€ ç¼–è¯‘è¿è¡Œ"]
        C1[JIT ç¼–è¯‘] --> C2[æ¨ç†å¾ªç¯]
        C2 --> C3[ä¿å­˜ç»“æœ]
        C3 --> C4[æ˜¾å¼é€€å‡º]
    end
    
    PREP --> SETUP --> PATCH --> LOAD --> COMPILE
```

### æ­¥éª¤ 1: åˆ›å»º JAX Mesh

```python
from jax.sharding import Mesh
from jax.experimental import mesh_utils

tp_dim = jax.device_count()  # 8 ä¸ª TPU cores
dp_dim = 1
sp_dim = 1

mesh_devices = mesh_utils.create_device_mesh(
    (tp_dim, dp_dim, sp_dim),
    allow_split_physical_axes=True
)
mesh = Mesh(mesh_devices, ('tp', 'dp', 'sp'))
```

### æ­¥éª¤ 2: åˆ›å»º torchax ç¯å¢ƒ

```python
import torchax

env = torchax.default_env()
env._mesh = mesh
env.config.use_tpu_splash_attention = True

torch.set_default_dtype(torch.bfloat16)
```

### æ­¥éª¤ 3: æ³¨å†Œ Splash Attention

```python
# ä¿å­˜åŸå§‹ SDPA
_ORIGINAL_SDPA = torch.nn.functional.scaled_dot_product_attention

# æ³¨å†Œè‡ªå®šä¹‰ attention
custom_attention = functools.partial(scaled_dot_product_attention, env=env)
env._ops[torch.nn.functional.scaled_dot_product_attention] = ops_registry.Operator(
    torch.nn.functional.scaled_dot_product_attention,
    custom_attention,
    is_jax_function=False,
    is_user_defined=True,
    needs_env=False,
    is_view_op=False,
)
```

### æ­¥éª¤ 4: Monkey-Patch ä¸å…¼å®¹ä»£ç 

**å¿…é¡»åœ¨å¯¼å…¥æ¨¡å‹ä¹‹å‰ï¼**

```python
# Mock GPU åˆ†å¸ƒå¼çŠ¶æ€
import module.parallel_states as ps
from types import SimpleNamespace

ps.get_parallel_state = lambda: SimpleNamespace(
    sp=1,
    sp_enabled=False,
    sp_group=None,
)

# Patch æœ‰é—®é¢˜çš„å‡½æ•°
def patched_function(...):
    # ç§»é™¤è¿è¡Œæ—¶æ£€æŸ¥ï¼Œç®€åŒ–é€»è¾‘
    pass
module.original_function = patched_function

# ç°åœ¨æ‰å¯¼å…¥æ¨¡å‹
from module.model import Model
```

### æ­¥éª¤ 5: åŠ è½½å’Œè½¬æ¢æ¨¡å‹

```python
model = Model.from_pretrained(path, torch_dtype=torch.bfloat16)

with env:
    with jax.default_device('cpu'):
        state_dict = model.state_dict()
        state_dict = env.to_xla(state_dict)
        model.load_state_dict(state_dict, assign=True)
    
    weights = shard_weights(mesh, model.state_dict())
    model.load_state_dict(weights, assign=True, strict=False)
    torchax.interop.call_jax(jax.block_until_ready, weights)

model.eval()
```

### æ­¥éª¤ 6: é¢„è®¡ç®—å¹¶ JIT ç¼–è¯‘

```python
# é¢„è®¡ç®—åŠ¨æ€ tensor
with torch.no_grad():
    freqs = model.get_rotary_pos_embed(size)
    with env:
        model._cached_freqs = freqs.to('jax')

# JIT ç¼–è¯‘
with env:
    model = torchax.compile(model, torchax.CompileOptions(
        jax_jit_kwargs={'static_argnames': ('return_dict',)}
    ))
```

### æ­¥éª¤ 7: æ¨ç†å¾ªç¯

```python
with mesh, env:
    with torch.no_grad():
        for i, t in enumerate(timesteps):
            output = model(inputs)
            latents = scheduler.step(output, t, latents)[0]
            latents = latents.to(target_dtype)  # è½¬å› bf16

# ä¿å­˜ç»“æœ
save_results(latents.cpu())

# æ˜¾å¼é€€å‡º
sys.exit(0)
```

---

## 6. ä»£ç æ¨¡æ¿

### 6.1 Splash Attention å®Œæ•´å®ç°

```python
from jax.experimental.pallas.ops.tpu import splash_attention
from jax.experimental.shard_map import shard_map

BQSIZE = 2048
BKVSIZE = 2048
BKVCOMPUTESIZE = 1024

def _tpu_splash_attention(query, key, value, mesh, scale=None, window_size=None):
    """
    TPU Splash Attention å®ç°
    
    Args:
        query: [B, H, Sq, D]
        key: [B, H, Skv, D]
        value: [B, H, Skv, D]
        mesh: JAX è®¾å¤‡ mesh
        scale: ç¼©æ”¾å› å­ï¼Œé»˜è®¤ 1/sqrt(D)
        window_size: å±€éƒ¨æ³¨æ„åŠ›çª—å£å¤§å°ï¼ŒNone è¡¨ç¤ºå…¨å±€æ³¨æ„åŠ›
    """
    num_heads = query.shape[1]

    def _attention_on_slices(q, k, v):
        scale_factor = 1.0 / math.sqrt(q.shape[-1]) if scale is None else scale
        q = q * scale_factor

        def pad_to_multiple(x, multiple, axis):
            seq_len = x.shape[axis]
            pad_len = (multiple - seq_len % multiple) % multiple
            if pad_len == 0:
                return x, seq_len
            pad_width = [(0, 0)] * x.ndim
            pad_width[axis] = (0, pad_len)
            return jnp.pad(x, pad_width), seq_len

        def kernel_3d(q_3d, k_3d, v_3d):
            num_heads_on_device = q_3d.shape[0]
            
            q_3d_padded, q_orig_len = pad_to_multiple(q_3d, BQSIZE, axis=1)
            k_3d_padded, _ = pad_to_multiple(k_3d, BKVSIZE, axis=1)
            v_3d_padded, _ = pad_to_multiple(v_3d, BKVSIZE, axis=1)

            if window_size is not None:
                mask_class = functools.partial(
                    splash_attention.LocalMask, 
                    window_size=window_size
                )
            else:
                mask_class = splash_attention.FullMask

            mask = splash_attention.MultiHeadMask([
                mask_class((q_3d_padded.shape[1], k_3d_padded.shape[1]))
                for _ in range(num_heads_on_device)
            ])

            block_sizes = splash_attention.BlockSizes(
                block_q=min(BQSIZE, q_3d_padded.shape[1]),
                block_kv=min(BKVSIZE, k_3d_padded.shape[1]),
                block_kv_compute=min(BKVCOMPUTESIZE, k_3d_padded.shape[1]),
            )
            
            kernel = splash_attention.make_splash_mha(
                mask=mask, block_sizes=block_sizes
            )
            out = kernel(q_3d_padded, k_3d_padded, v_3d_padded)
            return out[:, :q_orig_len, ...]

        return jax.vmap(kernel_3d)(q, k, v)

    # åˆ†ç‰‡è§„åˆ™
    q_spec = P('dp', 'tp', 'sp', None)
    kv_spec = P('dp', 'tp', None, None)

    sharded_fn = shard_map(
        _attention_on_slices,
        mesh=mesh,
        in_specs=(q_spec, kv_spec, kv_spec),
        out_specs=q_spec,
        check_rep=False,
    )
    return sharded_fn(query, key, value)
```

### 6.2 æƒé‡åˆ†ç‰‡æ¨¡æ¿

```python
from jax.sharding import PartitionSpec as P, NamedSharding
import re

# Tensor Parallel: Column-Row æ¨¡å¼
sharding_rules = {
    # Column Parallel: åœ¨ output ç»´åº¦åˆ†ç‰‡
    r'.*\.q_proj\.weight$': (('tp', 'sp'), None),
    r'.*\.k_proj\.weight$': (('tp', 'sp'), None),
    r'.*\.v_proj\.weight$': (('tp', 'sp'), None),
    r'.*\.fc1\.weight$': (('tp', 'sp'), None),
    
    # Row Parallel: åœ¨ input ç»´åº¦åˆ†ç‰‡
    r'.*\.o_proj\.weight$': (None, ('tp', 'sp')),
    r'.*\.fc2\.weight$': (None, ('tp', 'sp')),
}

def shard_weights(mesh, weights, rules):
    matched = 0
    for name, tensor in weights.items():
        for pattern, spec in rules.items():
            if re.fullmatch(pattern, name):
                tensor.apply_jax_(jax.device_put, NamedSharding(mesh, P(*spec)))
                matched += 1
                break
        else:
            # æœªåŒ¹é…ï¼šå¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡
            tensor.apply_jax_(jax.device_put, NamedSharding(mesh, P()))
    
    print(f"åˆ†ç‰‡å®Œæˆ: {matched} ä¸ªåŒ¹é…, {len(weights)-matched} ä¸ªå¤åˆ¶")
    return weights
```

---

## 7. æ€§èƒ½ä¼˜åŒ–

### 7.1 JIT ç¼–è¯‘ç¼“å­˜

```python
# å¯ç”¨æŒä¹…åŒ–ç¼“å­˜
jax.config.update("jax_compilation_cache_dir", "/dev/shm/jax_cache")
jax.config.update("jax_persistent_cache_min_entry_size_bytes", -1)
jax.config.update("jax_persistent_cache_min_compile_time_secs", 0)
```

**æ•ˆæœï¼š**
- é¦–æ¬¡è¿è¡Œï¼š~60s ç¼–è¯‘
- åç»­è¿è¡Œï¼š~5s åŠ è½½ç¼“å­˜

### 7.2 dtype ä¼˜åŒ–

```mermaid
flowchart LR
    A["æ¨¡å‹æƒé‡<br/>bf16"] --> B["ä¸­é—´è®¡ç®—<br/>bf16"]
    B --> C["æ³¨æ„åŠ›<br/>bf16"]
    C --> D["è¾“å‡º<br/>bf16"]
    
    E["ç‰¹æ®Šæƒ…å†µ"] --> E1["ByT5: float32<br/>ç²¾åº¦æ•æ„Ÿ"]
    E --> E2["Scheduler: float32<br/>ç´¯åŠ ç²¾åº¦"]
```

### 7.3 æ€§èƒ½åŸºå‡†

| é…ç½® | Token æ•° | æ€»æ—¶é—´ | æ¯æ­¥æ—¶é—´ |
|------|----------|--------|----------|
| 25å¸§, 720p | 25,200 | 114s | 2.3s |
| 49å¸§, 720p | 46,800 | 216s | 4.3s |
| 121å¸§, 720p | 111,600 | 512s | 10.3s |

---

## 8. è°ƒè¯•æŠ€å·§

### 8.1 æŸ¥çœ‹å®Œæ•´ traceback

```bash
JAX_TRACEBACK_FILTERING=off python script.py
```

### 8.2 é€æ­¥æµ‹è¯•

```python
# å…ˆç”¨ 1 æ­¥æµ‹è¯•
args.num_inference_steps = 1
# æˆåŠŸåå†å¢åŠ 
```

### 8.3 æ£€æµ‹ XLA tensor

```python
def is_xla_tensor(tensor):
    if tensor is None:
        return False
    if hasattr(tensor, '_elem'):
        return True
    if hasattr(tensor, 'device'):
        return 'jax' in str(tensor.device) or 'xla' in str(tensor.device)
    return False
```

### 8.4 è°ƒè¯•æ‰“å°

```python
def debug_tensor(name, t):
    if t is None:
        print(f"{name}: None")
    else:
        print(f"{name}: shape={t.shape}, dtype={t.dtype}, "
              f"mean={t.float().mean().item():.4f}")
```

---

## ğŸ“‹ è¿ç§» Checklist

### å¼€å§‹å‰

- [ ] è¯†åˆ«æ‰€æœ‰ CUDA ç‰¹å®šä»£ç 
- [ ] è¯†åˆ«æ‰€æœ‰è¿è¡Œæ—¶æ£€æŸ¥ (assert, if tensor.max())
- [ ] è¯†åˆ«æ‰€æœ‰åŠ¨æ€ tensor åˆ›å»º (torch.arange, torch.zeros)
- [ ] è¯†åˆ«æ‰€æœ‰å¸ƒå°”ç´¢å¼•
- [ ] ç¡®è®¤ dtype è¦æ±‚

### è¿ç§»ä¸­

- [ ] åˆ›å»º JAX Mesh
- [ ] æ³¨å†Œ Splash Attention
- [ ] Monkey-patch ä¸å…¼å®¹ä»£ç 
- [ ] åŠ è½½å¹¶åˆ†ç‰‡æƒé‡
- [ ] é¢„è®¡ç®—åŠ¨æ€ tensor
- [ ] JIT ç¼–è¯‘æ¨¡å‹

### å®Œæˆå

- [ ] ç¨‹åºæ­£å¸¸é€€å‡º
- [ ] è¾“å‡º dtype æ­£ç¡® (bf16)
- [ ] æ—  OOM é—®é¢˜
- [ ] ç”Ÿæˆè´¨é‡æ­£ç¡®

---

## ğŸ“š å‚è€ƒèµ„æº

- [torchax GitHub](https://github.com/pytorch/xla)
- [JAX Splash Attention](https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention)
- [JAX shard_map](https://jax.readthedocs.io/en/latest/notebooks/shard_map.html)
- [HunyuanVideo-1.5](https://github.com/Tencent/HunyuanVideo)