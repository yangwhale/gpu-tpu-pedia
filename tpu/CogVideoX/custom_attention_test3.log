/home/chrisya/.local/lib/python3.12/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.
Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.
/home/chrisya/gpu-tpu-pedia/tpu/CogVideoX/generate_flax.py:20: DeprecationWarning: jax.experimental.shard_map is deprecated in v0.8.0. Used jax.shard_map instead.
  from jax.experimental.shard_map import shard_map
æ³¨å†ŒPyTreeèŠ‚ç‚¹...
  - BaseModelOutputWithPooling å·²æ³¨å†Œ
  - BaseModelOutputWithPastAndCrossAttentions å·²æ³¨å†Œ
  - DecoderOutput å·²æ³¨å†Œ
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:01,  1.83it/s][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:01<00:01,  1.91it/s][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:01<00:00,  1.99it/s][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.61it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.31it/s]
Loading pipeline components...:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:07,  1.76s/it]Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.03it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 81.04it/s]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  3.58it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.48it/s]
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.

é…ç½®Pipelineä»¥ä½¿ç”¨JAXã€Splash Attention å’Œ Flax VAE...

é…ç½®Pipelineä»¥ä½¿ç”¨JAX...
  Mesh ç»´åº¦: tp_dim=8, dp_dim=1, sp_dim=1
- æ³¨å†Œ Custom Splash Attention (exp2)ï¼ˆçª—å£å¤§å°: Noneï¼‰...
å°†schedulerå‚æ•°ç§»åŠ¨åˆ°JAXè®¾å¤‡...
- å°†Text Encoderç§»åˆ°XLAå¹¶è¿›è¡Œåˆ†ç‰‡...
- åŠ è½½ Flax VAE (åŽŸç”Ÿ JAX å®žçŽ°)...
[1/4] åŠ è½½é…ç½®: zai-org/CogVideoX1.5-5B
[2/4] ä¸‹è½½ PyTorch æƒé‡...
[3/4] è½¬æ¢æƒé‡åˆ° JAX æ ¼å¼...
  âœ“ åŠ è½½äº† 436 ä¸ª PyTorch æƒé‡å¼ é‡
  âœ“ è½¬æ¢äº† 436 ä¸ªæƒé‡åˆ° JAX æ ¼å¼
[4/4] åˆå§‹åŒ–æ¨¡åž‹å¹¶åŠ è½½æƒé‡...
âœ“ æ¨¡åž‹åŠ è½½å®Œæˆ!
  é…ç½®: FlaxAutoencoderKLCogVideoXConfig(config_name='config.json', in_channels=3, out_channels=3, down_block_types=['CogVideoXDownBlock3D', 'CogVideoXDownBlock3D', 'CogVideoXDownBlock3D', 'CogVideoXDownBlock3D'], up_block_types=['CogVideoXUpBlock3D', 'CogVideoXUpBlock3D', 'CogVideoXUpBlock3D', 'CogVideoXUpBlock3D'], block_out_channels=[128, 256, 256, 512], latent_channels=16, layers_per_block=3, act_fn='silu', norm_eps=1e-06, norm_num_groups=32, temporal_compression_ratio=4, sample_height=480, sample_width=720, scaling_factor=0.7, shift_factor=None, latents_mean=None, latents_std=None, force_upcast=True, use_quant_conv=False, use_post_quant_conv=False, pad_mode='first')
- è·³è¿‡ VAE Tiling é…ç½®ï¼ˆæµ‹è¯•å®Œæ•´è§£ç ï¼‰...
  âœ“ Flax VAE å·²æ›¿æ¢ Pipeline çš„ VAEï¼ˆæ— ä¸å½“ JITï¼‰
Pipelineé…ç½®å®Œæˆ

è¿è¡Œ2æ¬¡è§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•...
æç¤ºè¯: 'A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.'
æŽ¨ç†æ­¥æ•°: 10
è§†é¢‘å¸§æ•°: 81
åˆ†è¾¨çŽ‡: 768x1360
å¼•å¯¼å°ºåº¦: 6.0

è¿­ä»£ 0 (åŒ…å« JIT ç¼–è¯‘ï¼Œä¼šæ¯”è¾ƒæ…¢):
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:13<01:58, 13.21s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:26<01:44, 13.01s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:30<01:02,  8.90s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:34<00:41,  6.97s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:38<00:29,  5.90s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:42<00:21,  5.26s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:46<00:14,  4.85s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:50<00:09,  4.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:54<00:04,  4.40s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:58<00:00,  4.29s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:58<00:00,  5.82s/it]
/home/chrisya/torchax/torchax/ops/mappings.py:84: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)
  res = torch.from_numpy(numpy.asarray(x))
  å®Œæˆæ—¶é—´: 86.83 ç§’ (åŒ…å« Transformer + Text Encoder çš„çœŸæ­£ JIT ç¼–è¯‘)

è¿­ä»£ 1 (ä½¿ç”¨å·²ç¼–è¯‘ä»£ç ):
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:04<00:40,  4.51s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:08<00:33,  4.22s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:28,  4.12s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:24,  4.08s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:20<00:20,  4.05s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:24<00:16,  4.04s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:28<00:12,  4.03s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:32<00:08,  4.02s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:36<00:04,  4.02s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:40<00:00,  4.01s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:40<00:00,  4.06s/it]
/usr/lib/python3.12/subprocess.py:1885: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = _fork_exec(
  å®Œæˆæ—¶é—´: 66.63 ç§’

ä¿å­˜ç”Ÿæˆçš„è§†é¢‘...
è§†é¢‘å·²ä¿å­˜åˆ°: output_video_flax_vae.mp4
æ€»è¿­ä»£æ¬¡æ•°: 2
ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆå«ç¼–è¯‘ï¼‰: 86.8335 ç§’
åŽç»­è¿è¡Œå¹³å‡æ—¶é—´: 66.6309 ç§’
åŠ é€Ÿæ¯”: 1.30x

è¯´æ˜Ž:
- ç¬¬ä¸€æ¬¡è¿è¡ŒåŒ…å«JITç¼–è¯‘æ—¶é—´, å› æ­¤è¾ƒæ…¢
- åŽç»­è¿è¡Œä½¿ç”¨ç¼–è¯‘åŽçš„ä»£ç , é€Ÿåº¦æ˜¾è‘—æå‡

å„æ¬¡è¿­ä»£è¯¦ç»†æ—¶é—´:
  è¿­ä»£ 0: 86.8335 ç§’
  è¿­ä»£ 1: 66.6309 ç§’

âœ“ æµ‹è¯•å®Œæˆï¼Œç¨‹åºé€€å‡º
