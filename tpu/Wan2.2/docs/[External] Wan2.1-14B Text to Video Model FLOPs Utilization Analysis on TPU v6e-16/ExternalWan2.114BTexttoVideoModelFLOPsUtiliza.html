<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=DFQxm4rd7fRHgM9OTejWVYbTZVi_Y5F9JqoQGMBQEYTYnt9737GpWhMUMfToBGM396xlbbE5D7Gw2o7jubnkMA);.lst-kix_wmwho698ixcb-6>li:before{content:"" counter(lst-ctn-kix_wmwho698ixcb-6,decimal) ". "}.lst-kix_wmwho698ixcb-7>li:before{content:"" counter(lst-ctn-kix_wmwho698ixcb-7,lower-latin) ". "}ol.lst-kix_wmwho698ixcb-2.start{counter-reset:lst-ctn-kix_wmwho698ixcb-2 0}.lst-kix_wmwho698ixcb-4>li:before{content:"" counter(lst-ctn-kix_wmwho698ixcb-4,lower-latin) ". "}.lst-kix_wmwho698ixcb-8>li:before{content:"" counter(lst-ctn-kix_wmwho698ixcb-8,lower-roman) ". "}.lst-kix_wmwho698ixcb-2>li:before{content:"" counter(lst-ctn-kix_wmwho698ixcb-2,lower-roman) ". "}.lst-kix_wmwho698ixcb-3>li:before{content:"" counter(lst-ctn-kix_wmwho698ixcb-3,decimal) ". "}.lst-kix_wmwho698ixcb-3>li{counter-increment:lst-ctn-kix_wmwho698ixcb-3}.lst-kix_wmwho698ixcb-1>li:before{content:"" counter(lst-ctn-kix_wmwho698ixcb-1,lower-latin) ". "}ol.lst-kix_wmwho698ixcb-5.start{counter-reset:lst-ctn-kix_wmwho698ixcb-5 0}.lst-kix_wmwho698ixcb-4>li{counter-increment:lst-ctn-kix_wmwho698ixcb-4}.lst-kix_wmwho698ixcb-5>li:before{content:"" counter(lst-ctn-kix_wmwho698ixcb-5,lower-roman) ". "}ol.lst-kix_wmwho698ixcb-6.start{counter-reset:lst-ctn-kix_wmwho698ixcb-6 0}ul.lst-kix_3tf2ctypfaxh-8{list-style-type:none}ul.lst-kix_3tf2ctypfaxh-7{list-style-type:none}ul.lst-kix_3tf2ctypfaxh-6{list-style-type:none}ul.lst-kix_3tf2ctypfaxh-5{list-style-type:none}ul.lst-kix_3tf2ctypfaxh-4{list-style-type:none}ul.lst-kix_3tf2ctypfaxh-3{list-style-type:none}ul.lst-kix_3tf2ctypfaxh-2{list-style-type:none}ul.lst-kix_3tf2ctypfaxh-1{list-style-type:none}.lst-kix_wmwho698ixcb-0>li:before{content:"" counter(lst-ctn-kix_wmwho698ixcb-0,decimal) ". "}ol.lst-kix_wmwho698ixcb-3.start{counter-reset:lst-ctn-kix_wmwho698ixcb-3 0}.lst-kix_3tf2ctypfaxh-3>li:before{content:"\0025cf   "}ol.lst-kix_wmwho698ixcb-7.start{counter-reset:lst-ctn-kix_wmwho698ixcb-7 0}.lst-kix_3tf2ctypfaxh-2>li:before{content:"\0025a0   "}.lst-kix_3tf2ctypfaxh-4>li:before{content:"\0025cb   "}.lst-kix_3tf2ctypfaxh-7>li:before{content:"\0025cb   "}.lst-kix_wmwho698ixcb-6>li{counter-increment:lst-ctn-kix_wmwho698ixcb-6}.lst-kix_wmwho698ixcb-0>li{counter-increment:lst-ctn-kix_wmwho698ixcb-0}.lst-kix_3tf2ctypfaxh-6>li:before{content:"\0025cf   "}ol.lst-kix_wmwho698ixcb-0.start{counter-reset:lst-ctn-kix_wmwho698ixcb-0 0}.lst-kix_3tf2ctypfaxh-5>li:before{content:"\0025a0   "}ol.lst-kix_wmwho698ixcb-4.start{counter-reset:lst-ctn-kix_wmwho698ixcb-4 0}ul.lst-kix_3tf2ctypfaxh-0{list-style-type:none}.lst-kix_wmwho698ixcb-7>li{counter-increment:lst-ctn-kix_wmwho698ixcb-7}.lst-kix_wmwho698ixcb-1>li{counter-increment:lst-ctn-kix_wmwho698ixcb-1}.lst-kix_3tf2ctypfaxh-0>li:before{content:"\0025cf   "}.lst-kix_3tf2ctypfaxh-1>li:before{content:"\0025cb   "}ol.lst-kix_wmwho698ixcb-1.start{counter-reset:lst-ctn-kix_wmwho698ixcb-1 0}ol.lst-kix_wmwho698ixcb-7{list-style-type:none}ol.lst-kix_wmwho698ixcb-6{list-style-type:none}.lst-kix_wmwho698ixcb-2>li{counter-increment:lst-ctn-kix_wmwho698ixcb-2}ol.lst-kix_wmwho698ixcb-5{list-style-type:none}ol.lst-kix_wmwho698ixcb-4{list-style-type:none}.lst-kix_wmwho698ixcb-5>li{counter-increment:lst-ctn-kix_wmwho698ixcb-5}ol.lst-kix_wmwho698ixcb-8.start{counter-reset:lst-ctn-kix_wmwho698ixcb-8 0}ol.lst-kix_wmwho698ixcb-8{list-style-type:none}ol.lst-kix_wmwho698ixcb-3{list-style-type:none}ol.lst-kix_wmwho698ixcb-2{list-style-type:none}.lst-kix_wmwho698ixcb-8>li{counter-increment:lst-ctn-kix_wmwho698ixcb-8}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ol.lst-kix_wmwho698ixcb-1{list-style-type:none}ol.lst-kix_wmwho698ixcb-0{list-style-type:none}.lst-kix_3tf2ctypfaxh-8>li:before{content:"\0025a0   "}ol{margin:0;padding:0}table td,table th{padding:0}.c17{border-right-style:solid;padding:7pt 7pt 7pt 7pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c13{border-right-style:solid;padding:7pt 7pt 7pt 7pt;border-bottom-color:#9e9e9e;border-top-width:1pt;border-right-width:1pt;border-left-color:#9e9e9e;vertical-align:top;border-right-color:#9e9e9e;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#9e9e9e;border-bottom-style:solid}.c3{border-right-style:solid;padding:7pt 7pt 7pt 7pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#9e9e9e;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c18{border-right-style:solid;padding:7pt 7pt 7pt 7pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#9e9e9e;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Google Sans Text";font-style:normal}.c14{color:#4a86e8;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Google Sans";font-style:normal}.c22{padding-top:24pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c4{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c5{color:#1976d2;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Google Sans";font-style:normal}.c16{padding-top:14pt;padding-bottom:10pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c8{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c10{padding-top:12pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c12{color:#4175ca;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Google Sans Text";font-style:normal}.c19{color:#457ba7;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans";font-style:normal}.c23{color:#4576c5;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c2{padding-top:0pt;padding-bottom:10pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c25{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c26{border-spacing:0;border-collapse:collapse;margin-right:auto}.c11{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#0000ee;text-decoration:underline}.c6{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c21{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c24{border:1px solid black;margin:5px}.c9{margin-left:36pt;padding-left:0pt}.c7{height:11pt}.c15{height:0pt}.title{padding-top:0pt;color:#1976d2;font-size:28pt;padding-bottom:6pt;font-family:"Google Sans Text";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.subtitle{padding-top:0pt;color:#2196f3;font-size:12pt;padding-bottom:10pt;font-family:"Google Sans Text";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Google Sans Text"}p{margin:0;color:#000000;font-size:11pt;font-family:"Google Sans Text"}h1{padding-top:24pt;color:#1976d2;font-size:18pt;padding-bottom:6pt;font-family:"Google Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#4a86e8;font-size:14pt;padding-bottom:6pt;font-family:"Google Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#4175ca;font-size:12pt;padding-bottom:10pt;font-family:"Google Sans Text";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#4576c5;font-size:11pt;padding-bottom:4pt;font-family:"Open Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#457ba7;font-size:11pt;padding-bottom:4pt;font-family:"Open Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#457ba7;font-size:11pt;padding-bottom:4pt;font-family:"Open Sans";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c21 doc-content"><h1 class="c22" id="h.tqtqt9akzzbz"><span>[External] </span><span>Wan2.1-14B</span><sup><a href="#cmnt1" id="cmnt_ref1">[a]</a></sup><span class="c5">&nbsp;Text to Video Model FLOPs Utilization Analysis on TPU v6e-16</span></h1><p class="c2"><span>Contributors</span><span>: </span><span class="c11"><a href="mailto:yuyanpeng@google.com">Yuyan Peng</a></span><span>, </span><span class="c11"><a href="mailto:haoluo@google.com">Hao Luo</a></span><span>, </span><span class="c11"><a href="mailto:wdhongtw@google.com">Weida Hong</a></span><span>, &lt;add your name&gt;</span><span><br>Last major revision: </span><span>Aug 22, 2025</span><span class="c0"><br>Status: Reviewed</span></p><p class="c2 c7"><span class="c0"></span></p><h2 class="c4" id="h.q8w92vwolif5"><span class="c14">Abstract</span></h2><p class="c2"><span class="c0">This document analyzes the FLOPs (Floating Point Operations Per Second) utilization of the Wan2.1-14B text-to-video model on a TPU v6e-16. Our analysis reveals an overall Model FLOPs Utilization (MFU) of 34% for a single Diffusion Transformer (DiT) step. A significant bottleneck is identified in the self-attention mechanism, which accounts for approximately 66% of the execution time but achieves only 37% MFU. This low MFU is primarily attributed to the small head dimension (128) and the performance of element-wise operations within the softmax calculation in the splash attention kernel, which lead to MXU underutilization and VPU bound operations. While other linear operations demonstrate a higher MFU of around 66%, the overall non-self-attention MFU is 29% due to HBM-bound operations. These findings highlight key areas for optimization, particularly in refining the self-attention mechanism, to improve the overall efficiency of the Wan2.1 model.</span></p><p class="c2 c7"><span class="c0"></span></p><h2 class="c4" id="h.lbxbu3pngwuh"><span class="c14">Introduction</span></h2><p class="c2"><span class="c0">This document presents an analysis of the FLOPs (Floating Point Operations Per Second) utilization of the Wan2.1-14B text-to-video model when run on a TPU v6e-16 accelerator. The primary goal is to identify bottlenecks and areas for optimization to improve the efficiency of the model&#39;s execution. We will delve into specific components of the model, such as self-attention and other transformer block elements, to assess their individual MFU (Model FLOPs Utilization) and compare the overall MFU with that of other established large language models. The insights gained from this analysis will inform future optimization efforts for the Wan2.1 model.</span></p><p class="c2 c7"><span class="c0"></span></p><h2 class="c4" id="h.7v16zvpuscfr"><span>Background</span></h2><p class="c2"><span class="c0">The document focuses on analyzing generating a 720p 81 frames text to video with Wan2.1 14B model, using v6e-16.</span></p><p class="c2"><span class="c0">&ldquo;Profiling tutorial&rdquo; contains how to create profiling used in this document.</span></p><p class="c2"><span>The github </span><span class="c6"><a href="https://www.google.com/url?q=https://github.com/shungcp/diffusers&amp;sa=D&amp;source=editors&amp;ust=1765684247706678&amp;usg=AOvVaw1ck0IrgVmvGLGwJpHJBIeh">repo</a></span><span class="c0">&nbsp;contains the proof of concept (POC) codes.</span></p><p class="c2"><span class="c0">Self attentions apply context parallelism (CP) and cross attentions apply sequence parallelism (SP).</span></p><p class="c2"><span class="c0">Context parallelism (CP) shards along the head number dimension, all-to-all (A2A) operations are needed to re-shard activations from the sequence dimension to the head number dimension.</span></p><p class="c2"><span class="c0">Sequence parallelism (SP) shards Q along the sequence dimension and K and V fully replicated, only K and V need all-gather operations from sharding along the sequence dimension.</span></p><p class="c2"><span class="c0">Apply data parallelism for positive and negative prompts.</span></p><p class="c2"><span class="c0">Apply splash attention kernel for attention.</span></p><p class="c2"><span>See more </span><span>detail</span><span>&nbsp;in the Method in Appendix.</span></p><p class="c2 c7"><span class="c0"></span></p><h2 class="c4" id="h.hl2eoga266d0"><span class="c14">Roofline Analysis</span></h2><p class="c2"><span class="c0">Model FLOPs Utilization (MFU) calculates the percentage of actual FLOPs by theoretical FLOPs.</span></p><p class="c2"><span class="c0">Each v6e chip has 918 peak bf16 TFLOPs and HBM bandwidth 1638 GB/s.</span></p><p class="c2"><span class="c0">The analysis is focusing on one prompt with 8 chips while data parallelism (DP) is used in diffusion transformers (DiT) by stacking positive prompt and negative prompt.</span></p><p class="c2 c7"><span class="c0"></span></p><h3 class="c16" id="h.2m8w6eawwfn4"><span class="c12">Overview</span></h3><p class="c2"><span class="c0">A video can be generated in 141 seconds. For each video, about 132s taken by DiT and 9s taken by VAE while text encoder time taken less than 100ms.</span></p><p class="c2"><span class="c0">The DiT takes 50 diffusion steps, and contains 40 layers of transformer blocks for each step.</span></p><p class="c2"><span class="c0">This document focuses on transformer blocks in DiT since it takes most of the time.</span></p><p class="c2"><span class="c0">In each transformer block, there is a self attention take 2/3 of the time. This document focuses on analysis of self attention and others in transformer blocks.</span></p><p class="c2"><span class="c0">See Profiling section in the Appendix for more details.</span></p><p class="c2 c7"><span class="c0"></span></p><h3 class="c16" id="h.2x75e1jutxs7"><span>Self Attention</span></h3><p class="c2"><span class="c0">The self-attention operations are shown in custom-call and take about 66% of the time. The FLOPs is not supported for the custom pallas kernel now.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 142.67px;"><img alt="" src="images/image9.png" style="width: 624.00px; height: 142.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">The latency from the profiling is 43.93ms.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 322.00px;"><img alt="" src="images/image1.png" style="width: 402.00px; height: 322.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span>Each self attention has the roofline of total latency 15.974ms and is compute bound. </span><span>See detailed calculation</span><span class="c0">&nbsp;in the Self Attention Roofline Calculation section in Appendix.</span></p><p class="c2"><span class="c0">The self attention MFU is about 15.974 / 43.93 = 37%.</span></p><p class="c2 c7"><span class="c0"></span></p><h3 class="c16" id="h.us6wqgr4cs3p"><span class="c12">Others in Transformer Blocks</span></h3><p class="c2"><span class="c0">In each attention block, besides self-attention, </span><span>latency is majorly taken by </span><span class="c0">a feed forwar</span><span>d network</span><span class="c0">&nbsp;(</span><span class="c0">FFN</span><span class="c0">)</span><span>&nbsp;, </span><span class="c0">attention projections, A2A communication, rope embedding, and cross-attention.</span></p><p class="c2"><span class="c0">The cross-attention part is in the custom-call and takes 0.93% time shown in the profiling.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 136.00px;"><img alt="" src="images/image10.png" style="width: 624.00px; height: 136.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">The all linear layers would be shown as convolution fusion in the profiling, which takes 14.28% time and FLOPs which means MFU shows in profiling is 66%.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 154.67px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 154.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span>For simplicity, FLOPs taken by cross-attention is</span><span>&nbsp;excluded from this analysis</span><span class="c0">&nbsp;as its contributions are relatively small due to K, V sequence length is 512 compared to 75600 in self-attention.</span></p><p class="c2"><span class="c0">Assume all the computations other than the self-attention are in the convolution fusion.</span></p><p class="c2 c7"><span class="c0"></span></p><h4 class="c8" id="h.a5fl48kfv2lv"><span class="c23">FFN + Attention Projection</span></h4><p class="c2"><span class="c0">FFN contains two linear with weight shapes [5120, 13824] and [13824, 5120].</span></p><p class="c2"><span class="c0">Linear project Q, K, and V with weight shapes [5120, 5120]. There are 2 Q projections, and 1 K, V projection since the cross attention K and V are constant, and 2 attention output projections.</span></p><p class="c2"><span>The activation shape are [batch, sequence length, embedding dimension] = &nbsp;[B, T, D] = [</span><span>1</span><span class="c0">, 75600, 5120]</span></p><p class="c2"><span class="c0">Linear projections need (2 B T D F / |X| C) flops, F for inner dimension, |X| for sharding along devices.</span></p><p class="c2"><span>The latencies</span><span>&nbsp;in</span></p><p class="c2"><span class="c0">FFN: 2 * (2 * 1 * 75600 * 5120 * 13824 / 8 / 918TFLOPs) = 2 * 1.45 = &nbsp;2.91 ms.<br>Attention projection: 6 * (2 * 1 * 75600 * 5120 * 5120 / 8 / 918TFLOPs) = 6 * 0.54 = &nbsp;3.24 ms.<br>The time spent in each block without self-attention is 21.322 ms.<br>(2.91 + 3.24) / 21.322 = 29% MFU for the remaining part.</span></p><p class="c2"><span class="c0">Take a look into the linear layers.</span></p><p class="c2"><span>1.45 / 2.0 = 73% MFU for FFN up projection.<br>1.45 / 2.465 = 59% MFU for FFN down projection.<br>0.54 &nbsp;/ 0.815 = 66% MFU for Q, K, V, and output projections.</span></p><p class="c2"><span class="c0">(1.45+1.45+0.54) / (2.0+2.465+0.815) = 65% which meet our observation in profiling. The linear operations can obtain about 66% MFU.</span></p><h3 class="c16" id="h.wyjzgh2ixec8"><span class="c12">One DiT Step</span></h3><p class="c2"><span class="c0">Ignoring parts other than transformer blocks, theoretically latency = 40 * transformer blocks tine ~= 40 * (self attention + FFN) = 40 * (15.974ms + 6.15ms) = 884.96ms</span></p><p class="c2"><span class="c0">Actual latency: 2604ms</span></p><p class="c2"><span class="c0">MFU = 884.96 / 2604 = 34%</span></p><p class="c2 c7"><span class="c0"></span></p><h3 class="c16" id="h.xqkc4l6jlbmu"><span class="c12">MFU Compare to Other Models</span></h3><table class="c26"><tr class="c15"><td class="c3" colspan="1" rowspan="1"><p class="c1"><span>Model</span><span class="c0">&nbsp;Name</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c1"><span class="c0">Accelerator</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">Sequence Length</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1"><span class="c0">MFU</span></p></td></tr><tr class="c15"><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c0">Llama3.1-8B</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c1"><span class="c0">v6e-8</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">8192</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1"><span class="c0">0.44</span></p></td></tr><tr class="c15"><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c0">Mistral-7B</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c1"><span class="c0">v6e-8</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">8192</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1"><span class="c0">0.47</span></p></td></tr><tr class="c15"><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c0">Llama3.1-70B</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c1"><span class="c0">v6e-256</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">8192</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1"><span class="c0">0.50</span></p></td></tr><tr class="c15"><td class="c3" colspan="1" rowspan="1"><p class="c1"><span class="c0">Mixtral-8x7B</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c1"><span class="c0">v6e-256</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c1"><span class="c0">4096</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1"><span class="c0">0.33</span></p></td></tr></table><p class="c2 c7"><span class="c0"></span></p><p class="c2"><span>The table provides MFUs of some models in training. These are the </span><span class="c6"><a href="https://www.google.com/url?q=https://github.com/AI-Hypercomputer/tpu-recipes/tree/main/training/trillium&amp;sa=D&amp;source=editors&amp;ust=1765684247721188&amp;usg=AOvVaw1c45ktOFYpZfvisvgL5bKa">training recipes</a></span><span class="c0">.</span></p><p class="c2"><span class="c0">The higher MFU is partially attributed to a larger micro batch size (MBS). The Llama3.1-70B training uses MBS=5.</span></p><p class="c2"><span>Also, the sequence length in Wan2.1 is 75600, which takes more time in self attention, and more </span><span>self attention tends to be lower MFU</span><span class="c0">&nbsp;compared to the linear operations.</span></p><p class="c2 c7"><span class="c0"></span></p><h3 class="c16" id="h.hlvo33idqwjw"><span class="c12">Splash Attention Kernel</span></h3><p class="c2"><span>Zoom into a block in the splash attention kernel. </span><span>By adding name scope in the splash attention kernel, it shows that the softmax takes about 1/3 times in each block calculation. </span></p><p class="c2"><span>The lower MFU in self attention is because of head dimension mismatch to the MXU and VPU bound in softmax. The head dimension is 128 in Wan2.1, QK MFU would be limited by 50% with 256x256 MXU and 128x128 tiling diagonal in v6e. </span><span>Small head dim also prevents QKV from getting the high MFU even if it is on non-contracting dimension.</span><span>&nbsp;Also, </span><span>the VPU used for elementwise calculation max, exp, sum in softmax takes a long while. The elementwise FLOPS in VPU could be 100 times slower than matrix multiplication in MXU according to the </span><span class="c6"><a href="https://www.google.com/url?q=https://jax-ml.github.io/scaling-book/tpus/&amp;sa=D&amp;source=editors&amp;ust=1765684247724179&amp;usg=AOvVaw0iuGHcF5pVV3zttAau2-AC">How to Think About TPUs</a></span><span>&nbsp;and </span><span class="c6"><a href="https://www.google.com/url?q=https://docs.jax.dev/en/latest/pallas/tpu/details.html%23elementwise-operations&amp;sa=D&amp;source=editors&amp;ust=1765684247724409&amp;usg=AOvVaw3az4RJ9CHi0YWw6j9R33YO">jnp.exp</a></span><span>&nbsp;would be even slower. </span></p><p class="c2"><span class="c0">Notice that adding named_scope prevents compiler optimization across named scope, which can possibly overlap softmax on VPU with matmul on MXU. The profiling result may take longer.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 301.33px;"><img alt="" src="images/image5.png" style="width: 624.00px; height: 301.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Given that MBS=1 and long sequence, the 34% overall MFU isn&rsquo;t particularly bad.</span></p><p class="c2 c7"><span class="c0"></span></p><h2 class="c4" id="h.q5e72iso1rcu"><span class="c14">Room for Optimization</span></h2><p class="c2"><span class="c0">Self attention consumes about 66% of the time and only reaches 37% MFU. It is the major bottleneck that can be further optimized.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 456.00px;"><img alt="" src="images/image11.png" style="width: 624.00px; height: 456.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2"><span>MFU other than self attention is about 29%, although the MFU of linear operations is about 66%. </span><span>The whole MFU still has room to optimize if the model can be further optimized </span><span class="c0">while there are still many HBM bound operations.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 326.67px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 326.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c7"><span class="c0"></span></p><h2 class="c4" id="h.msrognrfahv2"><span class="c14">Final Remarks</span></h2><p class="c2"><span class="c0">This analysis indicates that the Wan2.1-14B model achieves an overall MFU of 34% on TPU v6e-16 for one DiT step. While comparable to some existing models, there is significant room for optimization, particularly within the self-attention mechanism, which accounts for approximately 66% of the execution time. </span></p><p class="c2"><span class="c0">Optimizing the softmax calculation within the splash attention kernel and addressing the limitations imposed by the small head dimension (128) on MXU utilization are key areas for improvement. Further gains could also be achieved by co-optimizing the model and hardware to increase the MFU. </span></p><p class="c2"><span class="c0">The performance is further pushed to generate a video in 125 seconds now since the delivery of the 141 seconds recipe. It shows that there is still room for improvement.</span></p><p class="c2 c7"><span class="c0"></span></p><h2 class="c4" id="h.hgz11cxezqpj"><span class="c14">Appendix</span></h2><h3 class="c16" id="h.jz02n7fwl6fb"><span class="c12">Method</span></h3><h4 class="c8" id="h.f7n7yxq2sx25"><span class="c23">Sharding Strategy</span></h4><h5 class="c10" id="h.hi4no55yzu4v"><span class="c19">Text Encoder and VAE</span></h5><p class="c2"><span class="c0">Text encoder weights are sharded along all devices with tensor parallelism (TP).</span></p><p class="c2"><span class="c0">VAE weights are fully replicated along all devices.</span></p><p class="c2 c7"><span class="c0"></span></p><h5 class="c10" id="h.umn6yc2ayatn"><span class="c19">DiT</span></h5><p class="c2"><span class="c0">DiT weights are sharded along all devices with full sharded data parallelism (FSDP).</span></p><p class="c2"><span class="c0">Activations in DiT are sharded along 2 devices for data parallelism (DP) since there is a positive and a negative prompt which can be batched.</span></p><p class="c2"><span class="c0">Assuming there are only devices // 2 available and batch size = 1 for the remaining part for simplicity.</span></p><p class="c2"><span class="c0">Activations in DiT are sharded along remaining devices for sequence parallelism (SP) after patch embedding.</span></p><p class="c2"><span class="c0">QKV are sharded along head_number on remaining devices in self attention.</span></p><p class="c2"><span class="c0">Q is sharded along the sequence dimension and K, V are fully replicated in cross attention.</span></p><p class="c2 c7"><span class="c0"></span></p><h4 class="c8" id="h.b0swsezhkypv"><span class="c23">Flash Attention</span></h4><p class="c2"><span class="c0">Q, K, V tensor shapes are [1, 40, 75600, 128], which the whole QK matrix [1, 40, 75600, 75600] is too large to put into the TPU.</span></p><p class="c2"><span class="c0">Flash attention tile the QKV calculation, which decreases the peak memory usage and HBM accessing.</span></p><p class="c2"><span class="c0">Splash attention kernel in Jax library has been used to optimize the flash attention on the TPU.</span></p><p class="c2"><span class="c0">Block sizes in splash attentions are permanence sensitive, and need to be adjusted according to the input shape.</span></p><p class="c2 c7"><span class="c0"></span></p><h4 class="c8" id="h.7lht6jddnzvj"><span class="c23">Sequence Sharding</span></h4><p class="c2"><span class="c0">Sequence sharding shards the matrix along the sequence dimension, which can prevent collective operations in most layers except the attention layers.</span></p><p class="c2"><span class="c0">In the attention layers, QK needs to calculate the dot along the sequence dimension.</span></p><p class="c2"><span class="c0">Context parallelism (CP) shards along the head number dimension, all-to-all (A2A) operations are needed to re-shard activations from the sequence dimension to the head number dimension.</span></p><p class="c2"><span class="c0">Sequence parallelism (SP) shards Q along the sequence dimension and K and V fully replicated, only K and V need all-gather operations from sharding along the sequence dimension.</span></p><p class="c2"><span class="c0">Self attentions apply CP and cross attentions apply SP.</span></p><p class="c2 c7"><span class="c0"></span></p><h4 class="c8" id="h.a3fyarp5q2ix"><span class="c23">VAE from Maxdiffusion</span></h4><p class="c2"><span class="c0">The original pytorch implementation of VAE with torchax is replaced by the implementation in Maxdiffusion.</span></p><p class="c2"><span class="c0">VAE shards the activations along conv_in and conv_out.</span></p><p class="c2"><span class="c0">Encountering HBM OOM problems, there are some modifications added to the Maxdiffusion VAE to prevent useless tensors occupying the HBM.</span></p><p class="c2 c7"><span class="c0"></span></p><h3 class="c16" id="h.4uvd3rwyvdwl"><span class="c12">Profiling</span></h3><p class="c2"><span class="c0">The profile trace on text encoder and DiT is in the Xprof below. The trace does not contain VAE for simplicity.</span></p><p class="c2"><span class="c0">The profile contains text encoding and 3 steps of DiT.</span></p><p class="c2"><span class="c0">The time spent on the text encoder is about 50ms only, which is ignorable compared to the whole 141s.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 495.00px; height: 235.00px;"><img alt="" src="images/image14.png" style="width: 495.00px; height: 235.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Each DiT step cost 2.6s.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 142.67px;"><img alt="" src="images/image16.png" style="width: 624.00px; height: 142.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Each DiT attention block cost 64.46ms.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 616.00px; height: 308.00px;"><img alt="" src="images/image17.png" style="width: 616.00px; height: 308.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Each self attention cost 43.93ms.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 322.00px;"><img alt="" src="images/image1.png" style="width: 402.00px; height: 322.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Each attention block contains 4 A2A, each cost 1.125ms. Notice that the Q, K A2A are separated by the rope embedding to two blocks, which each cost 531us. The 4 A2A is caused by re-sharding between head_num dimension and sequence dimension.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 144.00px;"><img alt="" src="images/image19.png" style="width: 624.00px; height: 144.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Each cross attention cost 762us since the prompt embedding is only 512 length which is short.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 141.33px;"><img alt="" src="images/image12.png" style="width: 624.00px; height: 141.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Each up projection in FFN cost 2.0ms.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 113.33px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 113.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Each down projection in FFN cost 2.465ms.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 122.67px;"><img alt="" src="images/image13.png" style="width: 624.00px; height: 122.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Each Q, K, V, output project cost about 0.815ms.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 114.67px;"><img alt="" src="images/image20.png" style="width: 624.00px; height: 114.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">The self attention (custom-call) cost 66.8% time, other linear functions (convolution fusion) cost 14.3%, and A2A cost 6.7%. The MFU (FLOPS) in the pallas kernel splash attention cannot be calculated correctly, therefore the overall MFU seems very low. Although data formatting costs 6.45%, each copy and reshape are less than 1%, which cannot easily be eliminated.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 233.33px;"><img alt="" src="images/image18.png" style="width: 624.00px; height: 233.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 398.00px; height: 389.00px;"><img alt="" src="images/image7.png" style="width: 398.00px; height: 389.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Overall, the most time consuming part is self attentions which cost 66.8% of time, and the collective time is related short. The 66% FLOPS of convolution fusions shows that the overlapping of collective and computation are doing well which contains time of async all gather the weights.</span></p><p class="c2 c7"><span class="c0"></span></p><h3 class="c16" id="h.7inrjiqjs0yb"><span class="c12">Self Attention Roofline Calculation</span></h3><h4 class="c8" id="h.g3xzil1nqqpx"><span class="c23">Kernel Setup</span></h4><p class="c2"><span>Use </span><span class="c6"><a href="https://www.google.com/url?q=https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py&amp;sa=D&amp;source=editors&amp;ust=1765684247737652&amp;usg=AOvVaw1mEhZ5_GTtqi8xuGk6RpSd">splash attention kernel</a></span><span class="c0">&nbsp;in jax lib.</span></p><p class="c2"><span>The head number = 40 / 8 = 5 since DP and SP are used. For simplicity, this calculation only takes self attention in a chip into account. The K, V sequence lengths need padding to multiple of 256 since the constraint of the kernel.</span></p><h5 class="c10" id="h.jxzv15i01q8r"><span class="c19">Tensor shapes</span></h5><p class="c2"><span class="c0">Q: bf16[1,5,75600,128]</span></p><p class="c2"><span class="c0">K: bf16[1,5,75776,128]</span></p><p class="c2"><span class="c0">QK output: bf16[1,5,75600,75776]</span></p><p class="c2"><span class="c0">V: bf16[1,5,75776,128]</span></p><p class="c2"><span class="c0">Output: bf16[1,5,75600,128]</span></p><h5 class="c10" id="h.m17ep4gaknzx"><span class="c19">Block Spec</span></h5><p class="c2"><span class="c0">Batch_size = 1, num_q_heads = 40 (5 per shard), num_kv_heads = 40 (5 per shard), q_seq_len = 75600, kv_seq_len = 75776, head_dim = 128.</span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2"><span class="c0">Block_kv = 2048, block_kv_compute = 1024, block_q = 3024.</span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2"><span class="c0">Number of iterations over KV = kv_seq_len // block_kv = 37</span></p><p class="c2"><span class="c0">Number of iterations over Q = q_seq_len // block_q = 25</span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2"><span class="c0">Total number of iterations = 37 * 25 &nbsp;= 925.</span></p><h5 class="c10" id="h.gga46yvx609i"><span class="c19">Memory Spaces</span></h5><p class="c2"><span class="c0">Q and K are in HBM, V is in VMEM. The output is in HBM. According to the profiling.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 170.67px;"><img alt="" src="images/image15.png" style="width: 624.00px; height: 170.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Q</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 350.67px;"><img alt="" src="images/image6.png" style="width: 624.00px; height: 350.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">K</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 137.33px;"><img alt="" src="images/image8.png" style="width: 624.00px; height: 137.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">V: S(1) means it is in VMEM.</span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2 c7"><span class="c0"></span></p><h4 class="c8" id="h.47h8azxexazz"><span class="c23">Roofline Analysis</span></h4><h5 class="c10" id="h.9488tze8rsl9"><span class="c19">Methodology</span></h5><p class="c2"><span class="c0">To analyze the roofline for each Q, K, V block and multiply the block wise latency with the total number of blocks.</span></p><h5 class="c10" id="h.9yarvtceaxqw"><span class="c19">QK matmul</span></h5><p class="c2"><span class="c0">Block_q shape is bf16[1,block_q,num_q_heads,head_dim] = bf16[1,3024,5,128]</span></p><p class="c2"><span class="c0">Block_k shape is bf16[1,block_kv,num_kv_heads,head_dim] = bf16[1,2048,5,128].</span></p><p class="c2"><span class="c0">Output shape is bf16[1,block_q,block_kv,num_q_heads] = bf16[1,3024,2048,5]</span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2"><span class="c0">Compute_latency = flops / peak_flops = 2 * 1 * 3024 * 2048 * 5 * 128 / 918TFLOPS = 8.635us</span></p><p class="c2"><span class="c0">Memory_latency = (q_size + k_size) / HBM_bandwidth = (2 * 3024 * 5 * 128 + 2 * 2048 * 5 * 128) / 1638GB/s = 3.691us</span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2"><span class="c0">Roofline latency = max(compute_latency, memory_latency) = 8.635us</span></p><h5 class="c10" id="h.qe6d5sqg1hiq"><span class="c19">QKV matmul</span></h5><p class="c2"><span class="c0">Output of QK shape is bf16[1,block_q,block_kv,num_q_heads] = bf16[1,3024,2048,5]</span></p><p class="c2"><span class="c0">Block_v shape is bf16[1,block_kv,num_kv_heads,head_dim] = bf16[1,2048,5,128].</span></p><p class="c2"><span class="c0">Output shape is bf16[1,block_q,num_q_heads,head_dim] = bf16[1,3024,5,128].</span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2"><span class="c0">Compute_latency = flops / peak_flops = 2 * 1 * 3024 * 2048 * 5 * 128 / 918TFLOPS = 8.635us</span></p><p class="c2"><span class="c0">Memory_latency = output_size / HBM_bandwidth = (2 * 3024 * 5 * 128) / 1638GB/s = 2.2us</span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2"><span class="c0">Roofline latency = max(compute_latency, memory_latency) = 8.635us</span></p><h4 class="c8" id="h.e5kgcn5prsup"><span class="c23">Total Latency</span></h4><p class="c2"><span class="c0">The roofline of total latency = (8.635us + 8.635us) * 925 = 15.974ms. The measured latency of the splash kernel is 43.2ms. (~36.95%)</span></p><p class="c2 c7"><span class="c0"></span></p><p class="c2 c7"><span class="c0"></span></p><h2 class="c4" id="h.efujj26q2mfo"><span class="c14">Reference</span></h2><ol class="lst-kix_wmwho698ixcb-0 start" start="1"><li class="c2 c9 li-bullet-0"><span class="c6"><a href="https://www.google.com/url?q=https://github.com/shungcp/diffusers&amp;sa=D&amp;source=editors&amp;ust=1765684247744193&amp;usg=AOvVaw2i5Jv_5gFEOwvciOJ2U3PT">https://github.com/shungcp/diffusers</a></span></li><li class="c2 c9 li-bullet-0"><span class="c11"><a href="https://www.google.com/url?q=https://docs.google.com/document/d/1Yo3oJ8Ka-T8hw6O7AQm6W8yfV1fH9F8StnXmb4tJD1U/edit?tab%3Dt.0%23heading%3Dh.c7exogp3g0or&amp;sa=D&amp;source=editors&amp;ust=1765684247744485&amp;usg=AOvVaw1oPUmK6kU_f3HG1Px-mgxR">JAX Profiling Tutorial for diffusers Project</a></span></li><li class="c2 c9 li-bullet-0"><span class="c11"><a href="https://www.google.com/url?q=https://docs.google.com/presentation/d/17nh2Fdpl7obOEzq9Bz-IM7Lg--F_ZKuY1bQHUzzNWLI/edit?slide%3Did.g279d5d9a868_0_0%26resourcekey%3D0-i5M5yuYvkw9OeUcInIkCXw%23slide%3Did.g279d5d9a868_0_0&amp;sa=D&amp;source=editors&amp;ust=1765684247744730&amp;usg=AOvVaw19i7QyFxN91nC5JsFqILaR">Wan2.1 v6e Roofline Analysis</a></span></li><li class="c2 c9 li-bullet-0"><span class="c11"><a href="https://www.google.com/url?q=https://docs.google.com/presentation/d/195UtVO9ESoEDr19xT7n8KhygjM0KlTyw0yd3TdrJPDM/edit?resourcekey%3D0-nKzB6Wd9y9guAmCY9L-G7A%26slide%3Did.g375aa588c95_0_37%23slide%3Did.g375aa588c95_0_37&amp;sa=D&amp;source=editors&amp;ust=1765684247745002&amp;usg=AOvVaw3GwMlRTFqLA4sNtC6TmxaH">Copy of Wan2.1 v6e Roofline Analysis</a></span></li><li class="c2 c9 li-bullet-0"><span class="c11"><a href="https://www.google.com/url?q=https://docs.google.com/presentation/d/1ISeYkSe_rTCEc9j-Qf4Z7AjvrIBIqlACGqpgVj9yrmM/edit?slide%3Did.g34eb372a812_15_4%26resourcekey%3D0-Cv0KSvltYEuRAMwNyB9fRg%23slide%3Did.g34eb372a812_15_4&amp;sa=D&amp;source=editors&amp;ust=1765684247745272&amp;usg=AOvVaw2WCpCrzAicMYZnJCAVmhAY">[External] TPU handover guide 2025-04-17</a></span></li><li class="c2 c9 li-bullet-0"><span class="c11"><a href="https://www.google.com/url?q=https://docs.google.com/document/d/1cJw93j8gQ8r6pouA-gS4ynpbjNlkqggJ4Zs76G3Daqs/edit?resourcekey%3D0-rnX-srj_Hqsg_KsHqqYrqg%26tab%3Dt.0%23heading%3Dh.d7id8slf0vtu&amp;sa=D&amp;source=editors&amp;ust=1765684247745525&amp;usg=AOvVaw0GEPaaPQi6JlUMH43ZfAm0">Wan2.1 Optimization Report</a></span></li><li class="c2 c9 li-bullet-0"><span class="c11"><a href="https://www.google.com/url?q=https://docs.google.com/document/d/1lZ7YXOjWutTo2LvvTmZXpz1KIji6hkscS0MjzR5qjCM/edit?tab%3Dt.0%23heading%3Dh.xgjl2srtytjt&amp;sa=D&amp;source=editors&amp;ust=1765684247745774&amp;usg=AOvVaw3VzP68OLEOhBltagbZ5jBh">MaxText: Context Parallelism in TPU training</a></span></li><li class="c2 c9 li-bullet-0"><span class="c11"><a href="https://www.google.com/url?q=https://docs.google.com/document/d/1RWnSp7ic89aEU4mOrfmFG43iIzxoHvx72ou-qd_-iSU/edit?resourcekey%3D0-zksQ_5F-qe9VzONFrOE5yg%26tab%3Dt.0%23heading%3Dh.7m0pwjutesfj&amp;sa=D&amp;source=editors&amp;ust=1765684247746045&amp;usg=AOvVaw3c7iGhbw_DeDU4-eR0Phuf">Roofline of splash kernel for Wan2.1 </a></span></li><li class="c2 c9 li-bullet-0"><span class="c6"><a href="https://www.google.com/url?q=https://xprof.corp.google.com/overview_page/wdhongtw-3713754470591680052&amp;sa=D&amp;source=editors&amp;ust=1765684247746419&amp;usg=AOvVaw3IG5ct3UE4dE1Gm-85S99V">https://xprof.corp.google.com/overview_page/wdhongtw-3713754470591680052</a></span></li><li class="c2 c9 li-bullet-0"><span class="c6"><a href="https://www.google.com/url?q=https://jax-ml.github.io/scaling-book/sharding/&amp;sa=D&amp;source=editors&amp;ust=1765684247746753&amp;usg=AOvVaw1zMiJx0nh16M1NVLnjQM04">https://jax-ml.github.io/scaling-book/sharding/</a></span></li><li class="c2 c9 li-bullet-0"><span class="c11"><a href="https://www.google.com/url?q=https://b.corp.google.com/issues/439453386&amp;sa=D&amp;source=editors&amp;ust=1765684247746960&amp;usg=AOvVaw1XObE_AuV7qfj8RrGesWhV">Test Maxtext inference MFU</a></span></li><li class="c2 c9 li-bullet-0"><span class="c6"><a href="https://www.google.com/url?q=https://github.com/AI-Hypercomputer/tpu-recipes/tree/main/training/trillium&amp;sa=D&amp;source=editors&amp;ust=1765684247747398&amp;usg=AOvVaw29LvgTcLyvHtTqHdehWb9T">https://github.com/AI-Hypercomputer/tpu-recipes/tree/main/training/trillium</a></span><span class="c0">&nbsp;</span></li><li class="c2 c9 li-bullet-0"><span class="c11"><a href="https://www.google.com/url?q=https://docs.google.com/document/d/1-SQneQk-Ciy3c7o0ecaGnixVMwKHaMRBAIsFRZ4nnRs/edit?resourcekey%3D0-JSF2erehuijMCyapmPdbyQ%26tab%3Dt.0&amp;sa=D&amp;source=editors&amp;ust=1765684247747713&amp;usg=AOvVaw0hmfJU-t8QsSaNot5F89rV">[External] Wan2.1-14B Model Optimization on TPU v6e-16</a></span></li></ol><p class="c2 c7"><span class="c0"></span></p><div class="c24"><p class="c25"><a href="#cmnt_ref1" id="cmnt1">[a]</a><span class="c20">@kennyzheng@google.com Could you please review the document? Thanks.</span></p><p class="c25"><span class="c20">_Assigned to kennyzheng@google.com_</span></p></div></body></html>