# Splash Attention Kernel æ·±åº¦åˆ†ææ–‡æ¡£

## ç›®å½•
1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
3. [åºåˆ—åˆ†å—å¤„ç†æœºåˆ¶](#åºåˆ—åˆ†å—å¤„ç†æœºåˆ¶)
4. [Mask æœºåˆ¶è¯¦è§£](#mask-æœºåˆ¶è¯¦è§£)
5. [å‰å‘ä¼ æ’­å®ç°](#å‰å‘ä¼ æ’­å®ç°)
6. [åå‘ä¼ æ’­å®ç°](#åå‘ä¼ æ’­å®ç°)
7. [å†…å­˜ä¼˜åŒ–ç­–ç•¥](#å†…å­˜ä¼˜åŒ–ç­–ç•¥)
8. [API ä½¿ç”¨ç¤ºä¾‹](#api-ä½¿ç”¨ç¤ºä¾‹)

---

## æ¦‚è¿°

Splash Attentionï¼ˆSparse Flash Attentionï¼‰æ˜¯ JAX/Pallas ä¸º TPU ä¼˜åŒ–çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å®ç°ã€‚å®ƒç»“åˆäº† Flash Attention çš„å†…å­˜æ•ˆç‡å’Œç¨€ç–æ³¨æ„åŠ›çš„è®¡ç®—æ•ˆç‡ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†é•¿åºåˆ—ä»»åŠ¡ã€‚

### æ ¸å¿ƒç‰¹ç‚¹
- **ç¨€ç–æ€§æ”¯æŒ**ï¼šé€šè¿‡ block-level çš„ç¨€ç–æ€§è·³è¿‡ä¸å¿…è¦çš„è®¡ç®—
- **å†…å­˜æ•ˆç‡**ï¼šé‡‡ç”¨åˆ†å—è®¡ç®—ï¼Œé¿å…æåŒ–å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ
- **TPU ä¼˜åŒ–**ï¼šé’ˆå¯¹ TPU çš„ VMEMã€SMEM å±‚çº§è¿›è¡Œä¸“é—¨ä¼˜åŒ–
- **å¤šå¤´æ³¨æ„åŠ›æ”¯æŒ**ï¼šæ”¯æŒ MHAï¼ˆMulti-Head Attentionï¼‰ã€MQAï¼ˆMulti-Query Attentionï¼‰å’Œ GQAï¼ˆGrouped Query Attentionï¼‰

---

## æ•´ä½“æ¶æ„

```mermaid
flowchart TB
    subgraph Input["è¾“å…¥å±‚"]
        Q["Query [num_heads, q_seq_len, head_dim]"]
        K["Key [num_kv_heads, kv_seq_len, head_dim]"]
        V["Value [num_kv_heads, kv_seq_len, head_dim]"]
        Mask["Mask é…ç½®"]
    end

    subgraph MaskProcessing["Mask é¢„å¤„ç†"]
        MP["process_mask()"]
        MI["MaskInfo"]
        MP --> MI
        MI --> DN["data_next: ä¸‹ä¸€ä¸ªKVå—ç´¢å¼•"]
        MI --> MN["mask_next: ä¸‹ä¸€ä¸ªmaskå—ç´¢å¼•"]
        MI --> BM["block_mask: å—ç±»å‹æ ‡è®° (0/1/2)"]
        MI --> PMB["partial_mask_blocks: éƒ¨åˆ†maskå—"]
    end

    subgraph Kernel["Splash Attention Kernel"]
        FWD["Forward Pass<br/>flash_attention_kernel"]
        BWD_DQ["Backward dQ<br/>_flash_attention_dq_kernel"]
        BWD_DKV["Backward dKV<br/>_flash_attention_dkv_kernel"]
    end

    subgraph Output["è¾“å‡ºå±‚"]
        O["Output [num_heads, q_seq_len, head_dim]"]
        LSE["LogSumExp [num_heads, q_seq_len]"]
        DQ["dQ æ¢¯åº¦"]
        DK["dK æ¢¯åº¦"]
        DV["dV æ¢¯åº¦"]
    end

    Mask --> MP
    Q --> FWD
    K --> FWD
    V --> FWD
    MI --> FWD
    FWD --> O
    FWD --> LSE

    O --> BWD_DQ
    O --> BWD_DKV
    LSE --> BWD_DQ
    LSE --> BWD_DKV
    BWD_DQ --> DQ
    BWD_DKV --> DK
    BWD_DKV --> DV

    style Input fill:#e1f5fe
    style MaskProcessing fill:#fff3e0
    style Kernel fill:#f3e5f5
    style Output fill:#e8f5e9
```

---

## åºåˆ—åˆ†å—å¤„ç†æœºåˆ¶

Splash Attention ä½¿ç”¨åˆ†å—ï¼ˆTilingï¼‰ç­–ç•¥æ¥å¤„ç†é•¿åºåˆ—ï¼Œå…³é”®å‚æ•°å®šä¹‰åœ¨ [`BlockSizes`](splash_attention_kernel.py:494) ç±»ä¸­ï¼š

### BlockSizes é…ç½®

```python
@dataclasses.dataclass(frozen=True, slots=True)
class BlockSizes:
    # å‰å‘ä¼ æ’­å—å¤§å°
    block_q: int          # Q åºåˆ—å—å¤§å°
    block_kv: int         # KV åºåˆ—å—å¤§å°ï¼ˆå†…å­˜ï¼‰
    block_kv_compute: int # KV è®¡ç®—å—å¤§å°
    
    # dKV åå‘ä¼ æ’­å—å¤§å°
    block_q_dkv: int
    block_kv_dkv: int
    block_kv_dkv_compute: int
    
    # dQ åå‘ä¼ æ’­å—å¤§å°
    block_q_dq: int
    block_kv_dq: int
    
    # æ•°æ®å¸ƒå±€
    q_layout: QKVLayout   # HEAD_DIM_MINOR æˆ– SEQ_MINOR
    k_layout: QKVLayout
    v_layout: QKVLayout
```

### åˆ†å—è®¡ç®—æµç¨‹

```mermaid
flowchart LR
    subgraph Sequence["å®Œæ•´åºåˆ—"]
        direction TB
        S1["Q: [heads, q_seq, dim]"]
        S2["K: [heads, kv_seq, dim]"]
        S3["V: [heads, kv_seq, dim]"]
    end

    subgraph Blocking["åˆ†å—ç­–ç•¥"]
        direction TB
        B1["Q blocks: q_seq / block_q"]
        B2["KV blocks: kv_seq / block_kv"]
        B3["Grid: (heads, q_blocks, kv_blocks)"]
    end

    subgraph Compute["è®¡ç®—ç½‘æ ¼"]
        direction TB
        C1["program_id(0): head_idx"]
        C2["program_id(1): q_block_idx"]
        C3["program_id(2): kv_block_idx"]
    end

    S1 --> B1
    S2 --> B2
    S3 --> B2
    B1 --> C2
    B2 --> C3
    B3 --> C1
```

### å†…éƒ¨è®¡ç®—å¾ªç¯

åœ¨ [`flash_attention_kernel`](splash_attention_kernel.py:702) ä¸­ï¼ŒKV å—å¯ä»¥è¿›ä¸€æ­¥ç»†åˆ†ä¸ºæ›´å°çš„è®¡ç®—å—ï¼š

```mermaid
flowchart TB
    subgraph OuterLoop["å¤–å±‚å¾ªç¯ (Grid)"]
        H["éå† heads"]
        I["éå† Q blocks"]
        J["éå† KV blocks"]
    end

    subgraph InnerLoop["å†…å±‚å¾ªç¯ (block_kv_compute)"]
        K["åˆ’åˆ† KV block ä¸º compute blocks"]
        L["æ¯ä¸ª compute block æ‰§è¡Œ:<br/>1. QK^T çŸ©é˜µä¹˜<br/>2. Apply Mask<br/>3. Softmax<br/>4. SV çŸ©é˜µä¹˜"]
    end

    subgraph Accumulation["åœ¨çº¿ç´¯ç§¯"]
        M["m_scratch: æœ€å¤§å€¼"]
        N["l_scratch: exp æ±‚å’Œ"]
        O["o_scratch: è¾“å‡ºç´¯ç§¯"]
    end

    H --> I --> J --> K --> L
    L --> M
    L --> N
    L --> O

    style OuterLoop fill:#e3f2fd
    style InnerLoop fill:#fce4ec
    style Accumulation fill:#e8f5e9
```

---

## Mask æœºåˆ¶è¯¦è§£

Splash Attention æ”¯æŒå¤šç§ Mask ç±»å‹ï¼Œé€šè¿‡ [`splash_attention_mask.py`](splash_attention_mask.py) å®šä¹‰ï¼š

### Mask ç±»å‹å±‚æ¬¡ç»“æ„

```mermaid
classDiagram
    class Mask {
        <<abstract>>
        +shape: tuple
        +__getitem__(idx)
        +__or__(other)
        +__and__(other)
    }

    class _ComputableMask {
        +_shape: tuple
        +q_sequence: ndarray
        +mask_function: Callable
    }

    class CausalMask {
        +offset: int
        +causal_mask_function()
    }

    class LocalMask {
        +window_size: tuple
        +offset: int
        +local_mask_function()
    }

    class ChunkedCausalMask {
        +chunk_size: int
        +chunked_causal_mask_function()
    }

    class NumpyMask {
        +array: ndarray
    }

    class FullMask {
        +_shape: tuple
    }

    class MultiHeadMask {
        +masks: Sequence~Mask~
    }

    class LogicalOr {
        +left: Mask
        +right: Mask
    }

    class LogicalAnd {
        +left: Mask
        +right: Mask
    }

    Mask <|-- _ComputableMask
    Mask <|-- NumpyMask
    Mask <|-- FullMask
    Mask <|-- MultiHeadMask
    Mask <|-- LogicalOr
    Mask <|-- LogicalAnd

    _ComputableMask <|-- CausalMask
    _ComputableMask <|-- LocalMask
    _ComputableMask <|-- ChunkedCausalMask
```

### Mask é¢„å¤„ç†æµç¨‹

[`MaskInfo`](splash_attention_mask_info.py:33) ç»“æ„åŒ…å«è¿è¡Œæ—¶ mask ä¿¡æ¯ï¼š

```mermaid
flowchart TB
    subgraph Input["è¾“å…¥ Mask"]
        M1["MultiHeadMask"]
        M2["æˆ– jax.Array (åŠ¨æ€)"]
    end

    subgraph Processing["é¢„å¤„ç† process_mask()"]
        P1["åˆ†ææ¯ä¸ª block"]
        P2["åˆ¤æ–­ block ç±»å‹"]
        P3["æ”¶é›†éƒ¨åˆ† mask å—"]
        P4["æ„å»ºç´¢å¼•æ•°ç»„"]
    end

    subgraph BlockTypes["Block ç±»å‹åˆ†ç±»"]
        T0["block_mask = 0<br/>å…¨é›¶å— (è·³è¿‡)"]
        T1["block_mask = 1<br/>éƒ¨åˆ†å— (éœ€è¦mask)"]
        T2["block_mask = 2<br/>å…¨ä¸€å— (æ— éœ€mask)"]
    end

    subgraph MaskInfo["MaskInfo è¾“å‡º"]
        MI1["data_next: int[heads, q_blocks, kv_blocks]<br/>ä¸‹ä¸€ä¸ªéé›¶ KV å—ç´¢å¼•"]
        MI2["mask_next: int[heads, q_blocks, kv_blocks]<br/>ä¸‹ä¸€ä¸ª mask å—ç´¢å¼•"]
        MI3["block_mask: int[heads, q_blocks, kv_blocks]<br/>å—ç±»å‹æ ‡è®°"]
        MI4["partial_mask_blocks: bool[N, bq, bkv]<br/>éƒ¨åˆ† mask å—é›†åˆ"]
        MI5["q_sequence: int[q_seq_len]<br/>Q åºåˆ—ç´¢å¼• (ç”¨äºå¯è®¡ç®—mask)"]
    end

    M1 --> P1
    M2 --> P1
    P1 --> P2
    P2 --> T0
    P2 --> T1
    P2 --> T2
    T1 --> P3
    P3 --> P4
    P4 --> MI1
    P4 --> MI2
    P2 --> MI3
    P3 --> MI4
    P1 --> MI5

    style BlockTypes fill:#fff9c4
    style MaskInfo fill:#e8f5e9
```

### Block Mask å€¼çš„å«ä¹‰

| block_mask å€¼ | å«ä¹‰ | å¤„ç†æ–¹å¼ |
|--------------|------|---------|
| 0 | å…¨é›¶å— | å®Œå…¨è·³è¿‡ï¼Œä¸è®¡ç®— |
| 1 | éƒ¨åˆ†å— | ä» partial_mask_blocks åŠ è½½å®é™… mask |
| 2 | å…¨ä¸€å— | ä¸åº”ç”¨ maskï¼Œç›´æ¥è®¡ç®— |

---

## Mask ç±»å‹ä¸å†…å­˜å¼€é”€è¯¦ç»†åˆ†æ

Splash Attention çš„æ ¸å¿ƒä¼˜åŠ¿ä¹‹ä¸€æ˜¯**ç»å¤§å¤šæ•°æƒ…å†µä¸‹ä¸éœ€è¦å­˜å‚¨å®Œæ•´çš„ [B, H, S, L] å¤§å°çš„ mask çŸ©é˜µ**ã€‚

### ğŸ”‘ å…³é”®é—®é¢˜å›ç­”

**Q: Splash Attention æœ‰å‡ ç§ Mask æœºåˆ¶ï¼Ÿ**

å…±æœ‰ **6 ç§**ä¸»è¦çš„ Mask ç±»å‹ï¼Œå¯åˆ†ä¸ºä¸¤å¤§ç±»ï¼š

```mermaid
flowchart TB
    subgraph ComputableMasks["å¯è®¡ç®— Mask (ä¸éœ€è¦å­˜å‚¨å®Œæ•´çŸ©é˜µ)"]
        CM1["CausalMask<br/>å› æœæ³¨æ„åŠ›"]
        CM2["LocalMask<br/>å±€éƒ¨çª—å£æ³¨æ„åŠ›"]
        CM3["ChunkedCausalMask<br/>åˆ†å—å› æœæ³¨æ„åŠ›"]
        CM4["FullMask<br/>å…¨æ³¨æ„åŠ›"]
    end

    subgraph StoredMasks["å­˜å‚¨å‹ Mask (éœ€è¦å­˜å‚¨éƒ¨åˆ†/å…¨éƒ¨çŸ©é˜µ)"]
        SM1["NumpyMask<br/>è‡ªå®šä¹‰numpyæ©ç "]
        SM2["åŠ¨æ€ jax.Array Mask<br/>è¿è¡Œæ—¶åŠ¨æ€æ©ç "]
    end

    style ComputableMasks fill:#c8e6c9
    style StoredMasks fill:#ffcdd2
```

**Q: æ˜¯å¦éœ€è¦ [B, H, S, L] è¿™ä¹ˆå¤§çš„çŸ©é˜µï¼Ÿ**

| Mask ç±»å‹ | æ˜¯å¦éœ€è¦ O(seqÂ²) å­˜å‚¨ | å®é™…å†…å­˜éœ€æ±‚ |
|-----------|---------------------|-------------|
| `CausalMask` | âŒ **å¦** | O(seq_len) åªå­˜ç´¢å¼• |
| `LocalMask` | âŒ **å¦** | O(seq_len) åªå­˜ç´¢å¼• |
| `ChunkedCausalMask` | âŒ **å¦** | O(seq_len) åªå­˜ç´¢å¼• |
| `FullMask` | âŒ **å¦** | O(1) åªå­˜ shape |
| `NumpyMask` | âš ï¸ **éƒ¨åˆ†** | O(unique_blocks Ã— blockÂ²) |
| åŠ¨æ€ `jax.Array` | âš ï¸ **æ˜¯** | O(H Ã— seqÂ²) éœ€å®Œæ•´å­˜å‚¨ |

### Mask å†…å­˜éœ€æ±‚è¯¦è§£

#### 1. å¯è®¡ç®— Maskï¼ˆé›¶é¢å¤–å­˜å‚¨ï¼‰ âœ…

`CausalMask`ã€`LocalMask`ã€`ChunkedCausalMask` ç»§æ‰¿è‡ª `_ComputableMask`ï¼š

```mermaid
flowchart LR
    subgraph Storage["å­˜å‚¨éœ€æ±‚"]
        S1["q_sequence: int32[q_seq_len]<br/>ä¾‹: 8192 Ã— 4 bytes = 32 KB"]
        S2["mask_function: å‡½æ•°æŒ‡é’ˆ â‰ˆ 0"]
    end

    subgraph Runtime["è¿è¡Œæ—¶æŒ‰éœ€è®¡ç®—"]
        R1["kernel å†…éƒ¨å®æ—¶è®¡ç®—"]
        R2["åªè®¡ç®—å½“å‰ block çš„ mask"]
        R3["æ— éœ€é¢„å­˜å‚¨å®Œæ•´çŸ©é˜µ"]
    end

    Storage --> Runtime

    style Storage fill:#c8e6c9
    style Runtime fill:#e3f2fd
```

**CausalMask å†…å­˜å…¬å¼**ï¼š
```
å†…å­˜ = q_seq_len Ã— sizeof(int32) = seq_len Ã— 4 bytes
```

**å…·ä½“ç¤ºä¾‹ (8192 tokens)**ï¼š
| æ–¹æ¡ˆ | å†…å­˜è®¡ç®— | å†…å­˜å¤§å° |
|-----|---------|---------|
| ä¼ ç»Ÿå®Œæ•´çŸ©é˜µ | 8192 Ã— 8192 Ã— 1 byte | **64 MB** (å•head) |
| Splash CausalMask | 8192 Ã— 4 bytes | **32 KB** (æ‰€æœ‰headså…±äº«) |
| èŠ‚çœæ¯”ä¾‹ | - | **99.95%** |

#### 2. NumpyMaskï¼ˆéƒ¨åˆ†å—å­˜å‚¨ï¼‰ âš ï¸

å¯¹äºè‡ªå®šä¹‰çš„ numpy maskï¼Œåªå­˜å‚¨"éƒ¨åˆ†å—"ï¼ˆæ—¢éå…¨é›¶ä¹Ÿéå…¨ä¸€ï¼‰ï¼š

```mermaid
flowchart TB
    subgraph Analysis["åˆ†æå®Œæ•´ Mask"]
        A1["éå†æ¯ä¸ª block"]
        A2["åˆ¤æ–­ block ç±»å‹"]
    end

    subgraph Classification["å—åˆ†ç±»"]
        C1["å…¨é›¶å— block_mask=0<br/>âŒ ä¸å­˜å‚¨"]
        C2["éƒ¨åˆ†å— block_mask=1<br/>âœ… å­˜å‚¨åˆ° partial_mask_blocks"]
        C3["å…¨ä¸€å— block_mask=2<br/>âŒ ä¸å­˜å‚¨"]
    end

    subgraph Dedup["å»é‡å­˜å‚¨"]
        D1["ç›¸åŒçš„éƒ¨åˆ†å—åªå­˜ä¸€ä»½"]
        D2["partial_mask_blocks[N, bq, bkv]"]
    end

    A1 --> A2
    A2 --> C1
    A2 --> C2
    A2 --> C3
    C2 --> D1 --> D2

    style C1 fill:#ffcdd2
    style C2 fill:#fff9c4
    style C3 fill:#c8e6c9
```

**NumpyMask å†…å­˜å…¬å¼**ï¼š
```
partial_mask_blocks = num_unique_partial_blocks Ã— block_q Ã— block_kv Ã— 1 byte
MaskInfo metadata  = heads Ã— q_blocks Ã— kv_blocks Ã— 3 bytes (int8)
```

**Causal Mask ä½œä¸º NumpyMask çš„ç¤ºä¾‹ (8192 tokens, block=128)**ï¼š
```
q_blocks = kv_blocks = 64
å¯¹è§’çº¿ä¸Šæœ‰ 64 ä¸ªéƒ¨åˆ†å—
ä½†ç”±äºå»é‡ï¼Œå®é™…åªéœ€ ~2 ä¸ªå”¯ä¸€æ¨¡å¼
partial_mask = 2 Ã— 128 Ã— 128 Ã— 1 = 32 KB
metadata = 1 Ã— 64 Ã— 64 Ã— 3 = 12 KB
æ€»è®¡ â‰ˆ 44 KB (vs 64 MB)
```

#### 3. åŠ¨æ€ jax.Array Maskï¼ˆæœ€å¤§å­˜å‚¨ï¼‰ âŒ

**è¿™æ˜¯å”¯ä¸€éœ€è¦ O(H Ã— seqÂ²) å†…å­˜çš„æƒ…å†µï¼**

```mermaid
flowchart TB
    subgraph Input["è¾“å…¥"]
        I1["åŠ¨æ€ mask: jax.Array[H, S, L]"]
    end

    subgraph Reshape["é‡å¡‘ä¸ºå—å½¢å¼"]
        R1["[H, q_blocks, kv_blocks, bq, bkv]"]
    end

    subgraph Storage["å­˜å‚¨"]
        S1["partial_mask_blocks å­˜å‚¨å®Œæ•´åˆ†å—çŸ©é˜µ"]
        S2["å†…å­˜ = H Ã— S Ã— L bytes"]
    end

    I1 --> R1 --> S1 --> S2

    style Input fill:#ffcdd2
    style Storage fill:#ffcdd2
```

**åŠ¨æ€ Mask å†…å­˜**ï¼š
```
å†…å­˜ = heads Ã— q_seq Ã— kv_seq Ã— sizeof(bool)
     = heads Ã— seqÂ² bytes  (ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸åŒ)
```

### 4. Segment IDsï¼ˆç‹¬ç«‹æœºåˆ¶ï¼‰

Segment IDs ç”¨äº packed sequencesï¼Œæ˜¯ç‹¬ç«‹çš„æœºåˆ¶ï¼š

```
segment_ids.q  = int32[q_seq_len]   â†’ q_seq Ã— 4 bytes
segment_ids.kv = int32[kv_seq_len]  â†’ kv_seq Ã— 4 bytes
æ€»è®¡ = (q_seq + kv_seq) Ã— 4 bytes â‰ˆ O(seq_len)
```

### å†…å­˜å¯¹æ¯”æ€»ç»“å›¾

ä»¥ **16 heads, 8192 tokens, block_size=128** ä¸ºä¾‹ï¼š

```mermaid
graph TB
    subgraph Traditional["ä¼ ç»Ÿ Attention"]
        T1["å®Œæ•´ Mask çŸ©é˜µ"]
        T2["16 Ã— 8192 Ã— 8192 Ã— 1 byte"]
        T3["= 1 GB"]
        T1 --> T2 --> T3
    end

    subgraph Splash["Splash Attention"]
        S1["CausalMask"]
        S2["8192 Ã— 4 + metadata"]
        S3["â‰ˆ 224 KB"]
        S4["èŠ‚çœ 99.98%"]
        
        N1["NumpyMask (ç¨€ç–)"]
        N2["unique_blocks Ã— 128Â²"]
        N3["â‰ˆ 256 KB"]
        N4["èŠ‚çœ 99.97%"]
        
        D1["åŠ¨æ€ jax.Array"]
        D2["16 Ã— 8192 Ã— 8192"]
        D3["â‰ˆ 1.2 GB"]
        D4["æ— èŠ‚çœ"]
        
        S1 --> S2 --> S3 --> S4
        N1 --> N2 --> N3 --> N4
        D1 --> D2 --> D3 --> D4
    end

    style T3 fill:#ffcdd2
    style S3 fill:#c8e6c9
    style N3 fill:#fff9c4
    style D3 fill:#ffcdd2
```

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®

| åœºæ™¯ | æ¨è Mask ç±»å‹ | å†…å­˜å¼€é”€ |
|-----|---------------|---------|
| æ ‡å‡† Decoder | `CausalMask` | O(seq) âœ… |
| å±€éƒ¨æ³¨æ„åŠ› | `LocalMask` | O(seq) âœ… |
| Llama4 é£æ ¼ | `ChunkedCausalMask` | O(seq) âœ… |
| å…¨æ³¨æ„åŠ› Encoder | `FullMask` | O(1) âœ… |
| **æœ‰ Padding çš„å˜é•¿åºåˆ—** | **Segment IDs** | **O(seq) âœ…** |
| å¤æ‚è‡ªå®šä¹‰é™æ€ | `NumpyMask` | O(blocks) âš ï¸ |
| è¿è¡Œæ—¶åŠ¨æ€ | é¿å…ä½¿ç”¨ | O(HÃ—seqÂ²) âŒ |

---

## ğŸ’¡ é‡è¦åœºæ™¯ï¼šPadding åºåˆ—çš„å¤„ç†ï¼ˆ36k é•¿åºåˆ—ç¤ºä¾‹ï¼‰

### åœºæ™¯æè¿°

å¯¹äº**å˜é•¿åºåˆ— padding åˆ°å›ºå®šé•¿åº¦**ï¼ˆå¦‚ 36kï¼‰çš„æƒ…å†µï¼Œ**æ¨èä½¿ç”¨ Segment IDs è€Œéå®Œæ•´ Mask çŸ©é˜µ**ã€‚

```mermaid
flowchart TB
    subgraph Input["è¾“å…¥åºåˆ— (paddingåˆ°36k)"]
        I1["å®é™… tokens: 0 ~ actual_len-1"]
        I2["padding tokens: actual_len ~ 36k-1"]
    end

    subgraph SegmentIds["Segment IDs æ–¹æ¡ˆ âœ… æ¨è"]
        S1["segment_ids.q = [0,0,...,0, -1,-1,...,-1]"]
        S2["segment_ids.kv = [0,0,...,0, -1,-1,...,-1]"]
        S3["åªæœ‰ segment_id=0 çš„ token äº’ç›¸å¯è§"]
        S4["å†…å­˜: 2 Ã— 36k Ã— 4 bytes = 288 KB"]
    end

    subgraph FullMask["å®Œæ•´ Mask çŸ©é˜µæ–¹æ¡ˆ âŒ ä¸æ¨è"]
        F1["mask = jax.Array[H, 36k, 36k]"]
        F2["å†…å­˜: 16 Ã— 36k Ã— 36k = 20.7 GB"]
    end

    I1 --> S1
    I2 --> S2
    S1 --> S3 --> S4

    style SegmentIds fill:#c8e6c9
    style FullMask fill:#ffcdd2
    style S4 fill:#c8e6c9
    style F2 fill:#ffcdd2
```

### å†…å­˜å¯¹æ¯” (36k åºåˆ—, 16 heads)

| æ–¹æ¡ˆ | å†…å­˜è®¡ç®— | å†…å­˜å¤§å° | æ˜¯å¦æ¨è |
|-----|---------|---------|---------|
| å®Œæ•´ Mask çŸ©é˜µ | 16 Ã— 36k Ã— 36k Ã— 1 byte | **20.7 GB** | âŒ ä¸å¯è¡Œ |
| åŠ¨æ€ jax.Array Mask | 16 Ã— 36k Ã— 36k Ã— 1 byte | **20.7 GB** | âŒ ä¸å¯è¡Œ |
| **Segment IDs** | 2 Ã— 36k Ã— 4 bytes | **288 KB** | âœ… **æ¨è** |
| Segment IDs + CausalMask | 3 Ã— 36k Ã— 4 bytes | **432 KB** | âœ… **æ¨è** |

### ä»£ç ç¤ºä¾‹

```python
import jax.numpy as jnp
from jax.experimental.pallas.ops.tpu.splash_attention import (
    splash_attention_kernel as splash,
    splash_attention_mask as mask_lib,
)

# å‡è®¾ï¼šå®é™…åºåˆ—é•¿åº¦ actual_lenï¼Œpadding åˆ° 36k
actual_len = 20000
padded_len = 36 * 1024  # 36k

# ===============================================
# æ–¹æ¡ˆ1ï¼šå…¨æ³¨æ„åŠ› + Padding Maskï¼ˆEncoder åœºæ™¯ï¼‰
# ===============================================
# åˆ›å»º segment_idsï¼šå®é™… token ä¸º 0ï¼Œpadding ä¸º -1
segment_ids = splash.SegmentIds(
    q=jnp.where(jnp.arange(padded_len) < actual_len, 0, -1),
    kv=jnp.where(jnp.arange(padded_len) < actual_len, 0, -1),
)

# ä½¿ç”¨ FullMaskï¼ˆåªå­˜å‚¨ shapeï¼Œä¸å­˜å‚¨ä»»ä½• mask æ•°æ®ï¼‰
mask = mask_lib.FullMask(shape=(padded_len, padded_len))
multi_head_mask = mask_lib.MultiHeadMask([mask] * num_heads)

kernel = splash.make_splash_mha_single_device(mask=multi_head_mask, ...)
output = kernel(q, k, v, segment_ids=segment_ids)

# ===============================================
# æ–¹æ¡ˆ2ï¼šCausal + Padding Maskï¼ˆDecoder åœºæ™¯ï¼‰
# ===============================================
# CausalMask ç¡®ä¿åªçœ‹å‰é¢çš„ tokenï¼ˆä¸å­˜å‚¨å®Œæ•´çŸ©é˜µï¼‰
# Segment IDs ç¡®ä¿ä¸ attend åˆ° padding
causal_mask = mask_lib.CausalMask(shape=(padded_len, padded_len))
multi_head_mask = mask_lib.MultiHeadMask([causal_mask] * num_heads)

kernel = splash.make_splash_mha_single_device(mask=multi_head_mask, ...)
output = kernel(q, k, v, segment_ids=segment_ids)
```

### Segment IDs å·¥ä½œåŸç†

```mermaid
flowchart LR
    subgraph Logic["Segment IDs é€»è¾‘"]
        direction TB
        L1["å¯¹æ¯å¯¹ (q_pos, kv_pos)"]
        L2["æ£€æŸ¥ segment_ids.q[q_pos] == segment_ids.kv[kv_pos]"]
        L3{"ç›¸ç­‰?"}
        L4["å…è®¸ attend"]
        L5["mask æ‰ (è®¾ä¸º -inf)"]
        
        L1 --> L2 --> L3
        L3 -->|Yes| L4
        L3 -->|No| L5
    end

    subgraph Example["ç¤ºä¾‹: actual_len=5, padded_len=8"]
        direction TB
        E1["segment_ids = [0,0,0,0,0,-1,-1,-1]"]
        E2["ä½ç½® 0-4 (segment=0) äº’ç›¸å¯è§"]
        E3["ä½ç½® 5-7 (segment=-1) è¢«å®Œå…¨å±è”½"]
    end

    style L4 fill:#c8e6c9
    style L5 fill:#ffcdd2
```

### ä¸å…¶ä»– Mask ç»„åˆ

Segment IDs ä¼šä¸å…¶ä»– Mask **åš AND ç»„åˆ**ï¼š

```python
# æœ€ç»ˆ mask = CausalMask AND SegmentIdsMask
#
# ä¾‹å¦‚ä½ç½® (3, 5):
#   - CausalMask: 3 >= 5? â†’ False (ä¸å¯è§)
#   - SegmentIds: segment[3]=0, segment[5]=-1 â†’ False (ä¸å¯è§)
#   - æœ€ç»ˆ: False
#
# ä¾‹å¦‚ä½ç½® (4, 2):
#   - CausalMask: 4 >= 2? â†’ True (å¯è§)
#   - SegmentIds: segment[4]=0, segment[2]=0 â†’ True (å¯è§)
#   - æœ€ç»ˆ: True
```

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **Segment IDs å¿…é¡»ç¡®ä¿æ¯è¡Œè‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆ token**
   - å¦åˆ™ softmax åˆ†æ¯ä¸º 0ï¼Œå¯¼è‡´ NaN
   - çº¯ padding è¡Œéœ€ç‰¹æ®Šå¤„ç†æˆ–ç¡®ä¿ä¸ä¼šè¢«æŸ¥è¯¢

2. **Segment IDs å€¼çš„é€‰æ‹©**
   - å®é™… token ä½¿ç”¨ **ç›¸åŒçš„éè´Ÿæ•´æ•°**ï¼ˆå¦‚ 0ï¼‰
   - padding ä½¿ç”¨ **ä¸åŒçš„å€¼**ï¼ˆå¦‚ -1ï¼‰
   - ä¸åŒçš„ç‹¬ç«‹åºåˆ—ï¼ˆbatch packingï¼‰ä½¿ç”¨ä¸åŒçš„æ•´æ•°

3. **æ‰¹å¤„ç†å¤šä¸ªåºåˆ—ï¼ˆPackingï¼‰**
   ```python
   # ä¾‹å¦‚ 3 ä¸ªåºåˆ— pack åˆ°ä¸€èµ·ï¼š
   # seq1: tokens 0-99, seq2: tokens 100-199, seq3: tokens 200-249, padding: 250-255
   segment_ids = splash.SegmentIds(
       q=jnp.array([0]*100 + [1]*100 + [2]*50 + [-1]*6),
       kv=jnp.array([0]*100 + [1]*100 + [2]*50 + [-1]*6),
   )
   # è¿™æ · seq1, seq2, seq3 äº’ç›¸ä¸å¯è§
   ```

---

## ğŸ¤” API è®¾è®¡è®¨è®ºï¼šä¸ºä»€ä¹ˆ Padding å¤„ç†ä¸å¤Ÿç®€æ´ï¼Ÿ

### é—®é¢˜ï¼šPadding æ˜¯æœ€å¸¸è§çš„åœºæ™¯ï¼Œä½† API ä¸å¤Ÿç›´è§‚

```mermaid
flowchart LR
    subgraph PyTorch["PyTorch / HuggingFace âœ… ç›´è§‚"]
        P1["padding_mask = [1,1,1,1,0,0,0,0]"]
        P2["output = model(x, attention_mask=padding_mask)"]
        P1 --> P2
    end

    subgraph Splash["Splash Attention âŒ ç¹ç"]
        S1["mask = FullMask(shape=(36k, 36k))"]
        S2["segment_ids = SegmentIds(...)"]
        S3["output = kernel(q, k, v, segment_ids)"]
        S1 --> S2 --> S3
    end

    style PyTorch fill:#c8e6c9
    style Splash fill:#fff9c4
```

### ä¸ºä»€ä¹ˆå¿…é¡»å†™ FullMask / CausalMaskï¼Ÿ

| é—®é¢˜ | è§£é‡Š |
|------|------|
| **API è®¾è®¡è¦æ±‚** | `make_splash_mha()` éœ€è¦ mask å‚æ•°æ¥ç”Ÿæˆ MaskInfo |
| **è¯­ä¹‰åŒºåˆ†** | FullMask = åŒå‘æ³¨æ„åŠ›ï¼ˆEncoderï¼‰ï¼ŒCausalMask = å•å‘æ³¨æ„åŠ›ï¼ˆDecoderï¼‰ |
| **è®¾è®¡åˆè¡·ä¸åŒ** | Splash Attention ä¸»è¦ä¸º**ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼**è®¾è®¡ï¼Œä¸æ˜¯ä¸º padding |

### Segment IDs çš„æœ¬æ„

Segment IDs åŸæœ¬æ˜¯ä¸º **sequence packing**ï¼ˆå¤šåºåˆ—æ‹¼æ¥ï¼‰è®¾è®¡çš„ï¼Œä¸æ˜¯ä¸“é—¨ä¸º paddingï¼š

```python
# Packing åœºæ™¯ï¼ˆåŸå§‹è®¾è®¡ç›®æ ‡ï¼‰
# å¤šä¸ªçŸ­åºåˆ—æ‹¼æ¥æˆä¸€ä¸ªé•¿åºåˆ—ï¼Œé¿å… padding æµªè´¹
segment_ids = [0,0,0, 1,1,1,1, 2,2]  # 3ä¸ªåºåˆ—
# seq1(3 tokens) + seq2(4 tokens) + seq3(2 tokens)

# Padding åœºæ™¯ï¼ˆå‰¯äº§å“ç”¨æ³•ï¼‰
segment_ids = [0,0,0,0,0, -1,-1,-1]  # 1ä¸ªåºåˆ— + padding
```

### ç†æƒ³çš„ APIï¼ˆå¦‚æœé‡æ–°è®¾è®¡ï¼‰

```python
# ç†æƒ³æƒ…å†µ - ç›´æ¥ä¼  1D padding mask
padding_mask = jnp.array([1,1,1,1,1, 0,0,0])  # 1=valid, 0=padding
output = kernel(q, k, v, padding_mask=padding_mask)

# æˆ–æ›´ç®€å•
output = kernel(q, k, v, valid_length=5)

# æˆ–è‡ªåŠ¨æ¨æ–­
output = kernel(q, k, v)  # è‡ªåŠ¨ä» q çš„å½¢çŠ¶æ¨æ–­ mask shape
```

### å®é™…å»ºè®®ï¼šå°è£…ä¸€ä¸ªä¾¿æ·å‡½æ•°

æ—¢ç„¶ API å·²ç»æ˜¯è¿™æ ·äº†ï¼Œå¯ä»¥è‡ªå·±å°è£…ç®€åŒ–ä½¿ç”¨ï¼š

```python
def make_padded_attention_kernel(
    padded_len: int,
    actual_len: int,
    num_heads: int,
    causal: bool = False,
    **kwargs
):
    """ä¾¿æ·çš„ padding-aware attention kernel å·¥å‚å‡½æ•°
    
    Args:
        padded_len: padding åçš„åºåˆ—é•¿åº¦
        actual_len: å®é™…æœ‰æ•ˆåºåˆ—é•¿åº¦
        num_heads: æ³¨æ„åŠ›å¤´æ•°
        causal: æ˜¯å¦ä½¿ç”¨å› æœæ³¨æ„åŠ›
        **kwargs: ä¼ é€’ç»™ make_splash_mha_single_device çš„å…¶ä»–å‚æ•°
    
    Returns:
        ä¸€ä¸ªç®€åŒ–çš„ attention å‡½æ•°ï¼Œåªéœ€ä¼ å…¥ q, k, v
    """
    from jax.experimental.pallas.ops.tpu.splash_attention import (
        splash_attention_kernel as splash,
        splash_attention_mask as mask_lib,
    )
    
    # é€‰æ‹© mask ç±»å‹
    if causal:
        mask = mask_lib.CausalMask(shape=(padded_len, padded_len))
    else:
        mask = mask_lib.FullMask(shape=(padded_len, padded_len))
    
    multi_head_mask = mask_lib.MultiHeadMask([mask] * num_heads)
    kernel = splash.make_splash_mha_single_device(mask=multi_head_mask, **kwargs)
    
    # åˆ›å»º segment_ids
    segment_ids = splash.SegmentIds(
        q=jnp.where(jnp.arange(padded_len) < actual_len, 0, -1),
        kv=jnp.where(jnp.arange(padded_len) < actual_len, 0, -1),
    )
    
    # è¿”å›ä¸€ä¸ªç®€åŒ–çš„è°ƒç”¨æ¥å£
    def call(q, k, v):
        return kernel(q, k, v, segment_ids=segment_ids)
    
    return call


# ä½¿ç”¨ç¤ºä¾‹ - ç®€æ´å¤šäº†ï¼
attention = make_padded_attention_kernel(
    padded_len=36*1024,
    actual_len=20000,
    num_heads=16,
    causal=True
)
output = attention(q, k, v)
```

### æ€»ç»“

| è§‚ç‚¹ | è¯´æ˜ |
|------|------|
| **ç”¨æˆ·ä½“éªŒ** | Padding æ˜¯æœ€å¸¸è§åœºæ™¯ï¼Œä½† API ç¡®å®ä¸å¤Ÿç›´è§‚ |
| **è®¾è®¡æƒè¡¡** | Splash Attention ä¼˜å…ˆè€ƒè™‘ç¨€ç–æ³¨æ„åŠ›çµæ´»æ€§ï¼Œç‰ºç‰²äº†æ˜“ç”¨æ€§ |
| **å®é™…è§£å†³** | è‡ªè¡Œå°è£…ä¾¿æ·å‡½æ•°ï¼Œæˆ–å‘ JAX å›¢é˜Ÿæ feature request |
| **æ­£é¢çœ‹æ³•** | ä¸€æ—¦ç†è§£ FullMask + Segment IDs çš„ç»„åˆï¼Œä½¿ç”¨ä¹Ÿä¸ç®—å¤ªå¤æ‚ |

---

### ç¨€ç–æ€§ä¼˜åŒ–ï¼šGrid Shrinking

```mermaid
flowchart LR
    subgraph Before["åŸå§‹ Grid"]
        direction TB
        B1["è®¸å¤šå…¨é›¶å—<br/>block_mask=0"]
        B2["ç¨€ç–çš„éé›¶å—<br/>block_mask>0"]
    end

    subgraph Shrink["Grid Shrinking"]
        S1["_shrink_mask_info()"]
        S2["å‹ç¼© KV ç»´åº¦"]
    end

    subgraph After["å‹ç¼©å Grid"]
        direction TB
        A1["åªä¿ç•™éé›¶å—"]
        A2["data_next æŒ‡å‘å®é™…æ•°æ®"]
    end

    B1 --> S1
    B2 --> S1
    S1 --> S2
    S2 --> A1
    S2 --> A2

    style Before fill:#ffcdd2
    style Shrink fill:#fff9c4
    style After fill:#c8e6c9
```

---

## å‰å‘ä¼ æ’­å®ç°

### æ ¸å¿ƒç®—æ³•æµç¨‹

[`flash_attention_kernel`](splash_attention_kernel.py:702) å®ç°äº†åœ¨çº¿ softmax ç®—æ³•ï¼š

```mermaid
flowchart TB
    subgraph Init["åˆå§‹åŒ– (j==0)"]
        I1["o_scratch = 0"]
        I2["m_scratch = mask_value æˆ– sinks"]
        I3["l_scratch = 0 æˆ– 1 (æœ‰sinksæ—¶)"]
    end

    subgraph Check["æ£€æŸ¥å½“å‰å—"]
        C1["_next_nonzero()"]
        C2{"should_run?"}
    end

    subgraph Compute["è®¡ç®—å¾ªç¯ body()"]
        direction TB
        L1["åŠ è½½ Q block"]
        L2["åŠ è½½ K block (slice_k)"]
        L3["QK = Q @ K^T"]
        L4["åº”ç”¨ Mask"]
        L5["è®¡ç®— m_curr = max(QK)"]
        L6["m_next = max(m_prev, m_curr)"]
        L7["s_curr = exp(QK - m_next)"]
        L8["l_curr = sum(s_curr)"]
        L9["alpha = exp(m_prev - m_next)"]
        L10["l_next = l_curr + alpha * l_prev"]
        L11["åŠ è½½ V block"]
        L12["o_curr = s_curr @ V"]
        L13["o_scratch = alpha * o_scratch + o_curr"]
    end

    subgraph Final["æœ€ç»ˆè¾“å‡º (j==grid_width-1)"]
        F1["o = o_scratch / l_scratch"]
        F2["logsumexp = log(l) + m"]
    end

    Init --> Check
    Check --> C2
    C2 -->|Yes| Compute
    C2 -->|No| Final
    Compute --> Final

    style Init fill:#e3f2fd
    style Compute fill:#fff3e0
    style Final fill:#e8f5e9
```

### Mask åº”ç”¨é€»è¾‘

[`_apply_mask_and_soft_cap`](splash_attention_kernel.py:603) å‡½æ•°å¤„ç†å¤šç§ mask ç»„åˆï¼š

```mermaid
flowchart TB
    subgraph Input["è¾“å…¥"]
        QK["QK çŸ©é˜µ"]
        MV["mask_value"]
        SNM["should_not_mask"]
    end

    subgraph MaskSources["Mask æ¥æº (å¯ç»„åˆ)"]
        MS1["mask_ref: é¢„è®¡ç®—çš„ partial mask"]
        MS2["mask_function: å¯è®¡ç®— mask<br/>(CausalMask, LocalMaskç­‰)"]
        MS3["segment_ids: åˆ†æ®µæ³¨æ„åŠ› mask"]
    end

    subgraph Combine["ç»„åˆ Masks"]
        C1["masks = []"]
        C2["mask_ref å­˜åœ¨? â†’ æ·»åŠ "]
        C3["mask_function å­˜åœ¨? â†’ è®¡ç®—å¹¶æ·»åŠ "]
        C4["segment_ids å­˜åœ¨? â†’ æ·»åŠ "]
        C5["final_mask = reduce(AND, masks)"]
    end

    subgraph Apply["åº”ç”¨"]
        A1{"attn_logits_soft_cap?"}
        A2["logits = tanh(QK/cap) * cap"]
        A3["QK = where(mask, QK, mask_value)"]
    end

    QK --> Combine
    MV --> Apply
    SNM --> Combine

    MS1 --> C2
    MS2 --> C3
    MS3 --> C4

    C2 --> C5
    C3 --> C5
    C4 --> C5

    C5 --> A1
    A1 -->|Yes| A2
    A1 -->|No| A3
    A2 --> A3

    style MaskSources fill:#fff9c4
    style Combine fill:#e1f5fe
    style Apply fill:#f3e5f5
```

---

## åå‘ä¼ æ’­å®ç°

### åå‘ä¼ æ’­ç­–ç•¥

Splash Attention æ”¯æŒä¸¤ç§åå‘ä¼ æ’­ç­–ç•¥ï¼š

```mermaid
flowchart TB
    subgraph Strategy["åå‘ä¼ æ’­ç­–ç•¥"]
        S1["åˆ†ç¦»å¼ (é»˜è®¤)"]
        S2["èåˆå¼ (use_fused_bwd_kernel=True)"]
    end

    subgraph Separate["åˆ†ç¦»å¼åå‘ä¼ æ’­"]
        SP1["_splash_attention_bwd_dkv()<br/>è®¡ç®— dK, dV"]
        SP2["_splash_attention_bwd_dq()<br/>è®¡ç®— dQ"]
    end

    subgraph Fused["èåˆå¼åå‘ä¼ æ’­"]
        F1["_splash_attention_bwd_dkv()<br/>åŒæ—¶è®¡ç®— dQ, dK, dV"]
    end

    S1 --> SP1
    S1 --> SP2
    S2 --> F1

    style Separate fill:#e3f2fd
    style Fused fill:#fff3e0
```

### dKV Kernel æµç¨‹

[`_flash_attention_dkv_kernel`](splash_attention_kernel.py:1673) çš„è®¡ç®—æµç¨‹ï¼š

```mermaid
flowchart TB
    subgraph Init["åˆå§‹åŒ–"]
        I1["dk_scratch = 0"]
        I2["dv_scratch = 0"]
        I3["dq_scratch = 0 (èåˆæ¨¡å¼)"]
    end

    subgraph Loop["è®¡ç®—å¾ªç¯"]
        L1["K, V blocks"]
        L2["QK = K @ Q^T"]
        L3["åº”ç”¨ Mask"]
        L4["P = exp(QK - logsumexp)"]
        L5["dV = P @ dO"]
        L6["dP = V @ dO^T"]
        L7["dS = (dP - di) * P"]
        L8["dK = dS @ Q"]
        L9["dQ = dS^T @ K (èåˆæ¨¡å¼)"]
    end

    subgraph Accumulate["ç´¯ç§¯æ¢¯åº¦"]
        A1["dk_scratch += dK"]
        A2["dv_scratch += dV"]
        A3["dq_scratch += dQ"]
    end

    subgraph Output["è¾“å‡º"]
        O1["dk_ref = dk_scratch"]
        O2["dv_ref = dv_scratch"]
    end

    Init --> Loop
    Loop --> L1
    L1 --> L2 --> L3 --> L4
    L4 --> L5 --> A2
    L4 --> L6 --> L7
    L7 --> L8 --> A1
    L7 --> L9 --> A3
    A1 --> O1
    A2 --> O2

    style Loop fill:#fff3e0
    style Accumulate fill:#e8f5e9
```

### dQ Kernel æµç¨‹

[`_flash_attention_dq_kernel`](splash_attention_kernel.py:1312) çš„è®¡ç®—æµç¨‹ï¼š

```mermaid
flowchart TB
    subgraph Init["åˆå§‹åŒ–"]
        I1["dq_scratch = 0"]
    end

    subgraph Loop["è®¡ç®—å¾ªç¯"]
        L1["åŠ è½½ Q, K, V blocks"]
        L2["QK = Q @ K^T"]
        L3["åº”ç”¨ Mask"]
        L4["P = exp(QK - logsumexp)"]
        L5["dP = dO @ V^T"]
        L6["dS = (dP - di) * P"]
        L7["dQ = dS @ K"]
    end

    subgraph Output["è¾“å‡º"]
        O1["dq_ref = dq_scratch"]
    end

    Init --> Loop
    Loop --> L1 --> L2 --> L3 --> L4
    L4 --> L5 --> L6 --> L7
    L7 --> O1

    style Loop fill:#e3f2fd
```

---

## å†…å­˜ä¼˜åŒ–ç­–ç•¥

### TPU å†…å­˜å±‚çº§åˆ©ç”¨

```mermaid
flowchart TB
    subgraph HBM["HBM (High Bandwidth Memory)"]
        H1["Q, K, V å®Œæ•´å¼ é‡"]
        H2["è¾“å‡º O"]
        H3["æ¢¯åº¦ dQ, dK, dV"]
    end

    subgraph VMEM["VMEM (Vector Memory)"]
        V1["å½“å‰è®¡ç®—å—"]
        V2["ç´¯ç§¯å™¨ scratch buffers"]
    end

    subgraph SMEM["SMEM (Scalar Memory)"]
        S1["data_next"]
        S2["mask_next"]
        S3["block_mask"]
        S4["segment_ids"]
    end

    H1 -->|BlockSpec| V1
    V1 -->|ç´¯ç§¯| V2
    V2 -->|å†™å›| H2

    S1 -->|æ§åˆ¶| V1
    S2 -->|ç´¢å¼•| V1
    S3 -->|è·³è¿‡åˆ¤æ–­| V1

    style HBM fill:#ffcdd2
    style VMEM fill:#fff9c4
    style SMEM fill:#c8e6c9
```

### æ•°æ®ç±»å‹ä¼˜åŒ–

MaskInfo ä¸­çš„æ•°ç»„ä¼šè‡ªåŠ¨é™çº§åˆ°æœ€å°æ‰€éœ€ç±»å‹ï¼š

```python
def _downcast_to_small_type(array: np.ndarray) -> np.ndarray:
    max_value = np.max(array)
    if max_value <= np.iinfo(np.int8).max:
        return array.astype(np.int8)
    elif max_value <= np.iinfo(np.int16).max:
        return array.astype(np.int16)
    else:
        return array.astype(np.int32)
```

---

## API ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
from jax.experimental.pallas.ops.tpu.splash_attention import (
    splash_attention_kernel as splash,
    splash_attention_mask as mask_lib,
)

# åˆ›å»º Causal Mask
mask = mask_lib.CausalMask(shape=(seq_len, seq_len))
multi_head_mask = mask_lib.MultiHeadMask([mask] * num_heads)

# é…ç½®å—å¤§å°
block_sizes = splash.BlockSizes(
    block_q=128,
    block_kv=128,
    block_kv_compute=128,
    block_q_dkv=128,
    block_kv_dkv=128,
    block_kv_dkv_compute=128,
    block_q_dq=128,
    block_kv_dq=128,
)

# åˆ›å»º kernel
kernel = splash.make_splash_mha_single_device(
    mask=multi_head_mask,
    block_sizes=block_sizes,
)

# æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—
output = kernel(q, k, v)
```

### ä½¿ç”¨ Local Attention

```python
# Local attention åªå…³æ³¨å‰å window_size ä¸ª token
local_mask = mask_lib.LocalMask(
    shape=(seq_len, seq_len),
    window_size=(256, 256),  # (å·¦ä¾§çª—å£, å³ä¾§çª—å£)
    offset=0,
)

# ç»„åˆ causal å’Œ local
combined_mask = causal_mask & local_mask
```

### ä½¿ç”¨ Segment IDs

```python
# ç”¨äºå¤„ç† packed sequences
segment_ids = splash.SegmentIds(
    q=jnp.array([0, 0, 0, 1, 1, 1, 2, 2]),   # Q åºåˆ—çš„æ®µ ID
    kv=jnp.array([0, 0, 0, 1, 1, 1, 2, 2]),  # KV åºåˆ—çš„æ®µ ID
)

output = kernel(q, k, v, segment_ids=segment_ids)
```

### åˆ†å¸ƒå¼ä½¿ç”¨

```python
# å¤šè®¾å¤‡åˆ†ç‰‡
kernel = splash.make_splash_mha(
    mask=multi_head_mask,
    block_sizes=block_sizes,
    head_shards=num_devices_per_head_dim,
    q_seq_shards=num_devices_per_seq_dim,
)

# è·å–åˆ†ç‰‡è§„èŒƒ
sharding_spec = kernel.manual_sharding_spec(named_sharding)
```

---

## å…³é”®å¸¸é‡

| å¸¸é‡ | å€¼ | è¯´æ˜ |
|-----|-----|------|
| `NUM_LANES` | 128 | TPU å‘é‡å®½åº¦ |
| `NUM_SUBLANES` | 8 | TPU å­é€šé“æ•° |
| `DEFAULT_MASK_VALUE` | -0.7 * float32_max | é»˜è®¤ mask å€¼ |

---

## æ€»ç»“

Splash Attention é€šè¿‡ä»¥ä¸‹æœºåˆ¶å®ç°é«˜æ•ˆçš„é•¿åºåˆ—æ³¨æ„åŠ›è®¡ç®—ï¼š

1. **Block-level ç¨€ç–æ€§**ï¼šé€šè¿‡ `block_mask` è·³è¿‡å…¨é›¶å—
2. **åœ¨çº¿ Softmax**ï¼šé¿å…æåŒ–å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ
3. **å¯è®¡ç®— Mask**ï¼šä½¿ç”¨ `mask_function` è€Œéå­˜å‚¨å®Œæ•´ mask
4. **Grid Shrinking**ï¼šå‹ç¼©ç¨€ç–çš„è®¡ç®—ç½‘æ ¼
5. **æ•°æ®ç±»å‹ä¼˜åŒ–**ï¼šè‡ªåŠ¨é™çº§ SMEM æ•°æ®ç±»å‹
6. **åˆ†å¸ƒå¼æ”¯æŒ**ï¼šæ”¯æŒ head å’Œ sequence ç»´åº¦çš„åˆ†ç‰‡

è¿™ä½¿å¾— Splash Attention æˆä¸º TPU ä¸Šå¤„ç†é•¿åºåˆ—ä»»åŠ¡çš„é¦–é€‰æ³¨æ„åŠ›å®ç°ã€‚