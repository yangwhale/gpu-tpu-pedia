# PyTorch GPU â†’ torchax TPU è¿ç§»æŒ‡å—

æœ¬æ–‡æ¡£è®°å½•äº†å°† HunyuanVideo-1.5 Transformer ä» GPU PyTorch è¿ç§»åˆ° TPU torchax çš„å®Œæ•´è¿‡ç¨‹ã€‚

---

## ğŸ“š ç›®å½•

1. [è¿ç§»æ¦‚è§ˆ](#1-è¿ç§»æ¦‚è§ˆ)
2. [å¿«é€Ÿå¼€å§‹](#2-å¿«é€Ÿå¼€å§‹)
3. [æ ¸å¿ƒä¿®å¤](#3-æ ¸å¿ƒä¿®å¤)
4. [DeepCache åŠ é€Ÿ](#4-deepcache-åŠ é€Ÿ)
5. [å¸¸è§é™·é˜±](#5-å¸¸è§é™·é˜±)
6. [æ€§èƒ½ä¼˜åŒ–](#6-æ€§èƒ½ä¼˜åŒ–)
7. [è°ƒè¯•æŠ€å·§](#7-è°ƒè¯•æŠ€å·§)

---

## 1. è¿ç§»æ¦‚è§ˆ

### æŠ€æœ¯æ ˆå¯¹æ¯”

| æŠ€æœ¯å±‚ | GPU ç‰ˆæœ¬ | TPU ç‰ˆæœ¬ |
|--------|----------|----------|
| è¿è¡Œæ¡†æ¶ | PyTorch | torchax (PyTorch â†’ JAX) |
| Attention | Flash Attention | Splash Attention (Pallas) |
| JIT ç¼–è¯‘ | torch.compile | XLA JIT |
| åˆ†å¸ƒå¼ | NCCL + æ‰‹åŠ¨ SP/TP | GSPMD (è‡ªåŠ¨åˆ†ç‰‡) |
| æ•°æ®ç±»å‹ | fp16 / fp32 | bf16 (åŸç”Ÿæ”¯æŒ) |

### è¿ç§»æµç¨‹

```
1. ç¯å¢ƒè®¾ç½® â†’ 2. Monkey-patch â†’ 3. æ¨¡å‹åŠ è½½ â†’ 4. æƒé‡åˆ†ç‰‡ â†’ 5. JIT ç¼–è¯‘ â†’ 6. æ¨ç†
```

---

## 2. å¿«é€Ÿå¼€å§‹

### 2.1 ç¯å¢ƒè®¾ç½®

```python
import jax
import torch
import torchax
from jax.sharding import Mesh
from jax.experimental import mesh_utils

# åˆ›å»º JAX Mesh
mesh_devices = mesh_utils.create_device_mesh((jax.device_count(), 1, 1))
mesh = Mesh(mesh_devices, ('tp', 'dp', 'sp'))

# åˆ›å»º torchax ç¯å¢ƒ
env = torchax.default_env()
env._mesh = mesh
env.config.use_tpu_splash_attention = True

torch.set_default_dtype(torch.bfloat16)
```

### 2.2 æ³¨å†Œ Splash Attention

```python
from torchax.ops import ops_registry

custom_attention = functools.partial(scaled_dot_product_attention, env=env)
env._ops[torch.nn.functional.scaled_dot_product_attention] = ops_registry.Operator(
    torch.nn.functional.scaled_dot_product_attention,
    custom_attention,
    is_jax_function=False,
    is_user_defined=True,
    needs_env=False,
    is_view_op=False,
)
```

### 2.3 æ¨¡å‹åŠ è½½å’Œåˆ†ç‰‡

```python
model = Model.from_pretrained(path, torch_dtype=torch.bfloat16)

with env:
    with jax.default_device('cpu'):
        state_dict = model.state_dict()
        state_dict = env.to_xla(state_dict)
        model.load_state_dict(state_dict, assign=True)
    
    weights = shard_weights(mesh, model.state_dict())
    model.load_state_dict(weights, assign=True, strict=False)

model.eval()
```

### 2.4 JIT ç¼–è¯‘å’Œæ¨ç†

```python
with env:
    model = torchax.compile(model, torchax.CompileOptions(
        jax_jit_kwargs={'static_argnames': ('return_dict',)}
    ))

with mesh, env:
    with torch.no_grad():
        for t in timesteps:
            output = model(inputs)
            latents = scheduler.step(output, t, latents)[0]
            latents = latents.to(torch.bfloat16)

os._exit(0)  # å¼ºåˆ¶é€€å‡ºï¼Œé¿å… JAX åå°çº¿ç¨‹é˜»å¡
```

---

## 3. æ ¸å¿ƒä¿®å¤

### 3.1 Attention Maskï¼ˆæ ¹æœ¬åŸå› ï¼‰

**é—®é¢˜**ï¼šGPU ä½¿ç”¨ `flex_attention` + `score_mod` å±è”½ paddingï¼ŒTPU ç‰ˆæœ¬å¿½ç•¥äº† maskã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šå°† padding ä½ç½®çš„ K/V è®¾ä¸ºé›¶

```python
if text_mask is not None:
    text_mask_expanded = text_mask.unsqueeze(-1).unsqueeze(-1).to(encoder_key.dtype)
    encoder_key = encoder_key * text_mask_expanded
    encoder_value = encoder_value * text_mask_expanded

query = torch.cat([query, encoder_query], dim=1)
key = torch.cat([key, encoder_key], dim=1)
value = torch.cat([value, encoder_value], dim=1)

hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=None)
```

**åŸç†**ï¼šå½“ K[i]=0 æ—¶ï¼ŒQK^T[i]â‰ˆ0ï¼Œsoftmax åæƒé‡å¾ˆä½ï¼Œæ•ˆæœè¿‘ä¼¼äº -inf maskã€‚

### 3.2 vision_states å¤„ç†

**é—®é¢˜**ï¼š`torch.all(vision_states == 0)` åœ¨ JIT ä¸­å¯¼è‡´ ConcretizationTypeErrorã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼št2v æ¨¡å¼ç›´æ¥ä¼ å…¥ `None`

```python
if task_type == 't2v':
    vision_states = None  # è·³è¿‡ vision_in åˆ†æ”¯
```

### 3.3 å¸ƒå°”ç´¢å¼•ä¸æ”¯æŒ

**é—®é¢˜**ï¼štorchax ä¸æ”¯æŒ `tensor[bool_mask]`

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ç®€åŒ–é€»è¾‘æˆ– argsort + gather

```python
# ç®€åŒ–æ–¹æ¡ˆï¼šç›´æ¥æ‹¼æ¥ï¼ˆç¦ç”¨ reorderï¼‰
reorder_txt = torch.concat([byt5_txt, txt], dim=1)
reorder_mask = torch.concat([byt5_text_mask, text_mask], dim=1)
```

### 3.4 åŠ¨æ€ Tensor åˆ›å»º

**é—®é¢˜**ï¼šJIT å†…éƒ¨ `torch.arange()` å¯¼è‡´é‡å¤ç¼–è¯‘

**è§£å†³æ–¹æ¡ˆ**ï¼šé¢„è®¡ç®—å¹¶ç¼“å­˜

```python
# JIT å¤–é¢„è®¡ç®—
freqs_cos, freqs_sin = model.get_rotary_pos_embed(size)
with env:
    model._cached_freqs_cos = freqs_cos.to('jax')
    model._cached_freqs_sin = freqs_sin.to('jax')

# Monkey-patch ä½¿ç”¨ç¼“å­˜
def cached_get_rotary(self, size):
    return self._cached_freqs_cos, self._cached_freqs_sin
model.get_rotary_pos_embed = types.MethodType(cached_get_rotary, model)
```

---

## 4. DeepCache åŠ é€Ÿ

### 4.1 æ¦‚è¿°

DeepCache é€šè¿‡ç¼“å­˜ transformer ä¸­é—´çŠ¶æ€æ¥è·³è¿‡éƒ¨åˆ†å±‚çš„è®¡ç®—ï¼Œå®ç°åŠ é€Ÿã€‚

**HunyuanVideo ç»“æ„**ï¼š
- 20 ä¸ª double_blocks + 40 ä¸ª single_blocks
- ç¼“å­˜ç‚¹ï¼šdouble_blocks è¾“å‡º (img, txt)
- è·³è¿‡ï¼š20 å±‚ double_blocks
- ç†è®ºåŠ é€Ÿæ¯”ï¼š61/41 â‰ˆ 1.49x

### 4.2 TPU å…¼å®¹æ€§é—®é¢˜

**é—®é¢˜**ï¼šå¸¸è§çš„ `jax.lax.cond` æ–¹æ¡ˆåœ¨ torchax ä¸­ä¸å¯ç”¨ã€‚

**åŸå› **ï¼š
1. `jax.lax.cond` è¦æ±‚ä¸¤ä¸ªåˆ†æ”¯è¿”å›å®Œå…¨ç›¸åŒçš„ pytree ç»“æ„
2. torchax çš„ tensor wrapper ä½¿ç»“æ„åŒ¹é…å›°éš¾
3. JAX tracer æ³„æ¼é—®é¢˜

### 4.3 è§£å†³æ–¹æ¡ˆï¼šåˆ†ç¦»æ¨¡å—

ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹ç¼–è¯‘çš„æ¨¡å—ï¼Œåœ¨ Python å±‚åšæ¡ä»¶åˆ†æ”¯ï¼š

```python
class FullForwardModule(torch.nn.Module):
    """å®Œæ•´ forwardï¼Œè¿”å› (output, img_cache, txt_cache, vec, text_mask)"""
    def forward(self, hidden_states, ...):
        # æ‰§è¡Œå®Œæ•´ transformer
        # ä¿å­˜ double_blocks åçš„çŠ¶æ€
        return (output, img_after_double, txt_after_double, vec, text_mask)

class CachedForwardModule(torch.nn.Module):
    """ä½¿ç”¨ç¼“å­˜ï¼Œè·³è¿‡ double_blocks"""
    def forward(self, cached_img, cached_txt, vec, freqs_cos, freqs_sin, text_mask):
        # åªæ‰§è¡Œ single_blocks + final_layer
        return output
```

### 4.4 TPUDeepCache ç±»

```python
class TPUDeepCache:
    def __init__(self, cache_start_step, cache_end_step, cache_step_interval, total_steps):
        self.no_cache_steps = set(
            list(range(0, cache_start_step)) +
            list(range(cache_start_step, cache_end_step, cache_step_interval)) +
            list(range(cache_end_step, total_steps))
        )
        self.cached_img = None
        self.cached_txt = None
    
    def should_use_cache(self, step):
        return step not in self.no_cache_steps and self.cached_img is not None
    
    def update_cache(self, img, txt, vec, text_mask):
        self.cached_img = img
        self.cached_txt = txt
        # ...
```

### 4.5 æ¨ç†å¾ªç¯é›†æˆ

```python
for i in range(num_steps):
    if deep_cache.should_use_cache(i):
        # ä½¿ç”¨ç¼“å­˜è·¯å¾„ï¼ˆè·³è¿‡ double_blocksï¼‰
        cached_img, cached_txt, vec, text_mask = deep_cache.get_cache()
        noise_pred = cached_forward_fn(cached_img, cached_txt, vec, ...)
    else:
        # å®Œæ•´ forwardï¼ˆåŒæ—¶æ›´æ–°ç¼“å­˜ï¼‰
        output = full_forward_fn(latent_model_input, ...)
        noise_pred, img_cache, txt_cache, vec, text_mask = output
        deep_cache.update_cache(img_cache, txt_cache, vec, text_mask)
```

### 4.6 ä½¿ç”¨æ–¹æ³•

```bash
python stage2_transformer_flax_experimental_deepcache.py \
    --enable_cache \
    --cache_start_step 11 \
    --cache_end_step 45 \
    --cache_step_interval 4 \
    --video_length 121
```

### 4.7 æ€§èƒ½ç»“æœ

| é…ç½® | æ—  DeepCache | æœ‰ DeepCache | åŠ é€Ÿæ¯” |
|------|-------------|-------------|--------|
| 121å¸§, 50æ­¥ | ~350s | ~203s | 1.72x |

### 4.8 å…³é”®ç»éªŒ

1. **ä¸è¦ä½¿ç”¨ jax.lax.cond**ï¼štorchax ç¯å¢ƒä¸‹ä¼šå¯¼è‡´ tracer æ³„æ¼
2. **åˆ†ç¦»ç¼–è¯‘**ï¼šä¸¤ä¸ªæ¨¡å—ç‹¬ç«‹ç¼–è¯‘ï¼Œé¿å…ç»“æ„åŒ¹é…é—®é¢˜
3. **Python åˆ†æ”¯**ï¼šæ¡ä»¶åˆ¤æ–­æ”¾åœ¨ Python å±‚ï¼Œä¸åœ¨ JIT å†…éƒ¨
4. **freqs_cos/sin ç¼“å­˜**ï¼šä½¿ç”¨ `transformer._cached_freqs_cos/sin`ï¼Œä¸ä¾èµ– JIT è¿”å›å€¼
5. **æ¸…é™¤é¢„çƒ­ç¼“å­˜**ï¼šwarmup åè°ƒç”¨ `deep_cache.clear()`

---

## 5. å¸¸è§é™·é˜±

### é€ŸæŸ¥è¡¨

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| è§†é¢‘æœ‰ç«–æ¡çº¹/ä¸è·Ÿéšæç¤ºè¯ | Attention Mask æœªå¤„ç† | K/V ç½®é›¶æ–¹æ¡ˆ |
| ConcretizationTypeError | åŠ¨æ€æ¡ä»¶/æ–­è¨€ | Monkey-patch ç§»é™¤ |
| å¸ƒå°”ç´¢å¼•æŠ¥é”™ | torchax ä¸æ”¯æŒ | ä½¿ç”¨ torch.where æˆ–ä¹˜æ³• |
| ç¨‹åºä¸é€€å‡º | JAX åå°çº¿ç¨‹ | `os._exit(0)` |
| ç¬¬ä¸€æ­¥æ…¢ï¼ˆ60s+ï¼‰ | XLA ç¼–è¯‘ | æ­£å¸¸ï¼Œä½¿ç”¨ warmup |
| OOM | å®Œæ•´ attention mask | Splash Attention + K/V ç½®é›¶ |
| Scheduler å dtype å˜åŒ– | å†…éƒ¨è½¬ fp32 | æ¯æ­¥å `.to(bf16)` |

### å…¸å‹ä¿®å¤æ¨¡å¼

```python
# å¸ƒå°”ç´¢å¼• â†’ ä¹˜æ³•
# âŒ selected = tensor[mask]
# âœ… selected = tensor * mask.unsqueeze(-1).float()

# è¿è¡Œæ—¶æ£€æŸ¥ â†’ ç§»é™¤
# âŒ assert tensor.min() >= 0
# âœ… ï¼ˆç›´æ¥åˆ é™¤ï¼‰

# åŠ¨æ€ tensor â†’ é¢„è®¡ç®—
# âŒ torch.arange(n) åœ¨ JIT å†…
# âœ… é¢„è®¡ç®—å¹¶ç¼“å­˜åˆ°æ¨¡å‹å±æ€§
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 æƒé‡åˆ†ç‰‡ç­–ç•¥

#### 6.1.1 TP + fc2/proj Replicatedï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰

å°† MLP fc2 å’Œ attention proj æƒé‡å®Œå…¨å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡ï¼Œæ¶ˆé™¤ all-reduce å¼€é”€ã€‚

```python
# åˆ†ç‰‡ç­–ç•¥å®šä¹‰
transformer_shardings_tp_fc2_replicated = {
    # Column Parallelï¼ˆQ/K/V, fc1ï¼‰- è¾“å‡ºç»´åº¦åˆ†ç‰‡
    r'.*\.img_attn_q\.weight$': (('tp', 'sp'), None),
    r'.*\.img_mlp\.fc1\.weight$': (('tp', 'sp'), None),
    
    # REPLICATEDï¼ˆfc2, projï¼‰- æ—  all-reduce
    r'.*\.img_attn_proj\.weight$': (None, None),
    r'.*\.img_mlp\.fc2\.weight$': (None, None),
}
```

**æ€§èƒ½å¯¹æ¯”ï¼ˆ121å¸§ 720p, 8Ã— TPU v6eï¼‰**ï¼š

| åˆ†ç‰‡æ¨¡å¼ | Step Time | ç›¸å¯¹æ€§èƒ½ | HBM å¢é‡ |
|----------|-----------|----------|----------|
| æ ‡å‡† TP | 8.12s | baseline | 0 GB |
| **TP + fc2 Replicated** | **7.29s** | **+10.2%** | ~12 GB |
| TP + å…¨ MLP Replicated | 8.18s | -0.7% | ~21 GB |

**å…³é”®å‘ç°**ï¼š
- åªå¤åˆ¶ Row Parallel å±‚ï¼ˆfc2, projï¼‰æ˜¯æœ€ä¼˜ç­–ç•¥
- å¤åˆ¶ Column Parallel å±‚ï¼ˆQ/K/V, fc1ï¼‰æ²¡æœ‰æ”¶ç›Šï¼Œåè€Œå¢åŠ  HBM å¸¦å®½å‹åŠ›
- åŸå› ï¼šColumn Parallel å±‚æœ¬æ¥å°±ä¸éœ€è¦ all-reduce

#### 6.1.2 æ ‡å‡† TPï¼ˆMegatron Column-Rowï¼‰

æ¯ä¸ª block æœ‰ 2 æ¬¡ all-reduceï¼š

```
Attention: Q/K/V (Column) â†’ proj (Row) â†’ all-reduce
MLP: fc1 (Column) â†’ fc2 (Row) â†’ all-reduce
```

```python
transformer_shardings_tp = {
    r'.*\.img_attn_q\.weight$': (('tp', 'sp'), None),   # Column
    r'.*\.img_attn_proj\.weight$': (None, ('tp', 'sp')),  # Row (all-reduce)
    r'.*\.img_mlp\.fc1\.weight$': (('tp', 'sp'), None),   # Column
    r'.*\.img_mlp\.fc2\.weight$': (None, ('tp', 'sp')),   # Row (all-reduce)
}
```

#### 6.1.3 Profiler åˆ†æ

ä½¿ç”¨ JAX Profiler å¯ä»¥è§‚å¯Ÿ all-reduce æ“ä½œï¼š

```bash
python stage2_transformer_flax.py --enable_profiler --num_inference_steps 3
```

å…¸å‹æ—¶é—´åˆ†å¸ƒï¼ˆå•ä¸ª blockï¼‰ï¼š
- Splash Attention: ~35msï¼ˆä¸»å¯¼ï¼‰
- Linear + all-reduce: ~45ms

å³ä½¿å¤åˆ¶äº† fc2/proj æƒé‡ï¼Œä»ä¼šæœ‰éƒ¨åˆ† all-reduceï¼Œå› ä¸ºæ¿€æ´»å€¼ä»æ˜¯ sharded çš„ã€‚

### 6.2 Warmup ç­–ç•¥

XLA ç¼–è¯‘æ˜¯æƒ°æ€§çš„ï¼Œå‰ 1-2 æ­¥ä¼šè§¦å‘ç¼–è¯‘ã€‚

```python
# æ¨è 2 æ­¥é¢„çƒ­
if args.warmup_steps > 0:
    run_denoising_loop(latents, timesteps, args.warmup_steps, is_warmup=True)
```

### 6.3 JIT ç¼“å­˜

```python
jax.config.update("jax_compilation_cache_dir", "/dev/shm/jax_cache")
jax.config.update("jax_persistent_cache_min_entry_size_bytes", -1)
jax.config.update("jax_persistent_cache_min_compile_time_secs", 0)
```

æ•ˆæœï¼šé¦–æ¬¡ ~60s ç¼–è¯‘ â†’ åç»­ ~5s åŠ è½½ç¼“å­˜

### 6.4 å‡†ç¡®è®¡æ—¶

```python
output = model(input)
torchax.interop.call_jax(jax.block_until_ready, output._elem)
step_time = time.perf_counter() - start  # å‡†ç¡®æ—¶é—´
```

### 6.5 æ€§èƒ½åŸºå‡†

| é…ç½® | Token æ•° | æ€»æ—¶é—´ | æ¯æ­¥æ—¶é—´ |
|------|----------|--------|----------|
| 49å¸§, 720p | 46,800 | ~215s | ~4.3s |
| 121å¸§, 720p (æ ‡å‡† TP) | 111,600 | ~406s | ~8.1s |
| 121å¸§, 720p (TP + fc2 Replicated) | 111,600 | ~365s | ~7.3s |
| 121å¸§ + DeepCache | 111,600 | ~203s | ~4.1s |

ç¯å¢ƒï¼šTPU v6e-8ï¼Œ50 æ­¥æ¨ç†

---

## 7. è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹å®Œæ•´ traceback

```bash
JAX_TRACEBACK_FILTERING=off python script.py
```

### æ£€æµ‹ XLA tensor

```python
def is_xla_tensor(t):
    return hasattr(t, '_elem') or ('jax' in str(getattr(t, 'device', '')))
```

### è°ƒè¯•æ‰“å°

```python
def debug_tensor(name, t):
    print(f"{name}: shape={t.shape}, dtype={t.dtype}, mean={t.float().mean():.4f}")
```

---

## ğŸ“‹ è¿ç§» Checklist

- [ ] åˆ›å»º JAX Mesh å’Œ torchax ç¯å¢ƒ
- [ ] æ³¨å†Œ Splash Attention
- [ ] Monkey-patch ä¸å…¼å®¹ä»£ç ï¼ˆåœ¨å¯¼å…¥æ¨¡å‹å‰ï¼‰
- [ ] åŠ è½½æ¨¡å‹å¹¶è½¬æ¢æƒé‡åˆ° XLA
- [ ] æƒé‡åˆ†ç‰‡
- [ ] é¢„è®¡ç®—åŠ¨æ€ tensorï¼ˆå¦‚ Rotary Embeddingsï¼‰
- [ ] JIT ç¼–è¯‘
- [ ] ä¿®å¤ Attention Maskï¼ˆK/V ç½®é›¶ï¼‰
- [ ] æ¯æ­¥å dtype è½¬æ¢
- [ ] ä½¿ç”¨ `os._exit(0)` é€€å‡º

---

## ğŸ“š å‚è€ƒèµ„æº

- [torchax GitHub](https://github.com/pytorch/xla)
- [JAX Splash Attention](https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention)
- [HunyuanVideo-1.5](https://github.com/Tencent/HunyuanVideo)
- [TPU bf16 ç²¾åº¦è¯´æ˜](https://cloud.google.com/tpu/docs/bfloat16)