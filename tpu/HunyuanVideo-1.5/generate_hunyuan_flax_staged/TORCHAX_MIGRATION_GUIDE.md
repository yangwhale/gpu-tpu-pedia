# PyTorch GPU â†’ torchax TPU å®Œæ•´è¿ç§»æŒ‡å—

æœ¬æ–‡æ¡£è®°å½•äº†å°† HunyuanVideo-1.5 Transformer ä» GPU PyTorch è¿ç§»åˆ° TPU torchax çš„å®Œæ•´è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é—®é¢˜åˆ†æã€è§£å†³æ–¹æ¡ˆå’Œæœ€ä½³å®è·µã€‚

---

## ğŸ“š ç›®å½•

1. [è¿ç§»æ¦‚è§ˆ](#1-è¿ç§»æ¦‚è§ˆ)
2. [æ¶æ„å¯¹æ¯”](#2-æ¶æ„å¯¹æ¯”)
3. [æ ¸å¿ƒä¿®å¤è¯¦è§£](#3-æ ¸å¿ƒä¿®å¤è¯¦è§£)
4. [å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ](#4-å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ)
5. [å®Œæ•´è¿ç§»æµç¨‹](#5-å®Œæ•´è¿ç§»æµç¨‹)
6. [ä»£ç æ¨¡æ¿](#6-ä»£ç æ¨¡æ¿)
7. [æ€§èƒ½ä¼˜åŒ–](#7-æ€§èƒ½ä¼˜åŒ–)
8. [ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–](#8-ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–)
9. [è°ƒè¯•æŠ€å·§](#9-è°ƒè¯•æŠ€å·§)

---

## 1. è¿ç§»æ¦‚è§ˆ

### 1.1 æ•´ä½“è¿ç§»æµç¨‹

```mermaid
flowchart TB
    subgraph GPU["ğŸ® GPU ç‰ˆæœ¬"]
        G1[PyTorch æ¨¡å‹] --> G2[Flash Attention]
        G2 --> G3[CUDA Kernels]
        G3 --> G4[NCCL åˆ†å¸ƒå¼]
    end
    
    subgraph TPU["â˜ï¸ TPU ç‰ˆæœ¬"]
        T1[PyTorch æ¨¡å‹] --> T2[torchax æ¡¥æ¥]
        T2 --> T3[Splash Attention]
        T3 --> T4[XLA ç¼–è¯‘]
        T4 --> T5[GSPMD åˆ†å¸ƒå¼]
    end
    
    GPU --> |è¿ç§»| TPU
    
    style GPU fill:#ffcccc
    style TPU fill:#ccffcc
```

### 1.2 å…³é”®æŠ€æœ¯æ ˆå¯¹æ¯”

| æŠ€æœ¯å±‚ | GPU ç‰ˆæœ¬ | TPU ç‰ˆæœ¬ |
|--------|----------|----------|
| è¿è¡Œæ¡†æ¶ | PyTorch | torchax (PyTorch â†’ JAX) |
| Attention | Flash Attention 2/3 | Splash Attention (Pallas) |
| JIT ç¼–è¯‘ | torch.compile | XLA JIT |
| åˆ†å¸ƒå¼ | NCCL + æ‰‹åŠ¨ SP/TP | GSPMD (è‡ªåŠ¨åˆ†ç‰‡) |
| æ•°æ®ç±»å‹ | fp16 / fp32 | bf16 (åŸç”Ÿæ”¯æŒ) |
| è®¾å¤‡ç®¡ç† | CUDA | JAX Device Mesh |

---

## 2. æ¶æ„å¯¹æ¯”

### 2.1 æ•°æ®æµå¯¹æ¯”

```mermaid
flowchart LR
    subgraph GPU["GPU æ•°æ®æµ"]
        direction TB
        GA[CPU Tensor] --> GB[.cuda]
        GB --> GC[GPU Tensor]
        GC --> GD[Model Forward]
        GD --> GE[Output Tensor]
    end
    
    subgraph TPU["TPU æ•°æ®æµ"]
        direction TB
        TA[CPU Tensor] --> TB[.to jax]
        TB --> TC[env.to_xla]
        TC --> TD[XLA Tensor]
        TD --> TE[JIT Compiled Model]
        TE --> TF[Output Tensor]
    end
    
    style GPU fill:#f9f
    style TPU fill:#9ff
```

### 2.2 Attention å®ç°å¯¹æ¯”

```mermaid
flowchart TB
    subgraph Flash["Flash Attention (GPU)"]
        F1[Q, K, V] --> F2[Variable Length<br/>Packed Sequences]
        F2 --> F3[flash_attn_no_pad]
        F3 --> F4[CUDA Kernel<br/>åˆ†å—è®¡ç®—]
        F4 --> F5[Output]
    end
    
    subgraph Splash["Splash Attention (TPU)"]
        S1[Q, K, V] --> S2[Pad to Block Size]
        S2 --> S3[shard_map åˆ†ç‰‡]
        S3 --> S4[Pallas Kernel<br/>åˆ†å—è®¡ç®—]
        S4 --> S5[Trim Padding]
        S5 --> S6[Output]
    end
    
    style Flash fill:#ffcccc
    style Splash fill:#ccffcc
```

---

## 3. æ ¸å¿ƒä¿®å¤è¯¦è§£

åœ¨å°† HunyuanVideo-1.5 è¿ç§»åˆ° TPU æ—¶ï¼Œé‡åˆ°äº†å¤šä¸ªå¯¼è‡´ç”Ÿæˆè´¨é‡é—®é¢˜çš„å…³é”®å·®å¼‚ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†åˆ†æå’Œä¿®å¤è¿‡ç¨‹ã€‚

### 3.1 ä¿®å¤ #1: Attention Mask é—®é¢˜ï¼ˆæ ¹æœ¬åŸå› ï¼‰

> âš ï¸ **é‡è¦æ›´æ–°**ï¼šç»è¿‡è¿›ä¸€æ­¥è°ƒè¯•ï¼Œæˆ‘ä»¬å‘ç°è§†é¢‘è´¨é‡é—®é¢˜çš„**æ ¹æœ¬åŸå› **æ˜¯ Attention Mask å¤„ç†ï¼Œè€Œé ByT5 ç²¾åº¦é—®é¢˜ã€‚ByT5 å¯ä»¥å®‰å…¨ä½¿ç”¨ bf16ã€‚

#### é—®é¢˜åˆ†æ

è¿™æ˜¯æœ€å…³é”®çš„ä¿®å¤ã€‚GPU ç‰ˆæœ¬ä½¿ç”¨ `flex_attention` é…åˆ `score_mod` å‡½æ•°æ¥å±è”½ padding tokensï¼Œè€Œæˆ‘ä»¬çš„åˆå§‹ TPU ç‰ˆæœ¬å®Œå…¨å¿½ç•¥äº†è¿™ä¸ª maskã€‚

```mermaid
flowchart TB
    subgraph GPU["GPU: Attention Mask å¤„ç†"]
        G1[text_mask] --> G2[F.pad æ‰©å±•åˆ°å®Œæ•´åºåˆ—]
        G2 --> G3[flex_attention<br/>score_mod å‡½æ•°]
        G3 --> G4[Padding ä½ç½® â†’ -inf]
        G4 --> G5[Softmax åæƒé‡ = 0]
    end
    
    subgraph TPU_BAD["TPU (é”™è¯¯): æ—  Mask"]
        T1[text_mask] --> T2[å¿½ç•¥!]
        T2 --> T3[Splash Attention<br/>æ—  mask]
        T3 --> T4[Padding å‚ä¸è®¡ç®—]
        T4 --> T5[æ³¨æ„åŠ›æ±¡æŸ“]
    end
    
    subgraph TPU_GOOD["TPU (ä¿®å¤): K/V ç½®é›¶è¿‘ä¼¼"]
        F1[text_mask] --> F2[æ‰©å±• mask ç»´åº¦]
        F2 --> F3[K *= mask<br/>V *= mask]
        F3 --> F4[Padding ä½ç½® K/V = 0]
        F4 --> F5[QK^T å¯¹åº”ä½ç½® â‰ˆ 0]
        F5 --> F6[Softmax åæƒé‡å¾ˆä½]
    end
    
    style GPU fill:#ccffcc
    style TPU_BAD fill:#ffcccc
    style TPU_GOOD fill:#ccffcc
```

#### åŸå§‹ GPU ä»£ç  (attention.py)

```python
# GPU ä½¿ç”¨ flex_attention + score_mod
if text_mask is not None:
    attn_mask = F.pad(text_mask, (sequence_length, 0), value=True)

def score_mod(score, b, h, q_idx, kv_idx):
    return torch.where(attn_mask[b, q_idx] & attn_mask[b, kv_idx], score, float('-inf'))

hidden_states = flex_attention(query, key, value, score_mod=score_mod)
```

#### é”™è¯¯çš„ TPU ä»£ç 

```python
# âŒ é”™è¯¯ï¼šå®Œå…¨å¿½ç•¥ text_mask
attn_mask = None  # å¼ºåˆ¶ä½¿ç”¨ Splash Attentionï¼Œä½†æ²¡æœ‰ maskï¼
hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attn_mask)
```

#### ä¿®å¤åçš„ TPU ä»£ç 

```python
# âœ… æ­£ç¡®ï¼šå°† padding ä½ç½®çš„ K/V è®¾ä¸ºé›¶
if text_mask is not None:
    # text_mask: [B, text_len], 1=æœ‰æ•ˆ, 0=padding
    text_mask_expanded = text_mask.unsqueeze(-1).unsqueeze(-1).to(encoder_key.dtype)
    encoder_key = encoder_key * text_mask_expanded    # Padding ä½ç½® â†’ 0
    encoder_value = encoder_value * text_mask_expanded  # Padding ä½ç½® â†’ 0

# åˆå¹¶ image å’Œ text tokens
query = torch.cat([query, encoder_query], dim=1)
key = torch.cat([key, encoder_key], dim=1)     # text padding éƒ¨åˆ†æ˜¯ 0
value = torch.cat([value, encoder_value], dim=1)  # text padding éƒ¨åˆ†æ˜¯ 0

# Splash Attentionï¼ˆæ— éœ€æ˜¾å¼ maskï¼‰
hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=None)
```

#### ä¸ºä»€ä¹ˆ K/V ç½®é›¶æœ‰æ•ˆï¼Ÿ

```mermaid
flowchart LR
    subgraph æ•°å­¦åŸç†
        A["Q @ K^T"] --> B["å½“ K[i]=0 æ—¶<br/>score[i] â‰ˆ 0"]
        B --> C["Softmax å<br/>weight[i] å¾ˆå°"]
        C --> D["V[i] è´¡çŒ®<br/>æ¥è¿‘äº 0"]
    end
```

è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼æ–¹æ¡ˆï¼š
- ç²¾ç¡®æ–¹æ¡ˆï¼šå°† score è®¾ä¸º `-inf`ï¼Œsoftmax åæƒé‡ = 0
- è¿‘ä¼¼æ–¹æ¡ˆï¼šå°† K/V è®¾ä¸º 0ï¼Œscore â‰ˆ 0ï¼Œsoftmax åæƒé‡å¾ˆå°

è¿‘ä¼¼æ–¹æ¡ˆè¶³å¤Ÿæœ‰æ•ˆï¼Œå› ä¸º padding tokens çš„å½±å“è¢«å¤§å¹…é™ä½ã€‚

### 3.2 ByT5 Embeddings ç²¾åº¦ï¼ˆæœ€ç»ˆç»“è®ºï¼šbf16 å¯ç”¨ï¼‰

#### è°ƒè¯•å†ç¨‹

æœ€åˆæˆ‘ä»¬è®¤ä¸º ByT5 éœ€è¦ float32 ç²¾åº¦ï¼Œä½†åæ¥å‘ç°è¿™æ˜¯è¯¯åˆ¤ã€‚çœŸæ­£çš„é—®é¢˜æ˜¯ Attention Maskã€‚

```mermaid
flowchart TB
    subgraph è°ƒè¯•è¿‡ç¨‹
        A["è§†é¢‘è´¨é‡å·®"] --> B["å‡è®¾ï¼šByT5 ç²¾åº¦é—®é¢˜"]
        B --> C["ä¿®å¤1ï¼šByT5 æ”¹ float32"]
        C --> D["ä»æœ‰é—®é¢˜"]
        D --> E["çœŸæ­£åŸå› ï¼šAttention Mask"]
        E --> F["ä¿®å¤2ï¼šK/V ç½®é›¶"]
        F --> G["é—®é¢˜è§£å†³ï¼"]
        G --> H["éªŒè¯ï¼šByT5 æ”¹å› bf16"]
        H --> I["è´¨é‡æ­£å¸¸ âœ“"]
    end
```

#### æœ€ç»ˆç»“è®º

```python
# âœ… æ­£ç¡®ï¼šByT5 ä½¿ç”¨ bf16ï¼ˆTPU åŸç”Ÿä¼˜åŒ–ï¼Œç´¯åŠ å™¨ä¸º float32ï¼‰
prompt_embeds_2 = prompt_embeds_2.to(dtype=torch.bfloat16).to('jax')
```

> **TPU bf16 ç‰¹æ€§**ï¼šTPU çš„ MXUï¼ˆçŸ©é˜µä¹˜æ³•å•å…ƒï¼‰åŸç”Ÿæ”¯æŒ bf16ï¼Œå¹¶ä½¿ç”¨ float32 ç´¯åŠ å™¨ã€‚è¿™æ„å‘³ç€è®¡ç®—è¿‡ç¨‹ä¸­ç²¾åº¦å·²ç»å¾—åˆ°ä¿æŠ¤ï¼Œæ— éœ€æ˜¾å¼ä½¿ç”¨ float32ã€‚

### 3.3 vision_states å¤„ç†

#### é—®é¢˜åˆ†æ

```mermaid
flowchart LR
    subgraph GPU["GPU: t2v æ¨¡å¼"]
        G1["vision_states = zeros(...)"] --> G2["torch.all(x==0) æ£€æŸ¥"]
        G2 --> G3["extra_attention_mask = 0"]
    end
    
    subgraph TPU["TPU: t2v æ¨¡å¼"]
        T1["vision_states = None"] --> T2["è·³è¿‡ vision_in åˆ†æ”¯"]
        T2 --> T3["ç­‰æ•ˆæ•ˆæœ"]
    end
    
    style GPU fill:#ccffcc
    style TPU fill:#ccffcc
```

#### ä¸ºä»€ä¹ˆä½¿ç”¨ None è€Œéé›¶å‘é‡ï¼Ÿ

Transformer ä»£ç ä¸­æœ‰è¿™æ ·çš„æ£€æŸ¥ï¼š

```python
if mask_type == "t2v" and torch.all(vision_states == 0):
    ...
```

`torch.all()` åœ¨ JIT ç¼–è¯‘æ—¶ä¼šå¯¼è‡´ ConcretizationTypeErrorï¼Œå› ä¸ºå®ƒéœ€è¦å…·ä½“çš„å¸ƒå°”å€¼ã€‚ä½¿ç”¨ `None` å¯ä»¥å®Œå…¨è·³è¿‡è¿™ä¸ªåˆ†æ”¯ï¼Œé¿å…é—®é¢˜ã€‚

### 3.4 ä¿®å¤ #4: reorder_txt_token å¸ƒå°”ç´¢å¼•é—®é¢˜

#### é—®é¢˜åˆ†æ

GPU ç‰ˆæœ¬ä½¿ç”¨å¸ƒå°”ç´¢å¼•æ¥é‡æ’ tokensï¼Œè¿™åœ¨ torchax ä¸­ä¸æ”¯æŒã€‚

```python
# GPU ä»£ç ï¼šä½¿ç”¨å¸ƒå°”ç´¢å¼•
valid_tokens = tensor[mask.bool()]  # âŒ torchax ä¸æ”¯æŒ
```

#### è§£å†³æ–¹æ¡ˆï¼šargsort + gather

```python
def _reorder_txt_token_tpu_compatible(self, byt5_txt, txt, byt5_text_mask, text_mask, ...):
    """
    ä½¿ç”¨ argsort + gather æ›¿ä»£å¸ƒå°”ç´¢å¼•
    
    æ’åºé€»è¾‘ï¼š
    - priority = 2*(1-mask) + group
    - æœ‰æ•ˆ byt5: 0, æœ‰æ•ˆ text: 1, padding byt5: 2, padding text: 3
    """
    # åˆ›å»ºåˆ†ç»„æ ‡è¯†
    group = torch.cat([
        torch.zeros(B, byt5_len, ...),  # byt5 = 0
        torch.ones(B, text_len, ...)    # text = 1
    ], dim=1)
    
    # è®¡ç®—æ’åºä¼˜å…ˆçº§
    priority = 2 * (1 - combined_mask) + group
    
    # ä½¿ç”¨ argsort è·å–æ’åºç´¢å¼•
    sort_indices = torch.argsort(priority, dim=1, stable=True)
    
    # ä½¿ç”¨ gather é‡æ’
    sort_indices_expanded = sort_indices.unsqueeze(-1).expand_as(combined_txt).to(torch.int32)
    reorder_txt = torch.gather(combined_txt, dim=1, index=sort_indices_expanded)
    reorder_mask = torch.gather(combined_mask, dim=1, index=sort_indices.to(torch.int32))
    
    return reorder_txt, reorder_mask
```

### 3.5 ä¿®å¤ #5: Splash Attention segment_ids æœºåˆ¶ï¼ˆå®éªŒæ€§ï¼‰

> âš ï¸ **æ€§èƒ½è­¦å‘Š**ï¼šsegment_ids æ–¹æ¡ˆè™½ç„¶æ­£ç¡®ï¼Œä½†ä¼šå¢åŠ çº¦ 30% çš„å¼€é”€ã€‚æ¨èä½¿ç”¨ K/V ç½®é›¶æ–¹æ¡ˆã€‚

#### segment_ids æœºåˆ¶åŸç†

Splash Attention æ”¯æŒ `segment_ids` å‚æ•°ï¼Œç”¨äºåŒºåˆ†ä¸åŒåºåˆ—çš„ tokensï¼š
- åªæœ‰ç›¸åŒ segment_id çš„ tokens æ‰èƒ½äº’ç›¸ attend
- å¯ç”¨äºå¤„ç† packed sequences å’Œ padding mask

```mermaid
flowchart TB
    subgraph SegmentIds["Segment IDs æœºåˆ¶"]
        S1["img tokens: segment_id = 0"]
        S2["valid txt tokens: segment_id = 0"]
        S3["padding txt tokens: segment_id = 1"]
        S4["åªæœ‰ segment_id ç›¸åŒæ‰èƒ½ attend"]
    end
```

#### å…³é”®é—®é¢˜ï¼šbatch ç»´åº¦

CFG æ¨¡å¼ä¸‹ batch_size=2ï¼ˆnegative + positive promptï¼‰ï¼Œå®ƒä»¬å¯èƒ½æœ‰ä¸åŒçš„ padding patternã€‚**segment_ids å¿…é¡»æœ‰ batch ç»´åº¦ï¼**

```python
# âŒ é”™è¯¯ï¼š1D segment_idsï¼ˆæ‰€æœ‰ batch å…±äº«ï¼‰
txt_segment = (1 - text_mask[0]).to(torch.int32)  # åªç”¨äº† batch[0]
segment_ids = torch.cat([img_segment, txt_segment], dim=0)  # [total_len]

# âœ… æ­£ç¡®ï¼š2D segment_idsï¼ˆper-batchï¼‰
img_segment = torch.zeros(batch_size, img_q_len, ...)  # [B, img_len]
txt_segment = (1 - text_mask).to(torch.int32)          # [B, txt_len]
segment_ids = torch.cat([img_segment, txt_segment], dim=1)  # [B, total_len]
```

#### vmap é€‚é…

segment_ids ä» 1D æ”¹ä¸º 2D åï¼Œéœ€è¦åœ¨ vmap ä¸­æ­£ç¡®å¤„ç†ï¼š

```python
# 1D segment_idsï¼šåœ¨ vmap å¤–éƒ¨ä¼ å…¥ï¼ˆä½œä¸ºå¸¸é‡å¹¿æ’­ï¼‰
vmapped_kernel = jax.vmap(
    lambda q, k, v: kernel_3d(q, k, v, seg_ids),  # seg_ids æ˜¯å¸¸é‡
    in_axes=(0, 0, 0), out_axes=0
)

# 2D segment_idsï¼šåœ¨ vmap å†…éƒ¨æŒ‰ batch ç´¢å¼•
vmapped_kernel = jax.vmap(
    kernel_3d,
    in_axes=(0, 0, 0, 0), out_axes=0  # seg_ids ä¹ŸæŒ‰ batch åˆ†
)
```

#### æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | 121å¸§ 50æ­¥ | æ¯æ­¥æ—¶é—´ | å¼€é”€ |
|------|-----------|---------|------|
| K/V ç½®é›¶ | ~350s | ~7.0s | åŸºå‡† |
| segment_ids (2D) | ~435s | ~8.7s | +24% |

**å¼€é”€åŸå› **ï¼š
1. Splash Attention å†…æ ¸çº§åˆ«çš„é¢å¤–æ£€æŸ¥ï¼ˆæ¯ä¸ª block æ£€æŸ¥ segment_idï¼‰
2. 2D segment_ids çš„ vmap å¼€é”€ï¼ˆæ¯ä¸ª batch ç‹¬ç«‹å¤„ç†ï¼‰

#### ç»“è®º

- **æ¨è**ï¼šK/V ç½®é›¶æ–¹æ¡ˆï¼ˆæ€§èƒ½æ›´å¥½ï¼Œä»£ç æ›´ç®€å•ï¼‰
- **å¤‡é€‰**ï¼šsegment_ids æ–¹æ¡ˆï¼ˆè¯­ä¹‰æ›´æ¸…æ™°ï¼Œä½†æ€§èƒ½å·®ï¼‰
- **å®éªŒç‰ˆæœ¬**ï¼šä¿å­˜åœ¨ `stage2_transformer_flax_experimental_segmented.py`

---

## 4. å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ

### 4.1 ConcretizationTypeError

```mermaid
flowchart TB
    A["JIT ç¼–è¯‘æ¨¡å‹"] --> B{"é‡åˆ°æ¡ä»¶åˆ¤æ–­?"}
    B -->|"if tensor.max() > 1"| C["ConcretizationTypeError!"]
    B -->|"if static_arg == 'value'"| D["æ­£å¸¸ç¼–è¯‘"]
    C --> E["è§£å†³æ–¹æ¡ˆ"]
    E --> E1["1. é¢„è®¡ç®—ç§»åˆ° JIT å¤–"]
    E --> E2["2. Monkey-patch ç§»é™¤æ£€æŸ¥"]
    E --> E3["3. ä½¿ç”¨ static_argnames"]
```

**å¸¸è§è§¦å‘åœºæ™¯ï¼š**

| ä»£ç æ¨¡å¼ | é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|----------|------|----------|
| `if tensor.max() > 1:` | éœ€è¦å…·ä½“å€¼ | ç§»åˆ° JIT å¤–æˆ–ç§»é™¤ |
| `assert tensor.min() >= 0` | æ–­è¨€éœ€è¦å…·ä½“å€¼ | Monkey-patch ç§»é™¤ |
| `torch.all(x == 0)` | éœ€è¦å…·ä½“å¸ƒå°”å€¼ | ä¼ å…¥ None è·³è¿‡åˆ†æ”¯ |
| `tensor.item()` | éœ€è¦æ ‡é‡å€¼ | ä½¿ç”¨ tensor è¿ç®—ä»£æ›¿ |

### 4.2 å¸ƒå°”ç´¢å¼•ä¸æ”¯æŒ

```mermaid
flowchart LR
    A["tensor[bool_mask]"] --> B["torchax ä¸æ”¯æŒ!"]
    B --> C["è§£å†³æ–¹æ¡ˆ"]
    C --> C1["torch.where()"]
    C --> C2["* mask ä¹˜æ³•"]
    C --> C3["ç®€åŒ–é€»è¾‘é¿å…"]
```

**ç¤ºä¾‹ä¿®å¤ï¼š**

```python
# âŒ é”™è¯¯
selected = tensor[~mask]

# âœ… æ–¹æ¡ˆ 1: torch.where
selected = torch.where(mask.unsqueeze(-1), tensor, torch.zeros_like(tensor))

# âœ… æ–¹æ¡ˆ 2: ä¹˜æ³•
selected = tensor * mask.unsqueeze(-1).float()
```

### 4.3 åŠ¨æ€ Tensor åˆ›å»º

```mermaid
flowchart TB
    A["JIT å†…éƒ¨è°ƒç”¨<br/>torch.arange()"] --> B["æ¯æ¬¡é‡æ–°ç¼–è¯‘!"]
    B --> C["æ€§èƒ½æå·®"]
    C --> D["è§£å†³æ–¹æ¡ˆï¼šé¢„è®¡ç®—"]
    D --> E["åœ¨ JIT å¤–è®¡ç®—ä¸€æ¬¡"]
    E --> F["ç¼“å­˜åˆ°æ¨¡å‹å±æ€§"]
    F --> G["JIT å†…ä½¿ç”¨ç¼“å­˜"]
```

**ç¤ºä¾‹ï¼šRotary Position Embeddings**

```python
# åœ¨ JIT ç¼–è¯‘å‰é¢„è®¡ç®—
with torch.no_grad():
    freqs_cos, freqs_sin = model.get_rotary_pos_embed((t, h, w))
    with env:
        model._cached_freqs_cos = freqs_cos.to('jax')
        model._cached_freqs_sin = freqs_sin.to('jax')

# Monkey-patch ä½¿ç”¨ç¼“å­˜
def cached_get_rotary_pos_embed(self, latent_size):
    return self._cached_freqs_cos, self._cached_freqs_sin
model.get_rotary_pos_embed = types.MethodType(cached_get_rotary_pos_embed, model)
```

### 4.4 Scheduler dtype é—®é¢˜

```mermaid
flowchart LR
    A["latents<br/>bf16"] --> B["scheduler.step()"]
    B --> C["å†…éƒ¨è½¬ fp32<br/>ç²¾åº¦ä¿æŠ¤"]
    C --> D["è¾“å‡º fp32"]
    D --> E["éœ€è¦è½¬å› bf16!"]
    E --> F["latents.to(bf16)"]
```

```python
# æ¯æ¬¡ scheduler.step åè½¬å› bf16
latents = scheduler.step(noise_pred, t, latents)[0]
latents = latents.to(target_dtype)  # è½¬å› bf16
```

### 4.5 OOM é—®é¢˜

```mermaid
flowchart TB
    A["åˆ›å»º attention mask<br/>[B, H, S, S]"] --> B{"S = 26456?"}
    B -->|Yes| C["çŸ©é˜µå¤ªå¤§<br/>26456 x 26456 x 2 x 64"]
    C --> D["OOM!"]
    B -->|No| E["æ­£å¸¸"]
    
    D --> F["è§£å†³æ–¹æ¡ˆ"]
    F --> F1["ä½¿ç”¨ Splash Attention<br/>åˆ†å—è®¡ç®—"]
    F --> F2["ä¸åˆ›å»ºå®Œæ•´ mask<br/>K/V ç½®é›¶è¿‘ä¼¼"]
```

---

## 5. å®Œæ•´è¿ç§»æµç¨‹

```mermaid
flowchart TB
    subgraph PREP["ğŸ“‹ å‡†å¤‡é˜¶æ®µ"]
        P1[åˆ†æ GPU ä»£ç ] --> P2[è¯†åˆ«ä¸å…¼å®¹æ¨¡å¼]
        P2 --> P3[è®¾è®¡è§£å†³æ–¹æ¡ˆ]
    end
    
    subgraph SETUP["âš™ï¸ ç¯å¢ƒè®¾ç½®"]
        S1[åˆ›å»º JAX Mesh] --> S2[åˆ›å»º torchax ç¯å¢ƒ]
        S2 --> S3[æ³¨å†Œ Splash Attention]
    end
    
    subgraph PATCH["ğŸ”§ ä»£ç é€‚é…"]
        M1[Mock GPU åˆ†å¸ƒå¼çŠ¶æ€] --> M2[Patch ä¸å…¼å®¹å‡½æ•°]
        M2 --> M3[å¯¼å…¥æ¨¡å‹]
    end
    
    subgraph LOAD["ğŸ“¦ æ¨¡å‹åŠ è½½"]
        L1[åŠ è½½æ¨¡å‹] --> L2[è½¬æ¢æƒé‡åˆ° XLA]
        L2 --> L3[æƒé‡åˆ†ç‰‡]
        L3 --> L4[é¢„è®¡ç®—åŠ¨æ€ Tensor]
    end
    
    subgraph COMPILE["ğŸš€ ç¼–è¯‘è¿è¡Œ"]
        C1[JIT ç¼–è¯‘] --> C2[æ¨ç†å¾ªç¯]
        C2 --> C3[ä¿å­˜ç»“æœ]
        C3 --> C4[æ˜¾å¼é€€å‡º]
    end
    
    PREP --> SETUP --> PATCH --> LOAD --> COMPILE
```

### æ­¥éª¤ 1: åˆ›å»º JAX Mesh

```python
from jax.sharding import Mesh
from jax.experimental import mesh_utils

tp_dim = jax.device_count()  # 8 ä¸ª TPU cores
dp_dim = 1
sp_dim = 1

mesh_devices = mesh_utils.create_device_mesh(
    (tp_dim, dp_dim, sp_dim),
    allow_split_physical_axes=True
)
mesh = Mesh(mesh_devices, ('tp', 'dp', 'sp'))
```

### æ­¥éª¤ 2: åˆ›å»º torchax ç¯å¢ƒ

```python
import torchax

env = torchax.default_env()
env._mesh = mesh
env.config.use_tpu_splash_attention = True

torch.set_default_dtype(torch.bfloat16)
```

### æ­¥éª¤ 3: æ³¨å†Œ Splash Attention

```python
# ä¿å­˜åŸå§‹ SDPA
_ORIGINAL_SDPA = torch.nn.functional.scaled_dot_product_attention

# æ³¨å†Œè‡ªå®šä¹‰ attention
custom_attention = functools.partial(scaled_dot_product_attention, env=env)
env._ops[torch.nn.functional.scaled_dot_product_attention] = ops_registry.Operator(
    torch.nn.functional.scaled_dot_product_attention,
    custom_attention,
    is_jax_function=False,
    is_user_defined=True,
    needs_env=False,
    is_view_op=False,
)
```

### æ­¥éª¤ 4: Monkey-Patch ä¸å…¼å®¹ä»£ç 

**å¿…é¡»åœ¨å¯¼å…¥æ¨¡å‹ä¹‹å‰ï¼**

```python
# Mock GPU åˆ†å¸ƒå¼çŠ¶æ€
import module.parallel_states as ps
from types import SimpleNamespace

ps.get_parallel_state = lambda: SimpleNamespace(
    sp=1,
    sp_enabled=False,
    sp_group=None,
)

# Patch æœ‰é—®é¢˜çš„å‡½æ•°
def patched_function(...):
    # ç§»é™¤è¿è¡Œæ—¶æ£€æŸ¥ï¼Œç®€åŒ–é€»è¾‘
    pass
module.original_function = patched_function

# ç°åœ¨æ‰å¯¼å…¥æ¨¡å‹
from module.model import Model
```

### æ­¥éª¤ 5: åŠ è½½å’Œè½¬æ¢æ¨¡å‹

```python
model = Model.from_pretrained(path, torch_dtype=torch.bfloat16)

with env:
    with jax.default_device('cpu'):
        state_dict = model.state_dict()
        state_dict = env.to_xla(state_dict)
        model.load_state_dict(state_dict, assign=True)
    
    weights = shard_weights(mesh, model.state_dict())
    model.load_state_dict(weights, assign=True, strict=False)
    torchax.interop.call_jax(jax.block_until_ready, weights)

model.eval()
```

### æ­¥éª¤ 6: é¢„è®¡ç®—å¹¶ JIT ç¼–è¯‘

```python
# é¢„è®¡ç®—åŠ¨æ€ tensor
with torch.no_grad():
    freqs = model.get_rotary_pos_embed(size)
    with env:
        model._cached_freqs = freqs.to('jax')

# JIT ç¼–è¯‘
with env:
    model = torchax.compile(model, torchax.CompileOptions(
        jax_jit_kwargs={'static_argnames': ('return_dict',)}
    ))
```

### æ­¥éª¤ 7: æ¨ç†å¾ªç¯

```python
with mesh, env:
    with torch.no_grad():
        for i, t in enumerate(timesteps):
            output = model(inputs)
            latents = scheduler.step(output, t, latents)[0]
            latents = latents.to(target_dtype)  # è½¬å› bf16

# ä¿å­˜ç»“æœ
save_results(latents.cpu())

# å¼ºåˆ¶é€€å‡ºï¼ˆé¿å… torchax/JAX åå°çº¿ç¨‹é˜»å¡ï¼‰
os._exit(0)  # ä¸è¦ç”¨ sys.exit(0)
```

> âš ï¸ **é‡è¦**ï¼šä½¿ç”¨ `os._exit(0)` è€Œé `sys.exit(0)`ã€‚torchax/JAX æœ‰åå°çº¿ç¨‹å¯èƒ½å¯¼è‡´ `sys.exit(0)` é˜»å¡ã€‚

---

## 6. ä»£ç æ¨¡æ¿

### 6.1 Splash Attention å®Œæ•´å®ç°

```python
from jax.experimental.pallas.ops.tpu import splash_attention
from jax.experimental.shard_map import shard_map

BQSIZE = 2048
BKVSIZE = 2048
BKVCOMPUTESIZE = 1024

def _tpu_splash_attention(query, key, value, mesh, scale=None, window_size=None):
    """
    TPU Splash Attention å®ç°
    
    Args:
        query: [B, H, Sq, D]
        key: [B, H, Skv, D]
        value: [B, H, Skv, D]
        mesh: JAX è®¾å¤‡ mesh
        scale: ç¼©æ”¾å› å­ï¼Œé»˜è®¤ 1/sqrt(D)
        window_size: å±€éƒ¨æ³¨æ„åŠ›çª—å£å¤§å°ï¼ŒNone è¡¨ç¤ºå…¨å±€æ³¨æ„åŠ›
    """
    num_heads = query.shape[1]

    def _attention_on_slices(q, k, v):
        scale_factor = 1.0 / math.sqrt(q.shape[-1]) if scale is None else scale
        q = q * scale_factor

        def pad_to_multiple(x, multiple, axis):
            seq_len = x.shape[axis]
            pad_len = (multiple - seq_len % multiple) % multiple
            if pad_len == 0:
                return x, seq_len
            pad_width = [(0, 0)] * x.ndim
            pad_width[axis] = (0, pad_len)
            return jnp.pad(x, pad_width), seq_len

        def kernel_3d(q_3d, k_3d, v_3d):
            num_heads_on_device = q_3d.shape[0]
            
            q_3d_padded, q_orig_len = pad_to_multiple(q_3d, BQSIZE, axis=1)
            k_3d_padded, _ = pad_to_multiple(k_3d, BKVSIZE, axis=1)
            v_3d_padded, _ = pad_to_multiple(v_3d, BKVSIZE, axis=1)

            if window_size is not None:
                mask_class = functools.partial(
                    splash_attention.LocalMask, 
                    window_size=window_size
                )
            else:
                mask_class = splash_attention.FullMask

            mask = splash_attention.MultiHeadMask([
                mask_class((q_3d_padded.shape[1], k_3d_padded.shape[1]))
                for _ in range(num_heads_on_device)
            ])

            block_sizes = splash_attention.BlockSizes(
                block_q=min(BQSIZE, q_3d_padded.shape[1]),
                block_kv=min(BKVSIZE, k_3d_padded.shape[1]),
                block_kv_compute=min(BKVCOMPUTESIZE, k_3d_padded.shape[1]),
            )
            
            kernel = splash_attention.make_splash_mha(
                mask=mask, block_sizes=block_sizes
            )
            out = kernel(q_3d_padded, k_3d_padded, v_3d_padded)
            return out[:, :q_orig_len, ...]

        return jax.vmap(kernel_3d)(q, k, v)

    # åˆ†ç‰‡è§„åˆ™
    q_spec = P('dp', 'tp', 'sp', None)
    kv_spec = P('dp', 'tp', None, None)

    sharded_fn = shard_map(
        _attention_on_slices,
        mesh=mesh,
        in_specs=(q_spec, kv_spec, kv_spec),
        out_specs=q_spec,
        check_rep=False,
    )
    return sharded_fn(query, key, value)
```

### 6.2 æƒé‡åˆ†ç‰‡æ¨¡æ¿

```python
from jax.sharding import PartitionSpec as P, NamedSharding
import re

# Tensor Parallel: Column-Row æ¨¡å¼
sharding_rules = {
    # Column Parallel: åœ¨ output ç»´åº¦åˆ†ç‰‡
    r'.*\.q_proj\.weight$': (('tp', 'sp'), None),
    r'.*\.k_proj\.weight$': (('tp', 'sp'), None),
    r'.*\.v_proj\.weight$': (('tp', 'sp'), None),
    r'.*\.fc1\.weight$': (('tp', 'sp'), None),
    
    # Row Parallel: åœ¨ input ç»´åº¦åˆ†ç‰‡
    r'.*\.o_proj\.weight$': (None, ('tp', 'sp')),
    r'.*\.fc2\.weight$': (None, ('tp', 'sp')),
}

def shard_weights(mesh, weights, rules):
    matched = 0
    for name, tensor in weights.items():
        for pattern, spec in rules.items():
            if re.fullmatch(pattern, name):
                tensor.apply_jax_(jax.device_put, NamedSharding(mesh, P(*spec)))
                matched += 1
                break
        else:
            # æœªåŒ¹é…ï¼šå¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡
            tensor.apply_jax_(jax.device_put, NamedSharding(mesh, P()))
    
    print(f"åˆ†ç‰‡å®Œæˆ: {matched} ä¸ªåŒ¹é…, {len(weights)-matched} ä¸ªå¤åˆ¶")
    return weights
```

---

## 7. æ€§èƒ½ä¼˜åŒ–

### 7.1 Warmupï¼ˆé¢„çƒ­ï¼‰ç­–ç•¥

XLA ç¼–è¯‘æ˜¯ä¸¤é˜¶æ®µçš„ï¼šé¦–å…ˆ trace è®¡ç®—å›¾ï¼Œç„¶åç¼–è¯‘åˆ° TPU å†…æ ¸ã€‚å‰ 1-2 æ­¥ä¼šæ¯”è¾ƒæ…¢ã€‚

```mermaid
flowchart LR
    subgraph ç¬¬ä¸€æ­¥
        A1["Trace è®¡ç®—å›¾"] --> B1["XLA ç¼–è¯‘"]
        B1 --> C1["æ‰§è¡Œ"]
    end
    
    subgraph ç¬¬äºŒæ­¥
        A2["é‡ç”¨å·²ç¼–è¯‘å†…æ ¸"] --> B2["æ‰§è¡Œ"]
        style A2 fill:#ccffcc
    end
    
    subgraph åç»­æ­¥éª¤
        A3["ç¨³å®šæ‰§è¡Œ"] --> B3["~8s/step"]
        style A3 fill:#ccffcc
    end
```

#### æ¨èï¼š2 æ­¥é¢„çƒ­

```python
parser.add_argument('--warmup_steps', type=int, default=2,
                    help='é¢„çƒ­æ­¥æ•°ï¼ˆ0=ä¸é¢„çƒ­ï¼Œ2=æ¨èï¼Œè§¦å‘ JIT ç¼–è¯‘ï¼‰')

if args.warmup_steps > 0:
    _, warmup_times, warmup_elapsed = run_denoising_loop(
        latents_input=latents,
        timesteps_input=timesteps,
        num_steps=args.warmup_steps,
        desc="Warmup (JIT)",
        is_warmup=True,
    )
    print(f"é¢„çƒ­å®Œæˆï¼Œè€—æ—¶: {warmup_elapsed:.2f}ç§’")
```

#### ç»Ÿä¸€ Warmup å’Œæ¨ç†ä»£ç 

æ¨èå°† warmup å’Œæ¨ç†ä½¿ç”¨åŒä¸€ä¸ªå‡½æ•°ï¼Œé¿å…ä»£ç é‡å¤ï¼š

```python
def run_denoising_loop(
    latents_input,
    timesteps_input,
    num_steps,
    desc="Denoising",
    is_warmup=False,
):
    """ç»Ÿä¸€çš„ Denoising å¾ªç¯ï¼Œé¢„çƒ­å’Œæ­£å¼æ¨ç†å…±ç”¨åŒä¸€å¥—ä»£ç ã€‚"""
    step_times = []
    start_time = time.perf_counter()
    
    with mesh, env:
        # clone å¿…é¡»åœ¨ torchax ç¯å¢ƒå†…æ‰§è¡Œ
        loop_latents = latents_input.clone() if is_warmup else latents_input
        with torch.no_grad():
            for i in tqdm(range(num_steps), desc=desc):
                # ... forward pass ...
                
                # ç­‰å¾…è®¡ç®—å®Œæˆï¼ˆå‡†ç¡®è®¡æ—¶ï¼‰
                torchax.interop.call_jax(jax.block_until_ready, loop_latents._elem)
                
                step_time = time.perf_counter() - step_start
                step_times.append(step_time)
    
    return loop_latents, step_times, time.perf_counter() - start_time
```

### 7.2 JIT ç¼–è¯‘ç¼“å­˜

```python
# å¯ç”¨æŒä¹…åŒ–ç¼“å­˜
jax.config.update("jax_compilation_cache_dir", "/dev/shm/jax_cache")
jax.config.update("jax_persistent_cache_min_entry_size_bytes", -1)
jax.config.update("jax_persistent_cache_min_compile_time_secs", 0)
```

**æ•ˆæœï¼š**
- é¦–æ¬¡è¿è¡Œï¼š~60s ç¼–è¯‘
- åç»­è¿è¡Œï¼š~5s åŠ è½½ç¼“å­˜ï¼ˆéœ€è¦ç›¸åŒçš„æ¨¡å‹å’Œè¾“å…¥å½¢çŠ¶ï¼‰

### 7.3 dtype ä¼˜åŒ–

```mermaid
flowchart LR
    A["æ¨¡å‹æƒé‡<br/>bf16"] --> B["ä¸­é—´è®¡ç®—<br/>bf16"]
    B --> C["æ³¨æ„åŠ›<br/>bf16"]
    C --> D["è¾“å‡º<br/>bf16"]
    
    E["ç‰¹æ®Šæƒ…å†µ"] --> E1["ByT5: float32<br/>ç²¾åº¦æ•æ„Ÿ"]
    E --> E2["Scheduler: float32<br/>ç´¯åŠ ç²¾åº¦"]
```

### 7.4 å‡†ç¡®è®¡æ—¶

JAX/XLA æ˜¯æƒ°æ€§æ‰§è¡Œçš„ï¼Œå¿…é¡»ä½¿ç”¨ `block_until_ready` æ‰èƒ½è·å¾—å‡†ç¡®çš„è®¡æ—¶ï¼š

```python
# âŒ é”™è¯¯ï¼šä¸å‡†ç¡®çš„è®¡æ—¶
step_start = time.perf_counter()
output = model(input)
step_time = time.perf_counter() - step_start  # å¯èƒ½åªæµ‹é‡äº† dispatch æ—¶é—´

# âœ… æ­£ç¡®ï¼šå‡†ç¡®è®¡æ—¶
step_start = time.perf_counter()
output = model(input)
torchax.interop.call_jax(jax.block_until_ready, output._elem)  # ç­‰å¾…è®¡ç®—å®Œæˆ
step_time = time.perf_counter() - step_start  # åŒ…å«å®é™…è®¡ç®—æ—¶é—´
```

### 7.5 æ€§èƒ½åŸºå‡†

| é…ç½® | Token æ•° | æ€»æ—¶é—´ | æ¯æ­¥æ—¶é—´ |
|------|----------|--------|----------|
| 25å¸§, 720p | 25,200 | ~115s | ~2.3s |
| 49å¸§, 720p | 46,800 | ~215s | ~4.3s |
| 121å¸§, 720p | 111,600 | ~406s | ~8.1s |

> æµ‹è¯•ç¯å¢ƒï¼šTPU v4-8ï¼Œ50 æ­¥æ¨ç†ï¼Œ2 æ­¥é¢„çƒ­

---

## 8. ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–

### 8.1 è­¦å‘Šè¿‡æ»¤

torchax/JAX ä¼šäº§ç”Ÿè®¸å¤šæ— å®³è­¦å‘Šï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿‡æ»¤ï¼š

```python
import warnings

# è¿‡æ»¤æ‰å„ç§æ— å®³è­¦å‘Š
warnings.filterwarnings('ignore', message='.*jax.experimental.shard_map is deprecated.*')
warnings.filterwarnings('ignore', message='.*NumPy array is not writable.*')
# int64 æˆªæ–­è­¦å‘Šæ¥è‡ª HunyuanVideo-1.5-TPU åº“ä»£ç ï¼Œæ— æ³•ä¿®æ”¹
warnings.filterwarnings('ignore', message='.*int64.*is not available.*')
# è¿‡æ»¤ flash attention fallback è­¦å‘Šï¼ˆæˆ‘ä»¬ç”¨ Splash Attention æ›¿ä»£ï¼‰
warnings.filterwarnings('ignore', message='.*Falling back from.*')
```

### 8.2 è¿›ç¨‹é€€å‡º

torchax/JAX æœ‰åå°çº¿ç¨‹ï¼Œ`sys.exit(0)` å¯èƒ½ä¼šé˜»å¡ã€‚æ¨èä½¿ç”¨å¼ºåˆ¶é€€å‡ºï¼š

```python
import os

# âŒ å¯èƒ½é˜»å¡
sys.exit(0)

# âœ… å¼ºåˆ¶é€€å‡º
os._exit(0)
```

### 8.3 è¿›åº¦æ¡ä¼˜åŒ–

æ˜¾ç¤ºæ¯æ­¥æ—¶é—´å’Œ ETAï¼š

```python
from tqdm import tqdm

progress_bar = tqdm(range(num_steps), desc="Denoising", ncols=130)

for i in progress_bar:
    step_start = time.perf_counter()
    
    # ... forward pass ...
    
    # ç­‰å¾…è®¡ç®—å®Œæˆ
    torchax.interop.call_jax(jax.block_until_ready, latents._elem)
    
    step_time = time.perf_counter() - step_start
    step_times.append(step_time)
    avg_time = sum(step_times) / len(step_times)
    remaining = num_steps - i - 1
    
    progress_bar.set_postfix({
        'step': f'{step_time:.2f}s',
        'avg': f'{avg_time:.2f}s',
        'eta': f'{avg_time * remaining:.1f}s'
    })
```

---

## 9. è°ƒè¯•æŠ€å·§

### 9.1 æŸ¥çœ‹å®Œæ•´ traceback

```bash
JAX_TRACEBACK_FILTERING=off python script.py
```

### 9.2 é€æ­¥æµ‹è¯•

```python
# å…ˆç”¨ 1 æ­¥æµ‹è¯•
args.num_inference_steps = 1
# æˆåŠŸåå†å¢åŠ 
```

### 9.3 æ£€æµ‹ XLA tensor

```python
def is_xla_tensor(tensor):
    if tensor is None:
        return False
    if hasattr(tensor, '_elem'):
        return True
    if hasattr(tensor, 'device'):
        return 'jax' in str(tensor.device) or 'xla' in str(tensor.device)
    return False
```

### 9.4 è°ƒè¯•æ‰“å°

```python
def debug_tensor(name, t):
    if t is None:
        print(f"{name}: None")
    else:
        print(f"{name}: shape={t.shape}, dtype={t.dtype}, "
              f"mean={t.float().mean().item():.4f}")
```

---

## ğŸ“‹ è¿ç§» Checklist

### å¼€å§‹å‰

- [ ] è¯†åˆ«æ‰€æœ‰ CUDA ç‰¹å®šä»£ç 
- [ ] è¯†åˆ«æ‰€æœ‰è¿è¡Œæ—¶æ£€æŸ¥ (assert, if tensor.max())
- [ ] è¯†åˆ«æ‰€æœ‰åŠ¨æ€ tensor åˆ›å»º (torch.arange, torch.zeros)
- [ ] è¯†åˆ«æ‰€æœ‰å¸ƒå°”ç´¢å¼•
- [ ] ç¡®è®¤ dtype è¦æ±‚

### è¿ç§»ä¸­

- [ ] åˆ›å»º JAX Mesh
- [ ] æ³¨å†Œ Splash Attention
- [ ] Monkey-patch ä¸å…¼å®¹ä»£ç 
- [ ] åŠ è½½å¹¶åˆ†ç‰‡æƒé‡
- [ ] é¢„è®¡ç®—åŠ¨æ€ tensor
- [ ] JIT ç¼–è¯‘æ¨¡å‹

### å®Œæˆå

- [ ] ç¨‹åºæ­£å¸¸é€€å‡º
- [ ] è¾“å‡º dtype æ­£ç¡® (bf16)
- [ ] æ—  OOM é—®é¢˜
- [ ] ç”Ÿæˆè´¨é‡æ­£ç¡®

---

## ğŸ“‹ é—®é¢˜æ’æŸ¥é€ŸæŸ¥è¡¨

| ç—‡çŠ¶ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|
| è§†é¢‘æœ‰ç«–æ¡çº¹/å™ªç‚¹ | Attention Mask æœªå¤„ç† | ä½¿ç”¨ K/V ç½®é›¶æ–¹æ¡ˆ |
| è§†é¢‘ä¸è·Ÿéšæç¤ºè¯ | Attention Mask æœªå¤„ç† | ä½¿ç”¨ K/V ç½®é›¶æ–¹æ¡ˆ |
| è§†é¢‘èŠ±å±ï¼ˆä½¿ç”¨ segment_idsï¼‰ | segment_ids ç¼ºå°‘ batch ç»´åº¦ | æ”¹ä¸º [B, total_len] å½¢çŠ¶ |
| ç¨‹åºä¸é€€å‡º | JAX åå°çº¿ç¨‹ | ä½¿ç”¨ `os._exit(0)` |
| ç¬¬ä¸€æ­¥å¾ˆæ…¢ï¼ˆ60s+ï¼‰ | XLA ç¼–è¯‘ | æ­£å¸¸ç°è±¡ï¼Œä½¿ç”¨ warmup |
| ç¬¬äºŒæ­¥ä»æ…¢ | XLA å¼‚æ­¥ç¼–è¯‘ | æ­£å¸¸ç°è±¡ï¼Œç¬¬ä¸‰æ­¥å¼€å§‹ç¨³å®š |
| å•æ­¥æ—¶é—´ä» 7s å˜ 8.7s | segment_ids å†…æ ¸å¼€é”€ | æ”¹ç”¨ K/V ç½®é›¶æ–¹æ¡ˆ |
| OOM | å®Œæ•´ attention mask | ä½¿ç”¨ Splash Attention + K/V ç½®é›¶ |
| ConcretizationTypeError | åŠ¨æ€æ¡ä»¶/æ–­è¨€ | Monkey-patch ç§»é™¤æˆ–é¢„è®¡ç®— |
| å¸ƒå°”ç´¢å¼•æŠ¥é”™ | torchax ä¸æ”¯æŒ `tensor[mask]` | ä½¿ç”¨ argsort + gather |

---

## ğŸ“š å‚è€ƒèµ„æº

- [torchax GitHub](https://github.com/pytorch/xla)
- [JAX Splash Attention](https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention)
- [JAX shard_map](https://jax.readthedocs.io/en/latest/notebooks/shard_map.html)
- [HunyuanVideo-1.5](https://github.com/Tencent/HunyuanVideo)
- [TPU bf16 ç²¾åº¦è¯´æ˜](https://cloud.google.com/tpu/docs/bfloat16)