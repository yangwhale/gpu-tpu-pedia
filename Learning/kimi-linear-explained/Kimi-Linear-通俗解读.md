# Kimi Linear: ä¸€ç§è¡¨è¾¾åŠ›å¼ºã€æ•ˆç‡é«˜çš„æ³¨æ„åŠ›æ¶æ„

> **è®ºæ–‡æ ‡é¢˜**: Kimi Linear: An Expressive, Efficient Attention Architecture  
> **å‘è¡¨ä¿¡æ¯**: arXiv:2510.26692, 2025å¹´10æœˆ  
> **ä½œè€…å›¢é˜Ÿ**: Kimi Team (Moonshot AI)  
> **ä¸€å¥è¯æ€»ç»“**: Kimi å›¢é˜Ÿæå‡ºçš„æ··åˆçº¿æ€§æ³¨æ„åŠ›æ¶æ„ï¼Œé¦–æ¬¡åœ¨å…¬å¹³å¯¹æ¯”ä¸‹å…¨é¢è¶…è¶Šå…¨æ³¨æ„åŠ›ï¼ŒåŒæ—¶å®ç° 6 å€è§£ç åŠ é€Ÿå’Œ 75% KV cache èŠ‚çœ

---

## ğŸ“– ç›®å½•

- [è¿™ç¯‡è®ºæ–‡åœ¨è®²ä»€ä¹ˆï¼Ÿ](#è¿™ç¯‡è®ºæ–‡åœ¨è®²ä»€ä¹ˆ)
- [ä¸ºä»€ä¹ˆè¿™ç¯‡è®ºæ–‡å¾ˆé‡è¦ï¼Ÿ](#ä¸ºä»€ä¹ˆè¿™ç¯‡è®ºæ–‡å¾ˆé‡è¦)
- [æ ¸å¿ƒæ–¹æ³•è¯¦è§£](#æ ¸å¿ƒæ–¹æ³•è¯¦è§£)
  - [ä»€ä¹ˆæ˜¯çº¿æ€§æ³¨æ„åŠ›ï¼Ÿ](#ä»€ä¹ˆæ˜¯çº¿æ€§æ³¨æ„åŠ›)
  - [Delta Ruleï¼šè®©æ¨¡å‹å­¦ä¼šçº é”™](#delta-ruleè®©æ¨¡å‹å­¦ä¼šçº é”™)
  - [KDAï¼šç»†ç²’åº¦çš„è®°å¿†æ§åˆ¶](#kdaç»†ç²’åº¦çš„è®°å¿†æ§åˆ¶)
  - [æ··åˆæ¶æ„ï¼š3:1 çš„é»„é‡‘æ¯”ä¾‹](#æ··åˆæ¶æ„31-çš„é»„é‡‘æ¯”ä¾‹)
- [å®éªŒç»“æœ](#å®éªŒç»“æœ)
- [ç¤ºä¾‹ä»£ç ](#ç¤ºä¾‹ä»£ç )
- [å±€é™æ€§ä¸æœªæ¥å·¥ä½œ](#å±€é™æ€§ä¸æœªæ¥å·¥ä½œ)
- [æ€»ç»“ä¸æ€è€ƒ](#æ€»ç»“ä¸æ€è€ƒ)
- [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

---

## è¿™ç¯‡è®ºæ–‡åœ¨è®²ä»€ä¹ˆï¼Ÿ

æƒ³è±¡ä½ åœ¨å›¾ä¹¦é¦†æ‰¾ä¸€æœ¬ä¹¦ã€‚æ ‡å‡†çš„ Transformer æ³¨æ„åŠ›æœºåˆ¶å°±åƒä¸€ä¸ª**è¶…çº§è®°å¿†åŠ›çš„å›¾ä¹¦ç®¡ç†å‘˜**â€”â€”ä»–èƒ½è®°ä½å›¾ä¹¦é¦†é‡Œæ¯ä¸€æœ¬ä¹¦çš„ä½ç½®ï¼Œæ— è®ºä½ é—®ä»€ä¹ˆéƒ½èƒ½ç²¾ç¡®æ‰¾åˆ°ã€‚ä½†é—®é¢˜æ˜¯ï¼Œå½“å›¾ä¹¦é¦†è¶Šæ¥è¶Šå¤§ï¼ˆåºåˆ—è¶Šæ¥è¶Šé•¿ï¼‰ï¼Œä»–éœ€è¦çš„"è„‘å®¹é‡"ä¹Ÿä¼šçˆ†ç‚¸å¼å¢é•¿ã€‚

è€Œ**çº¿æ€§æ³¨æ„åŠ›**æ›´åƒä¸€ä¸ªç”¨**ç¬”è®°æœ¬è®°å½•**çš„å›¾ä¹¦ç®¡ç†å‘˜â€”â€”ä»–åªç»´æŠ¤ä¸€ä¸ªå›ºå®šå¤§å°çš„ç¬”è®°æœ¬ï¼Œæ¯çœ‹åˆ°ä¸€æœ¬æ–°ä¹¦å°±åœ¨ç¬”è®°æœ¬ä¸Šæ›´æ–°è®°å½•ã€‚è¿™æ ·æ•ˆç‡å¾ˆé«˜ï¼Œä½†é—®é¢˜æ˜¯ç¬”è®°æœ¬å®¹é‡æœ‰é™ï¼Œè®°å½•å¤ªå¤šå°±ä¼šæ··ä¹±ã€‚

**Kimi Linear** æå‡ºäº†ä¸€ä¸ªæ›´èªæ˜çš„å›¾ä¹¦ç®¡ç†å‘˜ï¼š
1. **æœ‰é€‰æ‹©åœ°é—å¿˜** - ä¸é‡è¦çš„ä¹¦å¯ä»¥ä»ç¬”è®°æœ¬é‡Œåˆ’æ‰ï¼ˆç»†ç²’åº¦é—å¿˜é—¨ï¼‰
2. **ä¼šè‡ªæˆ‘çº é”™** - å‘ç°è®°é”™äº†èƒ½åŠæ—¶ä¿®æ­£ï¼ˆDelta Ruleï¼‰
3. **å¶å°”ç¿»ç¿»å…¨éƒ¨ä¹¦æ¶** - æ¯è®°å½• 3 æ¬¡ç¬”è®°ï¼Œå°±å…¨é¢æ£€æŸ¥ä¸€æ¬¡ä¹¦æ¶ï¼ˆ3:1 æ··åˆæ¶æ„ï¼‰

![Kimi Linear æ¶æ„æ¦‚è§ˆ](images/01-kimi-linear-overview.svg)

---

## ä¸ºä»€ä¹ˆè¿™ç¯‡è®ºæ–‡å¾ˆé‡è¦ï¼Ÿ

### ğŸ¯ é¦–æ¬¡å…¨é¢è¶…è¶Šå…¨æ³¨æ„åŠ›

è¿™æ˜¯**ç¬¬ä¸€æ¬¡**æœ‰çº¿æ€§æ³¨æ„åŠ›æ¶æ„åœ¨å…¬å¹³å¯¹æ¯”ä¸‹ï¼ˆç›¸åŒè®­ç»ƒæ•°æ®ã€ç›¸åŒå‚æ•°é‡ï¼‰å…¨é¢è¶…è¶Šå…¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰ï¼ŒåŒ…æ‹¬ï¼š
- çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼ˆ4k tokensï¼‰
- é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼ˆ128k tokensï¼‰
- å¼ºåŒ–å­¦ä¹ è®­ç»ƒåœºæ™¯

| ä»»åŠ¡ | MLA (å…¨æ³¨æ„åŠ›) | Kimi Linear |
|------|---------------|-------------|
| MMLU-Pro (4k) | 47.2 | **51.0** (+3.8) |
| RULER (128k) | 81.3 | **84.3** (+3.0) |
| AIME 2025 | 20.6% | **21.3%** |

### âš¡ æƒŠäººçš„æ•ˆç‡æå‡

åœ¨ 1Mï¼ˆç™¾ä¸‡ï¼‰tokens çš„é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹ï¼š
- **KV Cache èŠ‚çœ 75%** - å†…å­˜å ç”¨å¤§å¹…é™ä½
- **è§£ç åŠ é€Ÿ 6.3Ã—** - ç”Ÿæˆé€Ÿåº¦å¿«äº† 6 å€å¤š
- **é¢„å¡«å……åŠ é€Ÿ 2.9Ã—** - å¤„ç†é•¿è¾“å…¥ä¹Ÿæ›´å¿«

> **çŸ¥è¯†ç‚¹è¡¥å…… - KV Cache**  
> åœ¨ Transformer æ¨ç†æ—¶ï¼Œéœ€è¦å­˜å‚¨ä¹‹å‰æ‰€æœ‰ token çš„ Key å’Œ Value å‘é‡ï¼Œç”¨äºè®¡ç®—æ³¨æ„åŠ›ã€‚è¿™éƒ¨åˆ†ç¼“å­˜ä¼šéšç€åºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼Œæ˜¯é•¿åºåˆ—æ¨ç†çš„ä¸»è¦å†…å­˜ç“¶é¢ˆã€‚Kimi Linear é€šè¿‡çº¿æ€§æ³¨æ„åŠ›çš„å›ºå®šçŠ¶æ€å¤§å°ï¼Œå¤§å¹…å‡å°‘äº†è¿™éƒ¨åˆ†å¼€é”€ã€‚

### ğŸ”“ å®Œå…¨å¼€æº

Moonshot AI å¼€æºäº†ï¼š
- KDA å†…æ ¸å®ç°ï¼ˆflash-linear-attentionï¼‰
- vLLM æ¨ç†é›†æˆ
- é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹æƒé‡ï¼ˆ48B å‚æ•°ï¼‰

---

## æ ¸å¿ƒæ–¹æ³•è¯¦è§£

### ä»€ä¹ˆæ˜¯çº¿æ€§æ³¨æ„åŠ›ï¼Ÿ

æ ‡å‡† Softmax æ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦æ˜¯ **O(TÂ²)**ï¼Œå…¶ä¸­ T æ˜¯åºåˆ—é•¿åº¦ã€‚è¿™æ„å‘³ç€åºåˆ—é•¿åº¦ç¿»å€ï¼Œè®¡ç®—é‡ä¼šå˜æˆ 4 å€ï¼

çº¿æ€§æ³¨æ„åŠ›çš„æ ¸å¿ƒæ€æƒ³æ˜¯ç»´æŠ¤ä¸€ä¸ª**å›ºå®šå¤§å°çš„è®°å¿†çŸ©é˜µ S**ï¼š

```
æ ‡å‡†æ³¨æ„åŠ›ï¼ˆæ…¢ï¼‰:  o_t = Î£ softmax(qÂ·k) Ã— v  -- éœ€è¦å’Œæ‰€æœ‰å†å² k è®¡ç®—
çº¿æ€§æ³¨æ„åŠ›ï¼ˆå¿«ï¼‰:  S_t = S_{t-1} + k_t Ã— v_t^T  -- åªæ›´æ–°ä¸€ä¸ªçŸ©é˜µ
                   o_t = S_t Ã— q_t              -- ç›´æ¥ç”¨çŸ©é˜µæŸ¥è¯¢
```

**å¤§ç™½è¯ç¿»è¯‘**ï¼šæ ‡å‡†æ³¨æ„åŠ›åƒæ˜¯æ¯æ¬¡éƒ½è¦ç¿»éæ•´ä¸ªå›¾ä¹¦é¦†ï¼Œè€Œçº¿æ€§æ³¨æ„åŠ›åƒæ˜¯ç»´æŠ¤ä¸€ä¸ªç´¢å¼•å¡ç‰‡ç›’ï¼Œæ–°ä¹¦æ¥äº†å°±åŠ ä¸€å¼ å¡ç‰‡ã€‚

### Delta Ruleï¼šè®©æ¨¡å‹å­¦ä¼šçº é”™

çº¯ç²¹çš„çº¿æ€§æ³¨æ„åŠ›æœ‰ä¸ªé—®é¢˜â€”â€”å®ƒåªä¼š**ç´¯åŠ **æ–°ä¿¡æ¯ï¼Œä¸ä¼š**ä¿®æ­£**é”™è¯¯çš„è®°å¿†ã€‚è¿™å°±åƒå›¾ä¹¦ç®¡ç†å‘˜çš„ç¬”è®°æœ¬åªä¼šè¶Šå†™è¶Šå¤šï¼Œå†™é”™äº†ä¹Ÿä¸æ”¹ã€‚

**DeltaNet** å¼•å…¥äº† Delta Ruleï¼ˆæ¢¯åº¦ä¸‹é™æ€æƒ³ï¼‰ï¼š

```python
# ä¼ ç»Ÿçº¿æ€§æ³¨æ„åŠ›ï¼šåªä¼šç´¯åŠ 
S_t = S_{t-1} + k_t @ v_t.T

# DeltaNetï¼šä¼šçº é”™
# å¦‚æœ S è®°å½•çš„ k->v æ˜ å°„ä¸å‡†ç¡®ï¼Œå°±ä¿®æ­£å®ƒ
error = S_{t-1}.T @ k_t - v_t  # è®¡ç®—è®°å¿†è¯¯å·®
S_t = S_{t-1} - beta * k_t @ error.T + beta * k_t @ v_t.T
# ç®€åŒ–åï¼š
S_t = (I - beta * k_t @ k_t.T) @ S_{t-1} + beta * k_t @ v_t.T
```

**å¤§ç™½è¯ç¿»è¯‘**ï¼šDelta Rule å°±åƒç»™å›¾ä¹¦ç®¡ç†å‘˜ä¸€æ”¯**æ©¡çš®æ“¦**â€”â€”å‘ç°ç´¢å¼•å¡ç‰‡è®°é”™äº†ï¼Œå¯ä»¥æ“¦æ‰é‡å†™ã€‚

### KDAï¼šç»†ç²’åº¦çš„è®°å¿†æ§åˆ¶

**Gated DeltaNet** åœ¨ Delta Rule åŸºç¡€ä¸ŠåŠ å…¥äº†"é—å¿˜é—¨"Î±ï¼Œè®©æ¨¡å‹å­¦ä¼šå¿˜è®°ä¸é‡è¦çš„ä¿¡æ¯ã€‚ä½†å®ƒçš„é—å¿˜é—¨æ˜¯**æ ‡é‡**â€”â€”æ‰€æœ‰ç»´åº¦ç”¨åŒä¸€ä¸ªé—å¿˜ç‡ã€‚

**Kimi Delta Attention (KDA)** çš„æ ¸å¿ƒåˆ›æ–°æ˜¯**ç»†ç²’åº¦é€šé“çº§é—å¿˜**ï¼š

```python
# Gated DeltaNetï¼šæ‰€æœ‰ç»´åº¦åŒä¸€ä¸ªé—å¿˜ç‡
S_t = alpha_t * (I - beta * k @ k.T) @ S_{t-1} + beta * k @ v.T

# KDAï¼šæ¯ä¸ªç»´åº¦ç‹¬ç«‹é—å¿˜ç‡
S_t = (I - beta * k @ k.T) @ Diag(alpha_t) @ S_{t-1} + beta * k @ v.T
# alpha_t æ˜¯ä¸€ä¸ªå‘é‡ï¼Œæ¯ä¸ªç»´åº¦æœ‰è‡ªå·±çš„é—å¿˜ç‡ï¼
```

![KDA æœºåˆ¶è¯¦è§£](images/02-kda-mechanism.svg)

**ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿ** 

æƒ³è±¡ä½ åœ¨è®°ç¬”è®°ï¼Œæœ‰äº›ä¿¡æ¯ï¼ˆæ¯”å¦‚äººåï¼‰éœ€è¦é•¿æœŸè®°ä½ï¼Œæœ‰äº›ä¿¡æ¯ï¼ˆæ¯”å¦‚ä»Šå¤©çš„å¤©æ°”ï¼‰å¾ˆå¿«å°±å¯ä»¥å¿˜æ‰ã€‚KDA è®©æ¨¡å‹çš„æ¯ä¸ª"ç¥ç»å…ƒ"éƒ½èƒ½ç‹¬ç«‹å†³å®šè‡ªå·±çš„è®°å¿†å‘¨æœŸã€‚

> **çŸ¥è¯†ç‚¹è¡¥å…… - ä¸ RoPE çš„ç±»æ¯”**  
> è®ºæ–‡æŒ‡å‡ºï¼ŒKDA çš„ç»†ç²’åº¦é—å¿˜é—¨å®é™…ä¸Šç±»ä¼¼äº RoPE ä½ç½®ç¼–ç çš„ä½œç”¨ã€‚RoPE ç»™æ¯å¯¹ç»´åº¦åˆ†é…ä¸åŒçš„æ—‹è½¬é¢‘ç‡ï¼ŒKDA ç»™æ¯ä¸ªç»´åº¦åˆ†é…ä¸åŒçš„é—å¿˜ç‡ã€‚ä¸¤è€…éƒ½å®ç°äº†"ç»†ç²’åº¦çš„ä½ç½®æ„ŸçŸ¥"ï¼Œåªæ˜¯ KDA æ˜¯æ•°æ®ç›¸å…³çš„ã€å¯å­¦ä¹ çš„ã€‚

### æ··åˆæ¶æ„ï¼š3:1 çš„é»„é‡‘æ¯”ä¾‹

çº¯çº¿æ€§æ³¨æ„åŠ›è™½ç„¶å¿«ï¼Œä½†åœ¨ç²¾ç¡®æ£€ç´¢ä»»åŠ¡ä¸Šä»ç„¶å¼±äºå…¨æ³¨æ„åŠ›ã€‚Kimi Linear é‡‡ç”¨**æ··åˆæ¶æ„**ï¼š

```
[KDA å±‚] â†’ [KDA å±‚] â†’ [KDA å±‚] â†’ [MLA å±‚] â†’ é‡å¤ N æ¬¡
    ^           ^           ^           ^
  çº¿æ€§æ³¨æ„åŠ›   çº¿æ€§æ³¨æ„åŠ›   çº¿æ€§æ³¨æ„åŠ›   å…¨æ³¨æ„åŠ›(NoPE)
```

**å…³é”®è®¾è®¡**ï¼š
1. **3:1 æ¯”ä¾‹** - 3 ä¸ª KDA å±‚ + 1 ä¸ª MLA å±‚æ˜¯æœ€ä¼˜é…æ¯”
2. **NoPE (æ— ä½ç½®ç¼–ç )** - MLA å±‚ä¸ä½¿ç”¨ RoPEï¼Œè®© KDA è´Ÿè´£æ‰€æœ‰ä½ç½®ç¼–ç 
3. **MLA** - ä½¿ç”¨ DeepSeek çš„ Multi-Head Latent Attentionï¼Œåœ¨ä¿æŒèƒ½åŠ›çš„åŒæ—¶èŠ‚çœ KV cache

![æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”](images/03-attention-comparison.svg)

**ä¸ºä»€ä¹ˆ NoPEï¼Ÿ**
- KDA æœ¬èº«å°±åƒä¸€ä¸ª"å¯å­¦ä¹ çš„ä½ç½®ç¼–ç "
- å»æ‰ MLA çš„ RoPE åï¼Œé•¿åºåˆ—å¤–æ¨æ›´å®¹æ˜“
- MLA å¯ä»¥è½¬æ¢ä¸ºæ›´é«˜æ•ˆçš„ Multi-Query Attention

---

## å®éªŒç»“æœ

### åˆæˆä»»åŠ¡ï¼šKDA æ›´èªæ˜

åœ¨ä¸‰ä¸ªæŒ‘æˆ˜æ€§çš„åˆæˆä»»åŠ¡ä¸Šï¼ŒKDA æ˜¾è‘—ä¼˜äº Gated DeltaNet å’Œ Mamba2ï¼š

| ä»»åŠ¡ | è¯´æ˜ | KDA è¡¨ç° |
|------|------|----------|
| **Palindrome** | åè½¬è¾“å…¥åºåˆ—ï¼ˆå¦‚ "ABCD" â†’ "DCBA"ï¼‰ | 2048 é•¿åº¦ä»ä¿æŒé«˜å‡†ç¡®ç‡ |
| **MQAR** | å¤šæŸ¥è¯¢å…³è”æ£€ç´¢ | æ”¶æ•›é€Ÿåº¦æœ€å¿« |
| **Stack** | æ¨¡æ‹Ÿ 64 ä¸ªç‹¬ç«‹æ ˆçš„ push/pop æ“ä½œ | çŠ¶æ€è¿½è¸ªèƒ½åŠ›æœ€å¼º |

### é¢„è®­ç»ƒå¯¹æ¯”ï¼šå…¨é¢é¢†å…ˆ

ä½¿ç”¨ 1.4T tokens è®­ç»ƒï¼ŒKimi Linear åœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡ä¸Šè¶…è¶Š MLA åŸºçº¿ï¼š

**é€šç”¨ä»»åŠ¡**:
- HellaSwag: 82.9 (vs MLA 81.7)
- BBH: 72.9 (vs MLA 71.6)  
- MMLU: 73.8 (vs MLA 71.6)
- MMLU-Pro: **51.0** (vs MLA 47.2)

**æ•°å­¦å’Œä»£ç **:
- GSM8K: 83.9 (vs MLA 83.7)
- CRUXEval-O-cot: 62.0 (vs MLA 61.5)

### é•¿ä¸Šä¸‹æ–‡ï¼šä¼˜åŠ¿æ˜æ˜¾

åœ¨ 128k ä¸Šä¸‹æ–‡è¯„æµ‹ä¸­ï¼ŒKimi Linear è¾¾åˆ°æœ€é«˜å¹³å‡åˆ†ï¼š

| æ¨¡å‹ | RULER | MRCR | RepoQA | å¹³å‡ |
|------|-------|------|--------|------|
| MLA | 81.3 | 22.6 | 63.0 | 52.2 |
| GDN-H | 80.5 | 23.9 | 63.0 | 51.2 |
| **Kimi Linear** | **84.3** | **29.6** | **68.5** | **54.5** |

### å¼ºåŒ–å­¦ä¹ ï¼šæ”¶æ•›æ›´å¿«

åœ¨æ•°å­¦ RL è®­ç»ƒä¸­ï¼ŒKimi Linear çš„è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡éƒ½æ˜¾è‘—é«˜äº MLAï¼Œè¯´æ˜å®ƒæ›´é€‚åˆ RL åœºæ™¯çš„é•¿åºåˆ—ç”Ÿæˆã€‚

---

## ç¤ºä¾‹ä»£ç 

### KDA æ ¸å¿ƒè®¡ç®—ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
def kda_recurrent(q, k, v, alpha, beta, S_prev):
    """
    Kimi Delta Attention çš„é€’å½’å½¢å¼
    
    å‚æ•°:
        q: æŸ¥è¯¢å‘é‡ [d_k]
        k: é”®å‘é‡ [d_k] (å·² L2 å½’ä¸€åŒ–)
        v: å€¼å‘é‡ [d_v]
        alpha: ç»†ç²’åº¦é—å¿˜é—¨ [d_k]
        beta: å­¦ä¹ ç‡ (æ ‡é‡)
        S_prev: ä¸Šä¸€æ­¥çš„çŠ¶æ€çŸ©é˜µ [d_k, d_v]
    
    è¿”å›:
        o: è¾“å‡ºå‘é‡ [d_v]
        S: æ›´æ–°åçš„çŠ¶æ€çŸ©é˜µ [d_k, d_v]
    """
    # ç»†ç²’åº¦é—å¿˜ï¼šæ¯ä¸ªç»´åº¦ç‹¬ç«‹è¡°å‡
    S_decayed = torch.diag(alpha) @ S_prev
    
    # Delta Rule æ›´æ–°ï¼šå…ˆ"æ“¦é™¤"æ—§çš„ k å…³è”ï¼Œå†å†™å…¥æ–°çš„
    # (I - beta * k @ k.T) æ˜¯ Householder å˜æ¢
    S = (torch.eye(d_k) - beta * k.outer(k)) @ S_decayed + beta * k.outer(v)
    
    # æŸ¥è¯¢è¾“å‡º
    o = S.T @ q
    
    return o, S
```

### Chunk-wise å¹¶è¡Œè®¡ç®—ï¼ˆé«˜æ•ˆç‰ˆï¼‰

```python
def chunk_kda(q, k, v, g, beta, chunk_size=64):
    """
    KDA çš„åˆ†å—å¹¶è¡Œç®—æ³•
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. å°†åºåˆ—åˆ†æˆå¤šä¸ª chunk
    2. chunk å†…éƒ¨ä½¿ç”¨ WY è¡¨ç¤ºæ³•å¹¶è¡Œè®¡ç®—
    3. chunk ä¹‹é—´ä¸²è¡Œæ›´æ–°çŠ¶æ€
    """
    B, T, H, K = q.shape
    V = v.shape[-1]
    N = T // chunk_size
    
    # é‡æ’ä¸º chunk æ ¼å¼
    q, k, v, g, beta = [
        rearrange(x, 'b (n c) h ... -> b h n c ...', c=chunk_size)
        for x in [q, k, v, g, beta]
    ]
    
    # è®¡ç®—ç´¯ç§¯é—å¿˜å› å­
    g_cumsum = g.cumsum(dim=-2)
    
    # åˆå§‹åŒ–çŠ¶æ€
    S = torch.zeros(B, H, K, V, device=q.device)
    
    # æ„å»º chunk å†…çš„æ³¨æ„åŠ›çŸ©é˜µ
    # ä½¿ç”¨ WY è¡¨ç¤ºæ³•å‹ç¼© Householder å˜æ¢åºåˆ—
    M = build_wy_matrix(k, g_cumsum, beta)  # çŸ©é˜µé€†
    w = M @ (g_cumsum.exp() * k)  # è¾…åŠ©å‘é‡
    u = M @ v
    
    outputs = []
    for i in range(N):
        # Inter-chunk: ä»çŠ¶æ€ä¸­è¯»å–
        o_inter = (q[:,:,i] * g_cumsum[:,:,i].exp()) @ S
        
        # Intra-chunk: chunk å†…éƒ¨å¹¶è¡Œè®¡ç®—
        A_intra = compute_intra_attention(q[:,:,i], k[:,:,i], g_cumsum[:,:,i])
        o_intra = A_intra @ (u[:,:,i] - w[:,:,i] @ S)
        
        outputs.append(o_inter + o_intra)
        
        # æ›´æ–°çŠ¶æ€
        decay = g_cumsum[:,:,i,-1:].exp()
        S = S * decay + (k[:,:,i] * decay).transpose(-1,-2) @ (u[:,:,i] - w[:,:,i] @ S)
    
    return torch.cat(outputs, dim=2)
```

### ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½ Kimi Linear 48B æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "moonshotai/Kimi-Linear-48B-A3B-Instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained("moonshotai/Kimi-Linear-48B-A3B-Instruct")

# ç”Ÿæˆï¼ˆæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡ï¼‰
prompt = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶..."
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=1000)
print(tokenizer.decode(outputs[0]))
```

---

## å±€é™æ€§ä¸æœªæ¥å·¥ä½œ

### è®ºæ–‡æåˆ°çš„å±€é™

1. **ç²¾ç¡®æ£€ç´¢ä»æœ‰å·®è·** - å°½ç®¡ KDA æ˜¾è‘—æ”¹è¿›ï¼Œçº¯çº¿æ€§æ³¨æ„åŠ›åœ¨"å¤§æµ·æé’ˆ"ç±»ä»»åŠ¡ä¸Šä»ä¸å¦‚å…¨æ³¨æ„åŠ›
2. **è®­ç»ƒå¤æ‚åº¦** - æ··åˆæ¶æ„éœ€è¦ä»”ç»†è°ƒæ•´å±‚é—´æ¯”ä¾‹å’Œå…¶ä»–è¶…å‚æ•°
3. **ç¡¬ä»¶é€‚é…** - è™½ç„¶ç†è®ºä¸Šæ›´é«˜æ•ˆï¼Œä½†éœ€è¦ä¸“é—¨çš„å†…æ ¸ä¼˜åŒ–æ‰èƒ½å‘æŒ¥å…¨éƒ¨æ½œåŠ›

### æœªæ¥ç ”ç©¶æ–¹å‘

- **ç¨€ç–æ³¨æ„åŠ›èåˆ** - è®ºæ–‡æåˆ°çº¿æ€§æ³¨æ„åŠ›å’Œç¨€ç–æ³¨æ„åŠ›å¯ä»¥äº’è¡¥
- **çŠ¶æ€æ‰©å±•** - é€šè¿‡æ›´å¤§çš„çŠ¶æ€çŸ©é˜µæå‡æ£€ç´¢èƒ½åŠ›
- **è‡ªé€‚åº”æ¯”ä¾‹** - åŠ¨æ€è°ƒæ•´ KDA:MLA æ¯”ä¾‹

---

## æ€»ç»“ä¸æ€è€ƒ

### æ ¸å¿ƒè´¡çŒ®

1. **KDA (Kimi Delta Attention)** - é¦–ä¸ªç»†ç²’åº¦é€šé“çº§é—å¿˜çš„ Delta Rule å˜ä½“
2. **æ··åˆæ¶æ„** - 3:1 çš„ KDA:MLA æ¯”ä¾‹åœ¨è´¨é‡å’Œæ•ˆç‡é—´å–å¾—æœ€ä½³å¹³è¡¡
3. **å…¨é¢è¶…è¶Š** - é¦–æ¬¡åœ¨å…¬å¹³å¯¹æ¯”ä¸‹çº¿æ€§æ³¨æ„åŠ›å…¨é¢å‡»è´¥å…¨æ³¨æ„åŠ›
4. **å®Œå…¨å¼€æº** - å†…æ ¸ã€æ¨¡å‹æƒé‡å…¨éƒ¨å¼€æ”¾

### æˆ‘çš„æ€è€ƒ

ğŸ¤” **ä¸ºä»€ä¹ˆä¹‹å‰çš„çº¿æ€§æ³¨æ„åŠ›åšä¸åˆ°ï¼Ÿ**

å…³é”®åœ¨äº KDA çš„ä¸‰ä¸ªåˆ›æ–°å®Œç¾é…åˆï¼š
- ç»†ç²’åº¦é—å¿˜è®©æ¨¡å‹èƒ½ç²¾ç¡®æ§åˆ¶è®°å¿†å‘¨æœŸ
- Delta Rule è®©æ¨¡å‹èƒ½çº æ­£é”™è¯¯è®°å¿†
- æ··åˆæ¶æ„å¼¥è¡¥äº†çº¯çº¿æ€§æ³¨æ„åŠ›çš„æ£€ç´¢çŸ­æ¿

ğŸš€ **å¯¹è¡Œä¸šçš„å½±å“**

Kimi Linear çš„æˆåŠŸè¡¨æ˜ï¼Œçº¿æ€§æ³¨æ„åŠ›ç»ˆäºå¯ä»¥ä½œä¸ºå…¨æ³¨æ„åŠ›çš„**æ›¿ä»£å“**è€Œé**å¦¥åæ–¹æ¡ˆ**ä½¿ç”¨äº†ã€‚è¿™å¯¹äºï¼š
- **é•¿ä¸Šä¸‹æ–‡æ¨ç†**ï¼ˆä»£ç†ã€å·¥å…·è°ƒç”¨ï¼‰
- **è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²**ï¼ˆå†…å­˜å—é™ï¼‰
- **é«˜ååæœåŠ¡**ï¼ˆé™ä½æ¨ç†æˆæœ¬ï¼‰

éƒ½æ˜¯é‡å¤§åˆ©å¥½ã€‚

ğŸ“ˆ **Scaling Law å¯ç¤º**

è®ºæ–‡æ˜¾ç¤º Kimi Linear çš„ scaling curve æ¯” MLA æ›´é™¡å³­ï¼Œæ„å‘³ç€åœ¨æ›´å¤§è§„æ¨¡ä¸Šä¼˜åŠ¿å¯èƒ½æ›´æ˜æ˜¾ã€‚è¿™å¯èƒ½æ”¹å˜å¤§æ¨¡å‹çš„æ¶æ„é€‰æ‹©ã€‚

---

## å‚è€ƒèµ„æ–™

- **è®ºæ–‡åŸæ–‡**: [arXiv:2510.26692](https://arxiv.org/abs/2510.26692)
- **GitHub ä»“åº“**: [MoonshotAI/Kimi-Linear](https://github.com/MoonshotAI/Kimi-Linear)
- **KDA å†…æ ¸**: [fla-org/flash-linear-attention](https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/kda)
- **æ¨¡å‹æƒé‡**: [Hugging Face](https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct)
- **ç›¸å…³è®ºæ–‡**:
  - Gated DeltaNet (ICLR 2025)
  - Gated Linear Attention (ICML 2024)
  - Mamba2 (2024)
  - DeepSeek-V3 MLA (2024)
