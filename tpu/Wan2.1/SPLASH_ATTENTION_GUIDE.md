# TPU Splash Attention å®Œå…¨æŒ‡å—

> ğŸ¯ æœ¬æ–‡ç”¨å¤§ç™½è¯è®²è§£ Google TPU ä¸Šçš„ Splash Attention å®ç°ï¼Œé€‚åˆæƒ³æ·±å…¥ç†è§£ TPU ç¼–ç¨‹çš„å¼€å‘è€…ã€‚

---

## ğŸ“– ç›®å½•

1. [ä¸€å¥è¯æ€»ç»“](#ä¸€å¥è¯æ€»ç»“)
2. [è¿™ä¸ª Kernel åˆ°åº•åœ¨å¹²ä»€ä¹ˆï¼Ÿ](#è¿™ä¸ª-kernel-åˆ°åº•åœ¨å¹²ä»€ä¹ˆ)
3. [ç‰¹ç‚¹ä¸ä¼˜ç¼ºç‚¹](#ç‰¹ç‚¹ä¸ä¼˜ç¼ºç‚¹)
4. [æ ¸å¿ƒç®—æ³•ï¼šåœ¨çº¿ Softmax](#æ ¸å¿ƒç®—æ³•åœ¨çº¿-softmax)
5. [TPU ç¡¬ä»¶é€‚é…](#tpu-ç¡¬ä»¶é€‚é…)
6. [ä»£ç é€è¡Œè§£æ](#ä»£ç é€è¡Œè§£æ)
7. [åˆ†å¸ƒå¼æ‰§è¡Œç­–ç•¥](#åˆ†å¸ƒå¼æ‰§è¡Œç­–ç•¥)
8. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## è¿™ä¸ª Kernel åˆ°åº•åœ¨å¹²ä»€ä¹ˆï¼Ÿ

### èƒŒæ™¯ï¼šAttention çš„ç—›ç‚¹

Transformer çš„æ ¸å¿ƒæ˜¯ Self-Attentionï¼š

```
Attention(Q, K, V) = softmax(Q @ K^T / âˆšd) @ V
```

é—®é¢˜æ¥äº†ï¼šå½“åºåˆ—é•¿åº¦ N = 4096 æ—¶ï¼Œ`Q @ K^T` äº§ç”Ÿä¸€ä¸ª **4096 Ã— 4096 = 1600ä¸‡** ä¸ªå…ƒç´ çš„çŸ©é˜µï¼

```mermaid
graph LR
    subgraph "ä¼ ç»Ÿ Attention çš„é—®é¢˜"
        Q["Q<br/>(4096, 128)"] --> MM1["çŸ©é˜µä¹˜æ³•"]
        K["K<br/>(4096, 128)"] --> MM1
        MM1 --> S["S = QK^T<br/>âš ï¸ (4096, 4096)<br/>64MB å†…å­˜!"]
        S --> SM["softmax"]
        SM --> P["P<br/>(4096, 4096)"]
        P --> MM2["çŸ©é˜µä¹˜æ³•"]
        V["V<br/>(4096, 128)"] --> MM2
        MM2 --> O["Output<br/>(4096, 128)"]
    end
    
    style S fill:#ff6b6b,stroke:#333,stroke-width:2px
```

### è§£å†³æ–¹æ¡ˆï¼šåˆ†å— + åœ¨çº¿æ›´æ–°

Splash Attention çš„æ ¸å¿ƒæ€æƒ³ï¼š**ä¸å­˜å‚¨å®Œæ•´çš„ S çŸ©é˜µï¼Œè€Œæ˜¯åˆ†å—è®¡ç®—å¹¶åœ¨çº¿æ›´æ–°ç»“æœ**ã€‚

```mermaid
graph TB
    subgraph "Splash Attention çš„åˆ†å—ç­–ç•¥"
        Q2["Q åˆ†æˆå°å—<br/>(bq=3328)"] --> |"é€å—å¤„ç†"| LOOP
        K2["K åˆ†æˆå°å—<br/>(bkv=2816)"] --> LOOP
        V2["V åˆ†æˆå°å—<br/>(bkv=2816)"] --> LOOP
        
        LOOP["å¾ªç¯å¤„ç†æ¯ä¸ª KV å—"] --> UPDATE["åœ¨çº¿æ›´æ–°<br/>(m, l, o)"]
        UPDATE --> |"ä¸‹ä¸€ä¸ªå—"| LOOP
        UPDATE --> |"æœ€åä¸€ä¸ªå—"| NORM["å½’ä¸€åŒ–<br/>o / l"]
        NORM --> OUT["æœ€ç»ˆè¾“å‡º"]
    end
```

**å…³é”®æ´å¯Ÿ**ï¼šé€šè¿‡ç»´æŠ¤ä¸‰ä¸ªç»Ÿè®¡é‡ `(m, l, o)`ï¼Œå¯ä»¥å¢é‡æ›´æ–° softmax ç»“æœï¼Œä¸éœ€è¦ä¸€æ¬¡çœ‹åˆ°æ‰€æœ‰æ•°æ®ï¼

---

## ç‰¹ç‚¹ä¸ä¼˜ç¼ºç‚¹

### âœ… ä¼˜ç‚¹

| ä¼˜ç‚¹ | è¯´æ˜ |
|------|------|
| **å†…å­˜æ•ˆç‡** | O(N) è€Œé O(NÂ²)ï¼Œé•¿åºåˆ—ä¸å†çˆ†å†…å­˜ |
| **exp2 ä¼˜åŒ–** | åˆ©ç”¨ TPU VPU çš„ exp2 ç¡¬ä»¶æŒ‡ä»¤ï¼Œæ¯” exp å¿« |
| **åˆ†å—è®¡ç®—** | æ•°æ®ç•™åœ¨ç‰‡ä¸Š VMEMï¼Œå‡å°‘ HBM è®¿é—® |
| **å¤šæ ¸å¹¶è¡Œ** | è‡ªåŠ¨åˆ©ç”¨ TPU çš„å¤šæ ¸æ¶æ„ |
| **GQA æ”¯æŒ** | æ”¯æŒ Grouped Query Attentionï¼Œå¤šä¸ª Q head å…±äº« KV |

### âŒ ç¼ºç‚¹

| ç¼ºç‚¹ | è¯´æ˜ |
|------|------|
| **Sublane å†—ä½™** | ç»Ÿè®¡é‡ç”¨ (8, bq) å­˜å‚¨ï¼Œå®é™…åªéœ€è¦ (bq,)ï¼Œ7/8 VPU ç®—åŠ›æµªè´¹ |
| **å—å¤§å°å›ºå®š** | éœ€è¦ paddingï¼ŒçŸ­åºåˆ—å¯èƒ½æµªè´¹è®¡ç®— |
| **ä»£ç å¤æ‚** | Pallas ç¼–ç¨‹é—¨æ§›é«˜ï¼Œè°ƒè¯•å›°éš¾ |
| **æ— åå‘ä¼ æ’­** | å½“å‰å®ç°åªæœ‰å‰å‘ï¼Œbackward éœ€è¦å¦å¤–å®ç° |

### ğŸ“Š æ€§èƒ½æ•°æ®ï¼ˆå‚è€ƒï¼‰

```
åºåˆ—é•¿åº¦    ä¼ ç»Ÿæ–¹æ³•å†…å­˜    Splash å†…å­˜    åŠ é€Ÿæ¯”
1024        4 MB           ~1 MB          1.2x
4096        64 MB          ~4 MB          2.5x
16384       1 GB           ~16 MB         5x+
```

---

## æ ¸å¿ƒç®—æ³•ï¼šåœ¨çº¿ Softmax

### (m, l, o) ä¸‰å…ƒç»„æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨çº¿ Softmax çš„æ ¸å¿ƒæ˜¯ç»´æŠ¤ä¸‰ä¸ªç»Ÿè®¡é‡ **(m, l, o)**ï¼š

| å˜é‡ | å«ä¹‰ | æ•°å­¦å®šä¹‰ | å½¢çŠ¶ |
|------|------|----------|------|
| **m** | **m**axï¼šå½“å‰çœ‹åˆ°çš„æœ€å¤§å€¼ | `m = max(qk)` | `(8, bq)` |
| **l** | **l**ogsumï¼šexp çš„ç´¯åŠ å’Œ | `l = Î£ exp(qk - m)` | `(8, bq)` |
| **o** | **o**utputï¼šæœªå½’ä¸€åŒ–çš„è¾“å‡º | `o = Î£ exp(qk - m) Ã— V` | `(head_dim, bq)` |

### âš ï¸ å…³é”®é—®é¢˜ï¼šæ¯ä¸ª Q éƒ½æœ‰ä¸€å¥— (m, l, o) å—ï¼Ÿ

**æ˜¯çš„ï¼æ¯ä¸ª Query token éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„ (m, l, o)ï¼**

```mermaid
graph TB
    subgraph "å½¢çŠ¶è§£è¯»"
        M_SHAPE["m: (8, bq) = (8, 3328)"]
        L_SHAPE["l: (8, bq) = (8, 3328)"]
        O_SHAPE["o: (head_dim, bq) = (128, 3328)"]
    end
    
    subgraph "å«ä¹‰"
        M_MEAN["æ¯ä¸ª Q ä½ç½®æœ‰è‡ªå·±çš„ max<br/>bq=3328 ä¸ª Q token<br/>â†’ 3328 ä¸ªç‹¬ç«‹çš„ m å€¼"]
        L_MEAN["æ¯ä¸ª Q ä½ç½®æœ‰è‡ªå·±çš„ sum<br/>â†’ 3328 ä¸ªç‹¬ç«‹çš„ l å€¼"]
        O_MEAN["æ¯ä¸ª Q ä½ç½®æœ‰è‡ªå·±çš„è¾“å‡º<br/>â†’ 3328 ä¸ª 128 ç»´å‘é‡"]
    end
    
    M_SHAPE --> M_MEAN
    L_SHAPE --> L_MEAN
    O_SHAPE --> O_MEAN
```

**ä¸ºä»€ä¹ˆæ¯ä¸ª Q éƒ½è¦å•ç‹¬å­˜ï¼Ÿ**

å› ä¸º Attention çš„æœ¬è´¨æ˜¯ï¼š**æ¯ä¸ª Query ç‹¬ç«‹åœ°å¯¹æ‰€æœ‰ Key-Value åšåŠ æƒæ±‚å’Œ**ã€‚

```
å¯¹äº Q ä¸­çš„ç¬¬ i ä¸ª token (q_i):
  - m[i] = max over all K: (q_i Â· k_j) å¯¹æ‰€æœ‰ j çš„æœ€å¤§å€¼
  - l[i] = Î£_j exp(q_i Â· k_j - m[i])
  - o[i] = Î£_j exp(q_i Â· k_j - m[i]) Ã— v_j
  
æœ€ç»ˆè¾“å‡º: output[i] = o[i] / l[i]
```

**ä¸æ˜¯å…¨å±€ä¸€ä¸ª mï¼** æ¯ä¸ª Query token çœ‹åˆ°çš„ Key çš„"æœ€ç›¸å…³ç¨‹åº¦"æ˜¯ä¸åŒçš„ï¼Œæ‰€ä»¥ max å€¼ä¹Ÿä¸åŒã€‚

```mermaid
graph LR
    subgraph "å½¢è±¡ç†è§£"
        Q1["Q token 0"] --> |"çœ‹æ‰€æœ‰ K"| M1["m[0] = æœ€å¤§ç›¸å…³åº¦"]
        Q2["Q token 1"] --> |"çœ‹æ‰€æœ‰ K"| M2["m[1] = æœ€å¤§ç›¸å…³åº¦"]
        Q3["Q token ..."] --> |"çœ‹æ‰€æœ‰ K"| M3["m[...] = ..."]
        QN["Q token 3327"] --> |"çœ‹æ‰€æœ‰ K"| MN["m[3327] = æœ€å¤§ç›¸å…³åº¦"]
    end
```

```mermaid
graph LR
    subgraph "(m, l, o) çš„å«ä¹‰"
        M["m = max<br/>æ•°å€¼ç¨³å®šæ€§<br/>é˜²æ­¢ exp æº¢å‡º"]
        L["l = Î£exp(x-m)<br/>softmax åˆ†æ¯<br/>å½’ä¸€åŒ–å› å­"]
        O["o = Î£exp(x-m)Ã—V<br/>åŠ æƒå’Œ<br/>æœªå½’ä¸€åŒ–è¾“å‡º"]
    end
    
    M --> |"ç”¨äºè®¡ç®—"| L
    L --> |"æœ€åå½’ä¸€åŒ–"| RESULT["output = o / l"]
    O --> RESULT
```

**ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸‰ä¸ªé‡ï¼Ÿ**

ä¼ ç»Ÿ softmax éœ€è¦ä¸¤éæ‰«ææ•°æ®ï¼š
1. ç¬¬ä¸€éï¼šæ±‚ maxï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
2. ç¬¬äºŒéï¼šè®¡ç®— exp å’Œ sum

åœ¨çº¿ softmax åªéœ€ä¸€éæ‰«æï¼Œé€šè¿‡ **(m, l, o)** å¢é‡æ›´æ–°ï¼š
- æ¯æ¬¡åªçœ‹ä¸€å°å—æ•°æ®
- æ›´æ–° mã€lã€o
- æœ€åç”¨ l å½’ä¸€åŒ– o

### ä¼ ç»Ÿ Softmax çš„é—®é¢˜

```python
# ä¼ ç»Ÿ softmax éœ€è¦ä¸¤éæ‰«æ
def softmax(x):
    max_val = x.max()           # ç¬¬ä¸€éï¼šæ±‚ maxï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
    exp_x = exp(x - max_val)    # ç¬¬äºŒéï¼šè®¡ç®— exp
    sum_exp = exp_x.sum()       # è¿˜éœ€è¦æ±‚å’Œ
    return exp_x / sum_exp      # æœ€åå½’ä¸€åŒ–
```

**é—®é¢˜**ï¼šå¦‚æœ x å¤ªå¤§å­˜ä¸ä¸‹ï¼Œæ€ä¹ˆåˆ†å—è®¡ç®—ï¼Ÿ

### åœ¨çº¿ Softmax çš„é­”æ³•

```mermaid
graph LR
    subgraph "åœ¨çº¿ Softmax æ ¸å¿ƒå…¬å¼"
        PREV["ä¹‹å‰çš„çŠ¶æ€<br/>m_prev, l_prev, o_prev"] --> MERGE["åˆå¹¶"]
        CURR["å½“å‰å—<br/>m_curr, l_curr, o_curr"] --> MERGE
        MERGE --> NEXT["æ–°çŠ¶æ€<br/>m_next, l_next, o_next"]
    end
```

**æ ¸å¿ƒå…¬å¼**ï¼š

```
m_next = max(m_prev, m_curr)
Î± = exp(m_prev - m_next)       # æ—§çŠ¶æ€çš„ç¼©æ”¾å› å­
Î² = exp(m_curr - m_next)       # æ–°çŠ¶æ€çš„ç¼©æ”¾å› å­

l_next = Î± * l_prev + Î² * l_curr
o_next = Î± * o_prev + Î² * o_curr
```

**ç›´è§‰ç†è§£**ï¼šå½“ max å˜å¤§æ—¶ï¼Œä¹‹å‰çš„ exp å€¼éœ€è¦ç¼©å°ï¼ˆå› ä¸º exp(x-max) å˜å°äº†ï¼‰ã€‚

### ä»£ç ç¤ºä¾‹

```python
# åœ¨çº¿æ›´æ–°ä¼ªä»£ç 
def online_softmax_update(m_prev, l_prev, o_prev, qk_block, v_block):
    # 1. è®¡ç®—å½“å‰å—çš„ max
    m_curr = qk_block.max()
    
    # 2. æ›´æ–°å…¨å±€ max
    m_next = max(m_prev, m_curr)
    
    # 3. è®¡ç®—ç¼©æ”¾å› å­
    alpha = exp2(m_prev - m_next)  # ç”¨ exp2ï¼
    
    # 4. è®¡ç®—å½“å‰å—çš„è´¡çŒ®
    s_curr = exp2(qk_block - m_next)
    l_curr = s_curr.sum()
    o_curr = s_curr @ v_block
    
    # 5. åˆå¹¶
    l_next = alpha * l_prev + l_curr
    o_next = alpha * o_prev + o_curr
    
    return m_next, l_next, o_next
```

---

## TPU ç¡¬ä»¶é€‚é…

### TPU æ¶æ„æ¦‚è§ˆ

```mermaid
graph TB
    subgraph "TPU v6e èŠ¯ç‰‡æ¶æ„"
        HBM["HBM<br/>é«˜å¸¦å®½å†…å­˜<br/>16-32 GB"]
        
        subgraph "TPU Core"
            VMEM["VMEM<br/>å‘é‡å†…å­˜<br/>16-32 MB"]
            MXU["MXU<br/>çŸ©é˜µä¹˜æ³•å•å…ƒ<br/>128Ã—128"]
            VPU["VPU<br/>å‘é‡å¤„ç†å•å…ƒ<br/>8 ä¸ª sublane"]
        end
        
        HBM <-->|"å¸¦å®½ç“¶é¢ˆ"| VMEM
        VMEM --> MXU
        VMEM --> VPU
    end
    
    style MXU fill:#90EE90
    style VPU fill:#87CEEB
    style HBM fill:#FFB6C1
```

### å…³é”®ä¼˜åŒ–ç‚¹

#### 1. exp2 æ›¿ä»£ exp

```python
# TPU VPU æœ‰ exp2 ç¡¬ä»¶æŒ‡ä»¤ï¼Œä½†æ²¡æœ‰é«˜æ•ˆçš„ exp
# æ•°å­¦è½¬æ¢ï¼šexp(x) = 2^(x * log2(e))

LOG2_E = 1.44269504  # logâ‚‚(e)

# é¢„å¤„ç†ï¼šåœ¨è°ƒç”¨ kernel å‰å°† Q ä¹˜ä»¥ LOG2_E
q = q * scale * LOG2_E

# kernel å†…éƒ¨ç›´æ¥ç”¨ exp2
s = jnp.exp2(qk - max)  # è€Œä¸æ˜¯ jnp.exp(qk - max)
```

#### 2. å—å¤§å°é€‰æ‹©

```python
BQSIZE = 3328          # Query å—å¤§å° = 26 Ã— 128
BKVSIZE = 2816         # KV å—å¤§å° = 22 Ã— 128
BKVCOMPUTESIZE = 256   # å†…å±‚è®¡ç®—å— = 2 Ã— 128
```

**ä¸ºä»€ä¹ˆæ˜¯è¿™äº›æ•°å­—ï¼Ÿ**

```mermaid
graph LR
    subgraph "å—å¤§å°è®¾è®¡è€ƒé‡"
        A["128 çš„å€æ•°"] --> B["é€‚é… MXU 128Ã—128"]
        C["8 çš„å€æ•°"] --> D["é€‚é… VPU 8 sublanes"]
        E["æ€»å¤§å°é™åˆ¶"] --> F["ä¸è¶…è¿‡ VMEM å®¹é‡"]
    end
```

#### 3. NUM_SUBLANES = 8 çš„ç§˜å¯†

```mermaid
graph TB
    subgraph "VPU çš„ 8 ä¸ª Sublane"
        SL0["Sublane 0"]
        SL1["Sublane 1"]
        SL2["Sublane 2"]
        SL3["Sublane 3"]
        SL4["Sublane 4"]
        SL5["Sublane 5"]
        SL6["Sublane 6"]
        SL7["Sublane 7"]
    end
    
    DATA["m_scratch<br/>(8, bq)"] --> SL0
    DATA --> SL1
    DATA --> SL2
    DATA --> SL3
    DATA --> SL4
    DATA --> SL5
    DATA --> SL6
    DATA --> SL7
    
    NOTE["âš ï¸ 8 è¡Œå­˜å‚¨ç›¸åŒçš„å€¼ï¼<br/>è¿™æ˜¯ä¸ºäº†é€‚é… pltpu.repeat"]
```

**ä¸ºä»€ä¹ˆå†—ä½™å­˜å‚¨ï¼Ÿ** è§ [å¸¸è§é—®é¢˜](#ä¸ºä»€ä¹ˆ-m_scratch-æ˜¯-8-bq-è€Œä¸æ˜¯-bq)ã€‚

---

## ä»£ç é€è¡Œè§£æ

### æ•´ä½“ç»“æ„

```mermaid
graph TB
    subgraph "å‡½æ•°è°ƒç”¨é“¾"
        ENTRY["tpu_splash_attention()"] --> |"åˆ†ç‰‡ç­–ç•¥"| SHARD["shard_map"]
        SHARD --> KERNEL3D["kernel_3d()"]
        KERNEL3D --> |"padding"| MAKE["_make_splash_mha()"]
        MAKE --> FORWARD["_splash_attention_forward()"]
        FORWARD --> |"pallas_call"| FLASH["_flash_attention_kernel()"]
    end
```

### Kernel æ ¸å¿ƒä»£ç 

```python
def _flash_attention_kernel(
    q_ref,           # Query å—ï¼Œå½¢çŠ¶ (bq, head_dim)
    k_ref,           # Key å—ï¼Œå½¢çŠ¶ (bkv, head_dim)
    v_ref,           # Value å—ï¼Œå½¢çŠ¶ (bkv, head_dim)
    m_scratch_ref,   # max ä¸´æ—¶å­˜å‚¨ï¼Œå½¢çŠ¶ (8, bq)
    l_scratch_ref,   # sum ä¸´æ—¶å­˜å‚¨ï¼Œå½¢çŠ¶ (8, bq)
    o_scratch_ref,   # è¾“å‡ºç´¯ç§¯å™¨ï¼Œå½¢çŠ¶ (head_dim, bq)
    o_ref,           # æœ€ç»ˆè¾“å‡ºï¼Œå½¢çŠ¶ (head_dim, bq)
    *,
    mask_value, grid_width, bq, bkv, bkv_compute, bkv_compute_in, head_dim_v,
):
```

#### é˜¶æ®µ 1ï¼šåˆå§‹åŒ–ï¼ˆj=0 æ—¶ï¼‰

```python
@pl.when(j == 0)
def init():
    o_scratch_ref[...] = jnp.zeros_like(o_scratch_ref)  # è¾“å‡ºæ¸…é›¶
    m_scratch_ref[...] = jnp.full_like(m_scratch_ref, mask_value)  # max = -inf
    l_scratch_ref[...] = jnp.zeros_like(l_scratch_ref)  # sum = 0
```

### âš ï¸ `@pl.when` æ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼Ÿä¸ºä»€ä¹ˆæ²¡äººè°ƒç”¨ `init()` å’Œ `end()`ï¼Ÿ

**è¿™æ˜¯ Pallas æ¡†æ¶çš„é­”æ³•ï¼Œä¸éœ€è¦æ˜¾å¼è°ƒç”¨ï¼**

```mermaid
graph TB
    subgraph "ä½ çœ‹åˆ°çš„ä»£ç "
        CODE["@pl.when(j == 0)<br/>def init():<br/>    ..."]
    end
    
    subgraph "ç¼–è¯‘åå®é™…ç”Ÿæˆçš„ä»£ç "
        COMPILED["# è‡ªåŠ¨å†…è”åˆ° kernel ä¸»ä½“<br/>if j == 0:<br/>    o_scratch_ref[...] = 0<br/>    m_scratch_ref[...] = mask_value<br/>    l_scratch_ref[...] = 0"]
    end
    
    CODE -->|"Pallas ç¼–è¯‘å™¨<br/>è‡ªåŠ¨è½¬æ¢"| COMPILED
```

**å·¥ä½œåŸç†**ï¼š

1. **`@pl.when(condition)` æ˜¯ç¼–è¯‘æ—¶è£…é¥°å™¨**ï¼š
   - å®ƒå‘Šè¯‰ Pallas ç¼–è¯‘å™¨ï¼šå½“ `condition` ä¸ºçœŸæ—¶æ‰§è¡Œè¿™ä¸ªå‡½æ•°
   - ç¼–è¯‘å™¨ä¼šè‡ªåŠ¨å°†å‡½æ•°ä½“å†…è”åˆ° kernel ä¸»ä½“ä¸­

2. **ä¸éœ€è¦æ˜¾å¼è°ƒç”¨**ï¼š
   - è£…é¥°å™¨åœ¨å®šä¹‰æ—¶å°±å·²ç»"æ³¨å†Œ"äº†è¿™æ®µä»£ç 
   - Pallas ç¼–è¯‘å™¨ä¼šåœ¨æ­£ç¡®çš„ä½ç½®æ’å…¥æ¡ä»¶æ‰§è¡Œ

3. **ç±»ä¼¼äº Python è£…é¥°å™¨çš„å‰¯ä½œç”¨**ï¼š
   ```python
   # è¿™ä¸¤ç§å†™æ³•æ•ˆæœç›¸åŒï¼š
   
   # å†™æ³• 1ï¼šè£…é¥°å™¨ï¼ˆPallas é£æ ¼ï¼‰
   @pl.when(j == 0)
   def init():
       do_something()
   
   # å†™æ³• 2ï¼šç­‰ä»·çš„æ¦‚å¿µï¼ˆä¼ªä»£ç ï¼‰
   pl.register_conditional_block(
       condition=(j == 0),
       body=lambda: do_something()
   )
   ```

4. **ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ**ï¼š
   - TPU ä¸æ”¯æŒè¿è¡Œæ—¶åˆ†æ”¯é¢„æµ‹
   - ç¼–è¯‘æ—¶ç¡®å®šæ‰€æœ‰æ¡ä»¶åˆ†æ”¯ï¼Œç”Ÿæˆ VLIW æŒ‡ä»¤
   - æ‰€æœ‰ grid ä½ç½®æ‰§è¡Œç›¸åŒä»£ç ï¼Œåªæ˜¯æ¡ä»¶ä¸åŒ

**`init()` å’Œ `end()` çš„æ‰§è¡Œæ—¶æœº**ï¼š

```mermaid
sequenceDiagram
    participant J0 as j=0 (ç¬¬ä¸€ä¸ª KV å—)
    participant J1 as j=1
    participant JN as j=grid_width-1 (æœ€åä¸€ä¸ª)
    
    Note over J0: @pl.when(j == 0)<br/>init() æ‰§è¡Œ âœ“
    J0->>J0: åˆå§‹åŒ– m, l, o
    J0->>J0: ä¸»å¾ªç¯ body()
    
    Note over J1: @pl.when(j == 0)<br/>init() ä¸æ‰§è¡Œ âœ—
    J1->>J1: ä¸»å¾ªç¯ body()
    
    Note over JN: @pl.when(j == grid_width - 1)<br/>end() æ‰§è¡Œ âœ“
    JN->>JN: ä¸»å¾ªç¯ body()
    JN->>JN: å½’ä¸€åŒ–è¾“å‡º
```

```mermaid
sequenceDiagram
    participant Grid as Grid Position (h,i,j)
    participant Scratch as Scratch Memory
    
    Note over Grid: j = 0 (ç¬¬ä¸€ä¸ª KV å—)
    Grid->>Scratch: åˆå§‹åŒ– m = -âˆ
    Grid->>Scratch: åˆå§‹åŒ– l = 0
    Grid->>Scratch: åˆå§‹åŒ– o = 0
```

#### é˜¶æ®µ 2ï¼šä¸»å¾ªç¯

```python
def body(kv_compute_index, _):
    # è¯»å– Q å’Œ K å—
    q = q_ref[...]                        # (bq, head_dim)
    k = k_ref[slice_k, :]                 # (bkv_compute, head_dim)
    
    # è®¡ç®— QK^Tï¼ˆåœ¨ MXU ä¸Šæ‰§è¡Œï¼‰
    qk = lax.dot_general(k, q, NT_DIM_NUMBERS)  # (bkv_compute, bq)
    
    # åœ¨çº¿ softmax æ›´æ–°
    for i in range(0, qk.shape[0], step):
        m_curr = qk[i:i+step].max(axis=0)
        m_next = jnp.maximum(m_prev, m_curr)
        
        s_curr = jnp.exp2(qk[i:i+step] - m_next)  # â† exp2 ä¼˜åŒ–ï¼
        l_curr = s_curr.sum(axis=0)
        
        alpha = jnp.exp2(m_prev - m_next)
        l_next = l_curr + alpha * l_prev
        
        o_curr = lax.dot_general(v[i:i+step], s_curr, ...)  # S @ V
        o_prev = alpha * o_prev + o_curr
```

```mermaid
flowchart TB
    subgraph "æ¯ä¸ª KV å—çš„å¤„ç†æµç¨‹"
        A["è¯»å– Q, K å—"] --> B["MXU: QK^T"]
        B --> C["VPU: è®¡ç®— max"]
        C --> D["VPU: exp2(qk - max)"]
        D --> E["VPU: æ±‚å’Œ l"]
        E --> F["VPU: è®¡ç®— alpha"]
        F --> G["MXU: S @ V"]
        G --> H["VPU: æ›´æ–° o"]
        H --> I["å†™å› scratch"]
    end
    
    style B fill:#90EE90
    style G fill:#90EE90
    style C fill:#87CEEB
    style D fill:#87CEEB
    style E fill:#87CEEB
    style F fill:#87CEEB
    style H fill:#87CEEB
```

#### é˜¶æ®µ 3ï¼šå½’ä¸€åŒ–ï¼ˆæœ€åä¸€ä¸ª jï¼‰

```python
@pl.when(j == grid_width - 1)
def end():
    l = l_scratch_ref[...]                          # (8, bq)
    l_inv = pltpu.repeat(1.0 / l, repeats, axis=0)  # æ‰©å±•åˆ° (head_dim, bq)
    o_ref[...] = o_scratch_ref[...] * l_inv         # å½’ä¸€åŒ–
```

---

## åˆ†å¸ƒå¼æ‰§è¡Œç­–ç•¥

### ç­–ç•¥é€‰æ‹©é€»è¾‘

```mermaid
flowchart TD
    START["è¾“å…¥ Q, K, V"] --> CHECK{"KV åºåˆ—é•¿åº¦ > 10000?"}
    
    CHECK -->|"æ˜¯<br/>(Self-Attention)"| HEAD["Head Parallel<br/>æŒ‰ head åˆ‡åˆ†"]
    CHECK -->|"å¦<br/>(Cross-Attention)"| SEQ["Sequence Parallel<br/>æŒ‰ Q åºåˆ—åˆ‡åˆ†"]
    
    HEAD --> |"æ¯ä¸ªè®¾å¤‡"| H_EXEC["å¤„ç†éƒ¨åˆ† heads<br/>å®Œæ•´ KV"]
    SEQ --> |"æ¯ä¸ªè®¾å¤‡"| S_EXEC["å¤„ç†éƒ¨åˆ† Q<br/>å®Œæ•´ KV"]
    
    H_EXEC --> OUT["åˆå¹¶è¾“å‡º"]
    S_EXEC --> OUT
```

### Head Parallel vs Sequence Parallel

| ç‰¹æ€§ | Head Parallel | Sequence Parallel |
|------|---------------|-------------------|
| **é€‚ç”¨åœºæ™¯** | é•¿åºåˆ— Self-Attention | çŸ­åºåˆ— Cross-Attention |
| **åˆ‡åˆ†ç»´åº¦** | Q/K/V çš„ head ç»´åº¦ | Q çš„ sequence ç»´åº¦ |
| **KV å­˜å‚¨** | æ¯ä¸ªè®¾å¤‡åªæœ‰éƒ¨åˆ† KV | æ¯ä¸ªè®¾å¤‡æœ‰å®Œæ•´ KV |
| **é€šä¿¡å¼€é”€** | æ—  | éœ€è¦å¹¿æ’­ KV |
| **é˜ˆå€¼** | kv_seq_len > 10000 | kv_seq_len â‰¤ 10000 |

### ä»£ç ç¤ºä¾‹

```python
# é•¿ KV åºåˆ—ï¼ˆself-attentionï¼‰ä½¿ç”¨ head parallel
if kv_seq_len > 10000:
    q_spec = P(dp_mesh_key, remain_mesh_key, None, None)  # head ç»´åº¦åˆ‡åˆ†
    kv_spec = P(dp_mesh_key, remain_mesh_key, None, None)
else:
    # çŸ­ KV åºåˆ—ï¼ˆcross-attentionï¼‰ä½¿ç”¨ sequence parallel
    q_spec = P(dp_mesh_key, None, remain_mesh_key, None)  # seq ç»´åº¦åˆ‡åˆ†
    kv_spec = P(dp_mesh_key, None, None, None)            # KV ä¸åˆ‡åˆ†
```

---

## å¸¸è§é—®é¢˜

### ä¸ºä»€ä¹ˆ m_scratch æ˜¯ (8, bq) è€Œä¸æ˜¯ (bq)ï¼Ÿ

**ç®€çŸ­å›ç­”**ï¼šè¿™æ˜¯ TPU ç¡¬ä»¶å’Œ Pallas æ¡†æ¶çš„è¦æ±‚ã€‚

**è¯¦ç»†è§£é‡Š**ï¼š

```mermaid
graph TB
    subgraph "ç†æƒ³æƒ…å†µ vs ç°å®"
        IDEAL["ç†æƒ³ï¼š(bq,)<br/>æ¯ä¸ªä½ç½®ä¸€ä¸ª max å€¼"]
        REAL["ç°å®ï¼š(8, bq)<br/>8 ä¸ª sublane å­˜å‚¨ç›¸åŒå€¼"]
    end
    
    subgraph "ä¸ºä»€ä¹ˆéœ€è¦å†—ä½™ï¼Ÿ"
        R1["pltpu.repeat éœ€è¦<br/>æ‰€æœ‰ sublane æœ‰æ•°æ®"]
        R2["é¿å…è·¨ sublane<br/>é€šä¿¡å¼€é”€"]
        R3["Pallas å¸ƒå±€è¦æ±‚"]
    end
    
    IDEAL -.->|"æ— æ³•ç›´æ¥å®ç°"| R1
    REAL --> R1
    REAL --> R2
    REAL --> R3
```

**è¿™ä¸æ˜¯æµªè´¹ç®—åŠ›å—ï¼Ÿ**

æ˜¯çš„ï¼ŒVPU ä¸Š 7/8 çš„ç®—åŠ›è¢«"æµªè´¹"äº†ã€‚ä½†æ˜¯ï¼š

1. **MXU æ‰æ˜¯ä¸»è§’**ï¼šQK^T å’Œ S@V å ç”¨ >90% è®¡ç®—æ—¶é—´
2. **VPU æ“ä½œå¾ˆå°‘**ï¼šmax/sum/exp2 åªæ˜¯é…è§’
3. **é€šä¿¡æˆæœ¬æ›´é«˜**ï¼šå¦‚æœåˆ†å¸ƒå­˜å‚¨ï¼Œéœ€è¦ sublane é—´é€šä¿¡

### ä¸ºä»€ä¹ˆç”¨ exp2 è€Œä¸æ˜¯ expï¼Ÿ

```python
# TPU VPU ç¡¬ä»¶ç‰¹æ€§ï¼š
# - exp2 (2^x): å•å‘¨æœŸç¡¬ä»¶æŒ‡ä»¤ âš¡
# - exp (e^x): éœ€è¦å¤šå‘¨æœŸè½¯ä»¶æ¨¡æ‹Ÿ ğŸ¢

# æ•°å­¦è½¬æ¢ï¼š
# exp(x) = e^x = 2^(x Ã— logâ‚‚(e)) = exp2(x Ã— 1.44269504)

# å®ç°æŠ€å·§ï¼šé¢„ä¹˜ LOG2_Eï¼Œé¿å…å†…æ ¸ä¸­çš„é¢å¤–ä¹˜æ³•
q = q * scale * LOG2_E  # åœ¨ kernel å¤–åšä¸€æ¬¡
s = jnp.exp2(qk - max)  # kernel å†…ç›´æ¥ç”¨ exp2
```

### å—å¤§å°ä¸ºä»€ä¹ˆæ˜¯ 3328 å’Œ 2816ï¼Ÿ

```
BQSIZE = 3328 = 26 Ã— 128
BKVSIZE = 2816 = 22 Ã— 128
```

1. **128 çš„å€æ•°**ï¼šé€‚é… MXU çš„ 128Ã—128 çŸ©é˜µä¹˜æ³•å•å…ƒ
2. **8 çš„å€æ•°**ï¼šé€‚é… VPU çš„ 8 ä¸ª sublane
3. **VMEM å®¹é‡é™åˆ¶**ï¼šå—å¤ªå¤§ä¼šæº¢å‡ºç‰‡ä¸Šå†…å­˜
4. **ç»éªŒè°ƒä¼˜**ï¼šè¿™äº›å€¼æ˜¯å®éªŒå¾—å‡ºçš„æœ€ä½³ç‚¹

### å¦‚ä½•è°ƒè¯• Pallas Kernelï¼Ÿ

```python
# è®¾ç½® interpret=True ä½¿ç”¨ Python è§£é‡Šæ‰§è¡Œ
splash_kernel = _make_splash_mha(block_sizes, bkv_compute_in, interpret=True)

# å¯ä»¥æ·»åŠ  print å’Œæ–­ç‚¹ï¼ˆä»… interpret æ¨¡å¼ï¼‰
```

---

## æ·±å…¥è§£æ `_splash_attention_forward`ï¼šPallas æ ¸å¿ƒå†™æ³•

> ğŸ¯ è¿™ä¸ªå‡½æ•°æ˜¯ç†è§£ Pallas ç¼–ç¨‹çš„æœ€ä½³èŒƒä¾‹ã€‚æŒæ¡å®ƒï¼Œå°±æŒæ¡äº† TPU kernel ç¼–å†™çš„ç²¾é«“ã€‚

### å‡½æ•°ç­¾åä¸å‚æ•°

```python
def _splash_attention_forward(
    q: jax.Array,           # Query, å½¢çŠ¶ (num_q_heads, q_seq_len, head_dim)
    k: jax.Array,           # Key,   å½¢çŠ¶ (num_kv_heads, kv_seq_len, head_dim)
    v: jax.Array,           # Value, å½¢çŠ¶ (num_kv_heads, kv_seq_len, head_dim)
    block_sizes: _BlockSizes,   # å—å¤§å°é…ç½®
    bkv_compute_in: int,        # KV è®¡ç®—å†…å±‚å—å¤§å°
    interpret: bool = False,    # è°ƒè¯•æ¨¡å¼
):
```

### å‡½æ•°ç»“æ„æ€»è§ˆ

```mermaid
graph TB
    subgraph "_splash_attention_forward å‡½æ•°ç»“æ„"
        A["1. æå–ç»´åº¦ä¿¡æ¯"] --> B["2. å®šä¹‰ index_map å‡½æ•°"]
        B --> C["3. å®šä¹‰ BlockSpecï¼ˆin_specsï¼‰"]
        C --> D["4. å®šä¹‰è¾“å‡ºå½¢çŠ¶ï¼ˆout_shapesï¼‰"]
        D --> E["5. å®šä¹‰è¾“å‡ºè§„æ ¼ï¼ˆout_specsï¼‰"]
        E --> F["6. è®¡ç®— grid ç»´åº¦"]
        F --> G["7. è°ƒç”¨ pl.pallas_call()"]
        G --> H["8. è¿”å›ç»“æœ"]
    end
```

---

### ç¬¬ä¸€éƒ¨åˆ†ï¼šæå–ç»´åº¦ä¿¡æ¯

```python
num_q_heads, q_seq_len, head_dim_qk = q.shape   # ä¾‹å¦‚ï¼š(8, 36864, 128)
head_dim_v = v.shape[-1]                         # ä¾‹å¦‚ï¼š128
bq, bkv = block_sizes.block_q, block_sizes.block_kv  # 3328, 2816
bkv_compute = block_sizes.block_kv_compute       # 256
num_kv_heads = k.shape[0]                        # GQA æ—¶å¯èƒ½ä¸ç­‰äº num_q_heads
kv_seq_len = k.shape[1]                          # ä¾‹å¦‚ï¼š36864
q_heads_per_kv_head = num_q_heads // num_kv_heads  # GQA æ¯”ä¾‹
```

**ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›ä¿¡æ¯ï¼Ÿ**
- `bq`, `bkv`ï¼šå†³å®šæ¯ä¸ª kernel å®ä¾‹å¤„ç†å¤šå¤§çš„æ•°æ®å—
- `num_q_heads`ï¼šå†³å®š grid çš„ç¬¬ä¸€ä¸ªç»´åº¦
- `q_seq_len // bq`ï¼šå†³å®š grid çš„ç¬¬äºŒä¸ªç»´åº¦
- `kv_seq_len // bkv`ï¼šå†³å®š grid çš„ç¬¬ä¸‰ä¸ªç»´åº¦

---

### ç¬¬äºŒéƒ¨åˆ†ï¼šindex_map å‡½æ•° â€”â€” Pallas çš„æ ¸å¿ƒæ¦‚å¿µï¼

**ä»€ä¹ˆæ˜¯ index_mapï¼Ÿ**

```
index_map æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå‘Šè¯‰ Pallasï¼š
"å½“ kernel åœ¨ grid ä½ç½® (h, i, j) æ‰§è¡Œæ—¶ï¼Œåº”è¯¥è¯»å–/å†™å…¥æ•°æ®çš„å“ªä¸ªä½ç½®ï¼Ÿ"
```

```mermaid
graph LR
    subgraph "Grid ç©ºé—´"
        G["(h, i, j)<br/>ä¾‹å¦‚ (2, 5, 3)"]
    end
    
    subgraph "index_map"
        MAP["æ˜ å°„å‡½æ•°"]
    end
    
    subgraph "æ•°æ®ç©ºé—´"
        D["(head, seq_start, dim_start)<br/>ä¾‹å¦‚ (2, 16640, 0)"]
    end
    
    G --> MAP --> D
```

#### q_index_mapï¼šQuery çš„ç´¢å¼•æ˜ å°„

```python
def q_index_map(h, i, j, *_):
    return (h, i, 0)
```

**è§£è¯»**ï¼š
```
è¾“å…¥ï¼šgrid ä½ç½® (h, i, j)
è¾“å‡ºï¼šQ çš„è¯»å–èµ·å§‹ä½ç½® (head, seq_block, dim)

h: ç¬¬ h ä¸ª head â†’ Q çš„ç¬¬ h ä¸ª head
i: ç¬¬ i ä¸ª Q å— â†’ Q åºåˆ—çš„ç¬¬ i ä¸ªå—ï¼ˆè‡ªåŠ¨ä¹˜ä»¥ bqï¼‰
j: ç¬¬ j ä¸ª KV å— â†’ å¯¹ Q æ²¡å½±å“ï¼ŒQ æ˜¯å›ºå®šçš„ï¼
0: head_dim ä» 0 å¼€å§‹ï¼ˆè¯»å–å®Œæ•´ç»´åº¦ï¼‰
```

```mermaid
graph TB
    subgraph "Q çš„è¯»å–æ¨¡å¼"
        direction LR
        Q_DATA["Q æ•°æ®<br/>(8, 36864, 128)"]
        
        subgraph "grid (h=2, i=5, j=ä»»æ„)"
            READ["è¯»å–ä½ç½®:<br/>head=2<br/>seq=5*3328=16640<br/>dim=0"]
        end
        
        Q_DATA --> READ
    end
    
    NOTE["æ³¨æ„ï¼šj å¯¹ Q æ²¡å½±å“ï¼<br/>åŒä¸€ä¸ª Q å—è¦ä¸æ‰€æœ‰ KV å—è®¡ç®—"]
```

#### k_index_map å’Œ v_index_mapï¼šKV çš„ç´¢å¼•æ˜ å°„

```python
def k_index_map(h, i, j, *_):
    return (h // q_heads_per_kv_head, j, 0)

def v_index_map(h, i, j, *_):
    return (h // q_heads_per_kv_head, j, 0)
```

**è§£è¯»**ï¼š
```
h // q_heads_per_kv_head: GQAï¼å¤šä¸ª Q head å…±äº«åŒä¸€ä¸ª KV head
j: ç¬¬ j ä¸ª KV å— â†’ è¯»å– KV åºåˆ—çš„ç¬¬ j ä¸ªå—
i: å¯¹ KV æ²¡å½±å“ï¼æ‰€æœ‰ Q å—éƒ½è¦çœ‹ç›¸åŒçš„ KV
0: head_dim ä» 0 å¼€å§‹
```

```mermaid
graph TB
    subgraph "GQA æ˜ å°„ç¤ºä¾‹ (8 Q heads, 4 KV heads)"
        Q0["Q head 0"] --> K01["KV head 0"]
        Q1["Q head 1"] --> K01
        Q2["Q head 2"] --> K23["KV head 1"]
        Q3["Q head 3"] --> K23
        Q4["Q head 4"] --> K45["KV head 2"]
        Q5["Q head 5"] --> K45
        Q6["Q head 6"] --> K67["KV head 3"]
        Q7["Q head 7"] --> K67
    end
    
    FORMULA["h // 2 = KV head index<br/>0,1â†’0  2,3â†’1  4,5â†’2  6,7â†’3"]
```

#### out_index_mapï¼šè¾“å‡ºçš„ç´¢å¼•æ˜ å°„

```python
def out_index_map(h, i, j, *_):
    return h, 0, i
```

**è§£è¯»**ï¼š
```
h: ç¬¬ h ä¸ª head â†’ å†™å…¥è¾“å‡ºçš„ç¬¬ h ä¸ª head
0: head_dim ä» 0 å¼€å§‹ï¼ˆå®Œæ•´ç»´åº¦ï¼‰
i: ç¬¬ i ä¸ª Q å— â†’ å†™å…¥è¾“å‡ºåºåˆ—çš„ç¬¬ i ä¸ªå—

æ³¨æ„ j ä¸å½±å“è¾“å‡ºä½ç½®ï¼å› ä¸ºæ‰€æœ‰ KV å—çš„ç»“æœç´¯ç§¯åˆ°åŒä¸€ä¸ªè¾“å‡ºä½ç½®
```

---

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šBlockSpec â€”â€” å®šä¹‰æ•°æ®åˆ‡ç‰‡æ–¹å¼

**ä»€ä¹ˆæ˜¯ BlockSpecï¼Ÿ**

```
BlockSpec = (block_shape, index_map)

å®ƒå‘Šè¯‰ Pallasï¼š
1. æ¯ä¸ª kernel å®ä¾‹å¤„ç†çš„æ•°æ®å—å½¢çŠ¶
2. å¦‚ä½•æ ¹æ® grid ä½ç½®æ‰¾åˆ°æ•°æ®å—
```

```python
in_specs = [
    pl.BlockSpec((None, bq, head_dim_qk), q_index_map),   # Q çš„ spec
    pl.BlockSpec((None, bkv, head_dim_qk), k_index_map),  # K çš„ spec
    pl.BlockSpec((None, bkv, head_dim_v), v_index_map),   # V çš„ spec
]
```

#### BlockSpec å½¢çŠ¶ä¸­çš„ None

```mermaid
graph TB
    subgraph "BlockSpec å½¢çŠ¶è§£è¯»"
        Q_SPEC["Q: (None, bq, head_dim)<br/>= (None, 3328, 128)"]
        K_SPEC["K: (None, bkv, head_dim)<br/>= (None, 2816, 128)"]
    end
    
    subgraph "None çš„å«ä¹‰"
        NONE["None è¡¨ç¤ºè¿™ä¸ªç»´åº¦ä¸åˆ‡ç‰‡<br/>index_map ç›´æ¥è¿”å›ç´¢å¼•ï¼ˆä¸ä¹˜ä»¥å—å¤§å°ï¼‰"]
        NUM["æ•°å­—è¡¨ç¤ºå—å¤§å°<br/>index_map è¿”å›çš„æ˜¯å—å·ï¼ˆè‡ªåŠ¨ä¹˜ä»¥å—å¤§å°ï¼‰"]
    end
    
    Q_SPEC --> NONE
    Q_SPEC --> NUM
```

**å…·ä½“ä¾‹å­**ï¼š
```python
# Q: å½¢çŠ¶ (8, 36864, 128), BlockSpec (None, 3328, 128)
# index_map è¿”å› (2, 5, 0) æ—¶ï¼š
#   - ç»´åº¦ 0 (None): ç›´æ¥ç”¨ 2 â†’ ç¬¬ 2 ä¸ª head
#   - ç»´åº¦ 1 (3328): 5 Ã— 3328 = 16640 â†’ ä»ä½ç½® 16640 å¼€å§‹
#   - ç»´åº¦ 2 (128): 0 Ã— 128 = 0 â†’ ä»ä½ç½® 0 å¼€å§‹
# ç»“æœï¼šè¯»å– q[2, 16640:16640+3328, 0:128]
```

---

### ç¬¬å››éƒ¨åˆ†ï¼šout_shapes å’Œ out_specs â€”â€” è¾“å‡ºè§„æ ¼

```python
out_shapes = [
    jax.ShapeDtypeStruct((NUM_SUBLANES, bq), jnp.float32),   # m_scratch
    jax.ShapeDtypeStruct((NUM_SUBLANES, bq), jnp.float32),   # l_scratch
    jax.ShapeDtypeStruct((head_dim_v, bq), jnp.float32),     # o_scratch
    jax.ShapeDtypeStruct((num_q_heads, head_dim_v, q_seq_len), q.dtype),  # æœ€ç»ˆè¾“å‡º
]

out_specs = [
    pl.BlockSpec((NUM_SUBLANES, bq), lambda *_: (0, 0)),  # scratch æ˜¯å±€éƒ¨çš„
    pl.BlockSpec((NUM_SUBLANES, bq), lambda *_: (0, 0)),
    pl.BlockSpec((head_dim_v, bq), lambda *_: (0, 0)),
    pl.BlockSpec((None, head_dim_v, bq), out_index_map),  # è¾“å‡ºå†™å…¥å…¨å±€ä½ç½®
]
```

```mermaid
graph TB
    subgraph "è¾“å‡ºç±»å‹åŒºåˆ†"
        SCRATCH["Scratchï¼ˆä¸´æ—¶å˜é‡ï¼‰<br/>m, l, o"]
        FINAL["æœ€ç»ˆè¾“å‡º<br/>attention result"]
    end
    
    subgraph "Scratch ç‰¹ç‚¹"
        S1["index_map å›ºå®šè¿”å› (0, 0)"]
        S2["æ¯ä¸ª grid cell æœ‰è‡ªå·±çš„å‰¯æœ¬"]
        S3["è·¨ j ç»´åº¦å…±äº«ï¼ˆåŒä¸€ Q å—ï¼‰"]
    end
    
    subgraph "æœ€ç»ˆè¾“å‡ºç‰¹ç‚¹"
        F1["index_map æ ¹æ® (h, i) å®šä½"]
        F2["å†™å…¥å…¨å±€è¾“å‡ºæ•°ç»„"]
        F3["åªåœ¨ j == grid_width-1 æ—¶å†™å…¥"]
    end
    
    SCRATCH --> S1
    SCRATCH --> S2
    SCRATCH --> S3
    
    FINAL --> F1
    FINAL --> F2
    FINAL --> F3
```

**ä¸ºä»€ä¹ˆ scratch çš„ index_map æ˜¯ `lambda *_: (0, 0)`ï¼Ÿ**

```
scratch æ˜¯æ¯ä¸ª grid cell çš„å±€éƒ¨å˜é‡ï¼Œä¸éœ€è¦å…¨å±€å®šä½
æ¯æ¬¡éƒ½ä» (0, 0) å¼€å§‹è¯»å†™è‡ªå·±çš„é‚£ä»½ scratch
Pallas ä¼šè‡ªåŠ¨ä¸ºæ¯ä¸ª (h, i) ç»„åˆåˆ†é…ç‹¬ç«‹çš„ scratch ç©ºé—´
```

---

### ç¬¬äº”éƒ¨åˆ†ï¼šGrid è®¾è®¡ â€”â€” å¹¶è¡Œæ‰§è¡Œçš„å…³é”®

```python
grid_width = kv_seq_len // bkv  # ä¾‹å¦‚ï¼š36864 // 2816 = 13
grid = (num_q_heads, q_seq_len // bq, grid_width)
# ä¾‹å¦‚ï¼š(8, 11, 13) = 8 Ã— 11 Ã— 13 = 1144 ä¸ª kernel å®ä¾‹
```

```mermaid
graph TB
    subgraph "Grid ä¸‰ç»´ç»“æ„"
        H["ç»´åº¦ 0: h<br/>num_q_heads = 8<br/>æ¯ä¸ª head ç‹¬ç«‹"]
        I["ç»´åº¦ 1: i<br/>q_seq_len // bq = 11<br/>æ¯ä¸ª Q å—ç‹¬ç«‹"]
        J["ç»´åº¦ 2: j<br/>kv_seq_len // bkv = 13<br/>éå†æ‰€æœ‰ KV å—"]
    end
    
    subgraph "è¯­ä¹‰"
        H --> |"parallel"| H_SEM["å¯ä»¥å¹¶è¡Œ<br/>head ä¹‹é—´æ— ä¾èµ–"]
        I --> |"arbitrary"| I_SEM["ä»»æ„é¡ºåº<br/>Q å—ä¹‹é—´æ— ä¾èµ–"]
        J --> |"arbitrary"| J_SEM["å¿…é¡»é¡ºåºï¼ˆé€»è¾‘ä¸Šï¼‰<br/>ä½†ç¼–è¯‘å™¨å¯ä¼˜åŒ–"]
    end
```

**ä¸ºä»€ä¹ˆ j ç»´åº¦æ˜¯ "arbitrary" è€Œä¸æ˜¯ "sequential"ï¼Ÿ**

```
è™½ç„¶ Online Softmax éœ€è¦æŒ‰é¡ºåºå¤„ç† KV å—ï¼Œ
ä½† Pallas ç¼–è¯‘å™¨çŸ¥é“ scratch å˜é‡çš„ä¾èµ–å…³ç³»ï¼Œ
ä¼šè‡ªåŠ¨ä¿è¯æ­£ç¡®çš„æ‰§è¡Œé¡ºåºã€‚

"arbitrary" ç»™ç¼–è¯‘å™¨æ›´å¤šä¼˜åŒ–ç©ºé—´ï¼Œ
æ¯”å¦‚é¢„å–ä¸‹ä¸€ä¸ª KV å—ã€‚
```

---

### ç¬¬å…­éƒ¨åˆ†ï¼špallas_call â€”â€” ä¸€åˆ‡çš„å…¥å£

```python
all_out = pl.pallas_call(
    # 1. Kernel å‡½æ•°ï¼ˆç”¨ partial å›ºå®šé™æ€å‚æ•°ï¼‰
    functools.partial(
        _flash_attention_kernel,
        mask_value=DEFAULT_MASK_VALUE,
        grid_width=grid_width,
        bq=bq,
        bkv=bkv,
        bkv_compute=bkv_compute,
        bkv_compute_in=bkv_compute_in,
        head_dim_v=head_dim_v,
    ),
    
    # 2. Grid è§„æ ¼
    grid_spec=pltpu.PrefetchScalarGridSpec(
        num_scalar_prefetch=0,
        in_specs=in_specs,
        out_specs=out_specs,
        grid=grid,
    ),
    
    # 3. ç¼–è¯‘å™¨å‚æ•°
    compiler_params=pltpu.CompilerParams(
        dimension_semantics=("parallel", "arbitrary", "arbitrary"),
        flags={"XLA_TPU_FORCE_LP_LLO_SCHEDULER": True}
    ),
    
    # 4. è¾“å‡ºå½¢çŠ¶
    out_shape=out_shapes,
    
    # 5. è°ƒè¯•æ¨¡å¼
    interpret=interpret,
)(q, k, v)  # 6. è¾“å…¥æ•°æ®
```

```mermaid
graph TB
    subgraph "pallas_call æ‰§è¡Œæµç¨‹"
        INPUT["è¾“å…¥: q, k, v"] --> GRID["ç”Ÿæˆ Grid<br/>(8, 11, 13)"]
        GRID --> SCHEDULE["è°ƒåº¦ Kernel å®ä¾‹<br/>1144 ä¸ªå¹¶è¡Œ/é¡ºåºæ‰§è¡Œ"]
        
        subgraph "æ¯ä¸ª Kernel å®ä¾‹"
            FETCH["æ ¹æ® index_map<br/>é¢„å–æ•°æ®å—"]
            EXEC["æ‰§è¡Œ kernel å‡½æ•°"]
            WRITE["å†™å›ç»“æœ"]
            FETCH --> EXEC --> WRITE
        end
        
        SCHEDULE --> FETCH
        WRITE --> OUTPUT["è¾“å‡º: all_out"]
    end
```

#### PrefetchScalarGridSpec è¯¦è§£

```python
pltpu.PrefetchScalarGridSpec(
    num_scalar_prefetch=0,  # ä¸éœ€è¦é¢„å–æ ‡é‡
    in_specs=in_specs,      # è¾“å…¥æ•°æ®çš„ BlockSpec
    out_specs=out_specs,    # è¾“å‡ºæ•°æ®çš„ BlockSpec
    grid=grid,              # Grid å½¢çŠ¶
)
```

**ä¸ºä»€ä¹ˆç”¨ PrefetchScalarGridSpec è€Œä¸æ˜¯æ™®é€š GridSpecï¼Ÿ**

```
PrefetchScalarGridSpec æ˜¯ TPU ä¸“ç”¨çš„ GridSpecï¼Œæ”¯æŒï¼š
1. ç¡¬ä»¶é¢„å–ï¼šè‡ªåŠ¨é¢„å–ä¸‹ä¸€ä¸ª grid cell çš„æ•°æ®
2. Double Bufferingï¼šå½“å‰è®¡ç®—å’Œé¢„å–å¹¶è¡Œ
3. æ›´å¥½çš„å†…å­˜ç®¡ç†ï¼šTPU çš„ VMEM åˆ†é…ä¼˜åŒ–
```

#### compiler_params è¯¦è§£

```python
compiler_params=pltpu.CompilerParams(
    dimension_semantics=("parallel", "arbitrary", "arbitrary"),
    flags={"XLA_TPU_FORCE_LP_LLO_SCHEDULER": True}
)
```

**dimension_semantics å«ä¹‰**ï¼š

| å€¼ | å«ä¹‰ | é€‚ç”¨åœºæ™¯ |
|---|------|---------|
| `"parallel"` | å®Œå…¨å¹¶è¡Œï¼Œæ— ä¾èµ– | ç‹¬ç«‹çš„ head |
| `"arbitrary"` | ç¼–è¯‘å™¨è‡ªç”±è°ƒåº¦ | Q å—ã€KV å— |
| `"sequential"` | ä¸¥æ ¼é¡ºåºæ‰§è¡Œ | æœ‰å¼ºä¾èµ–æ—¶ä½¿ç”¨ |

**flags å«ä¹‰**ï¼š

```
XLA_TPU_FORCE_LP_LLO_SCHEDULER: True
â†’ å¼ºåˆ¶ä½¿ç”¨ä½å»¶è¿Ÿè°ƒåº¦å™¨
â†’ ä¼˜åŒ–å° kernel çš„å¯åŠ¨å»¶è¿Ÿ
â†’ å¯¹ attention è¿™ç§è®¡ç®—å¯†é›†å‹æœ‰å¸®åŠ©
```

---

### ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå®Œæ•´æ•°æ®æµå›¾

```mermaid
flowchart TB
    subgraph "è¾“å…¥å±‚"
        Q["Q<br/>(8, 36864, 128)"]
        K["K<br/>(4, 36864, 128)"]
        V["V<br/>(4, 36864, 128)"]
    end
    
    subgraph "Grid è°ƒåº¦å±‚"
        GRID["Grid (8, 11, 13)<br/>1144 ä¸ª kernel å®ä¾‹"]
    end
    
    subgraph "Kernel å®ä¾‹ (h=2, i=5, j=3)"
        Q_BLOCK["q_ref<br/>(3328, 128)"]
        K_BLOCK["k_ref<br/>(2816, 128)"]
        V_BLOCK["v_ref<br/>(2816, 128)"]
        
        QK["QK = K @ Q^T<br/>(2816, 3328)"]
        ONLINE["Online Softmax<br/>æ›´æ–° (m, l, o)"]
        SV["O += S @ V"]
    end
    
    subgraph "è¾“å‡ºå±‚"
        SCRATCH["scratch<br/>m, l, o"]
        OUTPUT["æœ€ç»ˆè¾“å‡º<br/>(8, 128, 36864)"]
    end
    
    Q --> |"q_index_map<br/>(2, 5, 0)"| Q_BLOCK
    K --> |"k_index_map<br/>(1, 3, 0)"| K_BLOCK
    V --> |"v_index_map<br/>(1, 3, 0)"| V_BLOCK
    
    Q_BLOCK --> QK
    K_BLOCK --> QK
    QK --> ONLINE
    ONLINE --> SV
    V_BLOCK --> SV
    
    SV --> SCRATCH
    SCRATCH --> |"j == 12 æ—¶<br/>å½’ä¸€åŒ–è¾“å‡º"| OUTPUT
```

---

### ç¬¬å…«éƒ¨åˆ†ï¼šä»£ç ä¸ç¡¬ä»¶çš„æ˜ å°„

```mermaid
graph LR
    subgraph "ä»£ç å±‚"
        QK_CODE["lax.dot_general(k, q, ...)"]
        EXP_CODE["jnp.exp2(qk - m)"]
        SV_CODE["lax.dot_general(v, s, ...)"]
    end
    
    subgraph "XLA ç¼–è¯‘"
        XLA["XLA TPU ç¼–è¯‘å™¨"]
    end
    
    subgraph "ç¡¬ä»¶å±‚"
        MXU["MXU<br/>128Ã—128 çŸ©é˜µä¹˜æ³•"]
        VPU["VPU<br/>å‘é‡è¿ç®—<br/>8 sublanes"]
        VMEM["VMEM<br/>ç‰‡ä¸Šç¼“å­˜"]
        HBM["HBM<br/>é«˜å¸¦å®½å†…å­˜"]
    end
    
    QK_CODE --> XLA --> MXU
    EXP_CODE --> XLA --> VPU
    SV_CODE --> XLA --> MXU
    
    HBM <--> |"BlockSpec<br/>æ§åˆ¶ä¼ è¾“"| VMEM
    VMEM --> MXU
    VMEM --> VPU
```

---

### ç¬¬ä¹éƒ¨åˆ†ï¼šå¸¸è§ Pallas æ¨¡å¼æ€»ç»“

| æ¨¡å¼ | ä»£ç ç¤ºä¾‹ | ç”¨é€” |
|------|---------|------|
| **é™æ€å‚æ•°ä¼ é€’** | `functools.partial(kernel, param=value)` | ç¼–è¯‘æ—¶å›ºå®šå‚æ•° |
| **æ¡ä»¶æ‰§è¡Œ** | `@pl.when(condition)` | åªåœ¨ç‰¹å®š grid ä½ç½®æ‰§è¡Œ |
| **Scratch å˜é‡** | `out_shapes` ä¸­å®šä¹‰ | è·¨ grid ç»´åº¦ç´¯ç§¯ç»“æœ |
| **index_map å¿½ç•¥ç»´åº¦** | `lambda h, i, j: (h, i, 0)` | æŸäº›ç»´åº¦ä¸å½±å“æ•°æ®ä½ç½® |
| **GQA æ˜ å°„** | `h // q_heads_per_kv_head` | å¤š Q head å…±äº« KV |
| **ç»´åº¦è¯­ä¹‰** | `dimension_semantics` | å‘Šè¯‰ç¼–è¯‘å™¨å¹¶è¡Œæ€§ |

---

### ç¬¬åéƒ¨åˆ†ï¼šå¦‚ä½•å†™è‡ªå·±çš„ Pallas Kernelï¼Ÿ

**æ­¥éª¤æ¨¡æ¿**ï¼š

```python
def my_pallas_forward(x, y, block_size):
    # 1. æå–ç»´åº¦
    batch, seq, dim = x.shape
    
    # 2. å®šä¹‰ index_map
    def x_index_map(b, s):
        return (b, s, 0)
    
    def y_index_map(b, s):
        return (b, s, 0)
    
    # 3. å®šä¹‰ BlockSpec
    in_specs = [
        pl.BlockSpec((None, block_size, dim), x_index_map),
        pl.BlockSpec((None, block_size, dim), y_index_map),
    ]
    
    # 4. å®šä¹‰è¾“å‡º
    out_shape = jax.ShapeDtypeStruct((batch, seq, dim), x.dtype)
    out_spec = pl.BlockSpec((None, block_size, dim), x_index_map)
    
    # 5. å®šä¹‰ grid
    grid = (batch, seq // block_size)
    
    # 6. å®šä¹‰ kernel
    def kernel(x_ref, y_ref, o_ref):
        o_ref[...] = x_ref[...] + y_ref[...]
    
    # 7. è°ƒç”¨ pallas_call
    return pl.pallas_call(
        kernel,
        grid_spec=pltpu.PrefetchScalarGridSpec(
            num_scalar_prefetch=0,
            in_specs=in_specs,
            out_specs=[out_spec],
            grid=grid,
        ),
        out_shape=[out_shape],
    )(x, y)
```

---

## æ€»ç»“

```mermaid
mindmap
    root((Splash Attention))
        æ ¸å¿ƒæ€æƒ³
            åˆ†å—è®¡ç®—
            åœ¨çº¿ Softmax
            å†…å­˜ O-N- è€Œé O-NÂ²-
        TPU ä¼˜åŒ–
            exp2 ç¡¬ä»¶æŒ‡ä»¤
            MXU çŸ©é˜µä¹˜æ³•
            VMEM ç‰‡ä¸Šç¼“å­˜
        åˆ†å¸ƒå¼
            Head Parallel
            Sequence Parallel
            shard_map
        æƒè¡¡
            é•¿åºåˆ—é«˜æ•ˆ
            å†…å­˜èŠ‚çœ
            sublane å†—ä½™
            ä»£ç å¤æ‚
```

---

## å‚è€ƒèµ„æ–™

- [Flash Attention è®ºæ–‡](https://arxiv.org/abs/2205.14135) - Tri Dao et al.
- [Flash Attention 2 è®ºæ–‡](https://arxiv.org/abs/2307.08691) - Tri Dao
- [JAX Pallas æ–‡æ¡£](https://jax.readthedocs.io/en/latest/pallas/)
- [TPU ç³»ç»Ÿæ¶æ„](https://cloud.google.com/tpu/docs/system-architecture)

---

*æœ€åæ›´æ–°: 2024-12*
