/home/chrisya/.local/lib/python3.12/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.
Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.
/home/chrisya/gpu-tpu-pedia/tpu/CogVideoX/generate_flax.py:20: DeprecationWarning: jax.experimental.shard_map is deprecated in v0.8.0. Used jax.shard_map instead.
  from jax.experimental.shard_map import shard_map
æ³¨å†ŒPyTreeèŠ‚ç‚¹...
  - BaseModelOutputWithPooling å·²æ³¨å†Œ
  - BaseModelOutputWithPastAndCrossAttentions å·²æ³¨å†Œ
  - DecoderOutput å·²æ³¨å†Œ
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 83.81it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:01,  2.27it/s][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  3.28it/s][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  3.77it/s][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  4.26it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.79it/s]
Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.60it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.14it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.74it/s]
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.

é…ç½®Pipelineä»¥ä½¿ç”¨JAXã€Splash Attention å’Œ Flax VAE...

é…ç½®Pipelineä»¥ä½¿ç”¨JAX...
  Mesh ç»´åº¦: tp_dim=8, dp_dim=1, sp_dim=1
- æ³¨å†Œ Custom Splash Attention (exp2)ï¼ˆçª—å£å¤§å°: Noneï¼‰...
å°†schedulerå‚æ•°ç§»åŠ¨åˆ°JAXè®¾å¤‡...
- å°†Text Encoderç§»åˆ°XLAå¹¶è¿›è¡Œåˆ†ç‰‡...
- åŠ è½½ Flax VAE (åŽŸç”Ÿ JAX å®žçŽ°)...
[1/4] åŠ è½½é…ç½®: zai-org/CogVideoX1.5-5B
[2/4] ä¸‹è½½ PyTorch æƒé‡...
[3/4] è½¬æ¢æƒé‡åˆ° JAX æ ¼å¼...
  âœ“ åŠ è½½äº† 436 ä¸ª PyTorch æƒé‡å¼ é‡
  âœ“ è½¬æ¢äº† 436 ä¸ªæƒé‡åˆ° JAX æ ¼å¼
[4/4] åˆå§‹åŒ–æ¨¡åž‹å¹¶åŠ è½½æƒé‡...
âœ“ æ¨¡åž‹åŠ è½½å®Œæˆ!
  é…ç½®: FlaxAutoencoderKLCogVideoXConfig(config_name='config.json', in_channels=3, out_channels=3, down_block_types=['CogVideoXDownBlock3D', 'CogVideoXDownBlock3D', 'CogVideoXDownBlock3D', 'CogVideoXDownBlock3D'], up_block_types=['CogVideoXUpBlock3D', 'CogVideoXUpBlock3D', 'CogVideoXUpBlock3D', 'CogVideoXUpBlock3D'], block_out_channels=[128, 256, 256, 512], latent_channels=16, layers_per_block=3, act_fn='silu', norm_eps=1e-06, norm_num_groups=32, temporal_compression_ratio=4, sample_height=480, sample_width=720, scaling_factor=0.7, shift_factor=None, latents_mean=None, latents_std=None, force_upcast=True, use_quant_conv=False, use_post_quant_conv=False, pad_mode='first')
- è·³è¿‡ VAE Tiling é…ç½®ï¼ˆæµ‹è¯•å®Œæ•´è§£ç ï¼‰...
  âœ“ Flax VAE å·²æ›¿æ¢ Pipeline çš„ VAEï¼ˆæ— ä¸å½“ JITï¼‰
Pipelineé…ç½®å®Œæˆ

è¿è¡Œ2æ¬¡è§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•...
æç¤ºè¯: 'A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.'
æŽ¨ç†æ­¥æ•°: 10
è§†é¢‘å¸§æ•°: 81
åˆ†è¾¨çŽ‡: 768x1360
å¼•å¯¼å°ºåº¦: 6.0

è¿­ä»£ 0 (åŒ…å« JIT ç¼–è¯‘ï¼Œä¼šæ¯”è¾ƒæ…¢):
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [01:06<09:59, 66.57s/it] 20%|â–ˆâ–ˆ        | 2/10 [02:16<09:07, 68.38s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [02:20<04:32, 38.94s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [02:24<02:30, 25.10s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [02:27<01:27, 17.45s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [02:31<00:51, 12.84s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:35<00:29,  9.92s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:39<00:16,  8.00s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:43<00:06,  6.72s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00,  5.86s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:47<00:00, 16.74s/it]
/home/chrisya/torchax/torchax/ops/mappings.py:84: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)
  res = torch.from_numpy(numpy.asarray(x))
  å®Œæˆæ—¶é—´: 196.38 ç§’ (åŒ…å« Transformer + Text Encoder çš„çœŸæ­£ JIT ç¼–è¯‘)

è¿­ä»£ 1 (ä½¿ç”¨å·²ç¼–è¯‘ä»£ç ):
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:04<00:41,  4.66s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:08<00:33,  4.21s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:28,  4.07s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:23,  4.00s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:20<00:19,  3.96s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:24<00:15,  3.94s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:28<00:11,  3.93s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:31<00:07,  3.92s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:35<00:03,  3.91s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.91s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.97s/it]
Traceback (most recent call last):
  File "/home/chrisya/gpu-tpu-pedia/tpu/CogVideoX/generate_flax.py", line 947, in <module>
    main()
  File "/home/chrisya/gpu-tpu-pedia/tpu/CogVideoX/generate_flax.py", line 923, in main
    frames, times = run_generation_benchmark(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/gpu-tpu-pedia/tpu/CogVideoX/generate_flax.py", line 851, in run_generation_benchmark
    result = pipe(prompt, num_inference_steps=num_inference_steps, num_frames=num_frames, height=height, width=width, guidance_scale=guidance_scale)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/pipelines/cogvideo/pipeline_cogvideox.py", line 778, in __call__
    video = self.decode_latents(latents)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/pipelines/cogvideo/pipeline_cogvideox.py", line 355, in decode_latents
    frames = self.vae.decode(latents).sample
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/gpu-tpu-pedia/tpu/CogVideoX/generate_flax.py", line 795, in decode
    frames_jax = self._flax_vae.decode(latents_jax)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/models/autoencoders/autoencoder_kl_cogvideox_flax.py", line 2021, in decode
    decoded = self._decode(z, zq, deterministic=deterministic)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/models/autoencoders/autoencoder_kl_cogvideox_flax.py", line 1966, in _decode
    decoded_frame, _ = self.decoder(
                       ^^^^^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/models/autoencoders/autoencoder_kl_cogvideox_flax.py", line 1592, in __call__
    hidden_states, _ = up_block(
                       ^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/models/autoencoders/autoencoder_kl_cogvideox_flax.py", line 1195, in __call__
    hidden_states, _ = resnet(
                       ^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/models/autoencoders/autoencoder_kl_cogvideox_flax.py", line 797, in __call__
    return self._call_with_feat_cache(inputs, temb, zq, feat_cache, feat_idx, deterministic)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/models/autoencoders/autoencoder_kl_cogvideox_flax.py", line 816, in _call_with_feat_cache
    hidden_states, _ = self.norm1(hidden_states, zq, feat_cache=feat_cache, feat_idx=feat_idx)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/models/autoencoders/autoencoder_kl_cogvideox_flax.py", line 584, in __call__
    return self._call_with_feat_cache(f, zq, feat_cache, feat_idx)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/models/autoencoders/autoencoder_kl_cogvideox_flax.py", line 621, in _call_with_feat_cache
    norm_f = self.norm_layer(f)
             ^^^^^^^^^^^^^^^^^^
  File "/home/chrisya/diffusers-tpu/src/diffusers/models/autoencoders/autoencoder_kl_cogvideox_flax.py", line 495, in __call__
    x_out = x_norm * self.scale.value.reshape(1, 1, 1, 1, C) + self.bias.value.reshape(1, 1, 1, 1, C)
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  File "/home/chrisya/.local/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py", line 608, in deferring_binary_op
    return binary_op(*args)
           ^^^^^^^^^^^^^^^^
  File "/home/chrisya/.local/lib/python3.12/site-packages/jax/_src/numpy/ufunc_api.py", line 182, in __call__
    return call(*args)
           ^^^^^^^^^^^
ValueError: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 1.99G. That was not possible. There are 1.03G free.; (0x0x0_HBM0)
