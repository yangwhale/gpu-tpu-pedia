apiVersion: v1
kind: Service
metadata:
  name: gpu-1
spec:
  selector:
    name: gpu-1
  clusterIP: None
---
apiVersion: v1
kind: Pod
metadata:
  name: gpu-1
  labels:
    name: gpu-1
  annotations:
    networking.gke.io/default-interface: 'eth0'
    networking.gke.io/interfaces: |
      [
        {"interfaceName":"eth0","network":"default"},
        {"interfaceName":"eth2","network":"rdma-0"},
        {"interfaceName":"eth3","network":"rdma-1"},
        {"interfaceName":"eth4","network":"rdma-2"},
        {"interfaceName":"eth5","network":"rdma-3"},
        {"interfaceName":"eth6","network":"rdma-4"},
        {"interfaceName":"eth7","network":"rdma-5"},
        {"interfaceName":"eth8","network":"rdma-6"},
        {"interfaceName":"eth9","network":"rdma-7"}
      ]
    kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
    gke-gcsfuse/volumes: "true"
    gke-gcsfuse/cpu-limit: "0"
    gke-gcsfuse/memory-limit: "0"
    gke-gcsfuse/ephemeral-storage-limit: "0"
spec:
  hostNetwork: false
  hostPID: false
  serviceAccount: workload-identity-k8s-sa
  nodeSelector:
    cloud.google.com/gke-nodepool: a4-highgpu-cos
  volumes:
    - name: local-ssd
      hostPath:
        path: /mnt/stateful_partition/kube-ephemeral-ssd
    - name: library-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: gib
      hostPath:
        path: /home/kubernetes/bin/gib
    - name: shared-memory
      emptyDir:
        medium: "Memory"
        sizeLimit: 250Gi
    # - name: data-volume
    #   persistentVolumeClaim:
    #     claimName: gcs-pvc
    # - name: cfs-volume
    #   persistentVolumeClaim:
    #     claimName: cfs-pvc
  containers:
    # - image: nvcr.io/nvidia/pytorch:25.01-py3
    # - image: nvcr.io/nvidia/pytorch:25.06-py3
    - image: us-central1-docker.pkg.dev/gpu-launchpad-playground/hzchen-poc/wxg-poc:0.2
    # - image: docker.io/pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel
      name: ngc-25-04
      resources:
        requests:
          # cpu: 32
          nvidia.com/gpu: "8"
        limits:
          # cpu: 32
          nvidia.com/gpu: "8"
      securityContext:
       privileged: true
      volumeMounts:
        - name: local-ssd
          mountPath: /scratch-data
        - name: library-dir-host
          mountPath: /usr/local/nvidia
        - name: gib
          mountPath: /usr/local/gib
        - name: shared-memory
          mountPath: /dev/shm
        # - name: data-volume
        #   mountPath: /data
        # - name: cfs-volume
        #   mountPath: /cfs
      env:
        # - name: LD_LIBRARY_PATH
        #   value: /usr/local/nvidia/lib64
        - name: N_NODES
          value: "2"
      command: ["/bin/bash", "-c"]
      args:
        - |
          cp -R /dev/shm/scripts /
          cp -R /dev/shm/diagnostic /
          cp -R /dev/shm/third_party /

          export N_NODES=${N_NODES}
          export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:${LD_LIBRARY_PATH}

          # Load all the cuda libs
          /sbin/ldconfig

          # Install ping
          apt update -y
          apt install -y iputils-ping openssh-server iproute2 pciutils curl

          # Install gcloud sdk
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | \
            tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
            curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | \
            gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg && \
            apt-get update -y && \
            apt-get install google-cloud-cli -y

          # Enable persistence
          for i in $(seq 0 7)
          do
            nvidia-smi -i $i -pm ENABLED
          done

          # Setup ssh
          mkdir -p /root/.ssh/
          # Note: SSH keys need to be provided via ConfigMap or Secret
          # cp /data/id_rsa* /data/authorized_keys /root/.ssh/
          # chmod 400 /root/.ssh/id_rsa
          ssh-keygen -t rsa -N "" -f /root/.ssh/id_rsa
          cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
          chmod 400 /root/.ssh/id_rsa
          sed -i 's/^#PermitRootLogin prohibit-password/PermitRootLogin prohibit-password/' /etc/ssh/sshd_config
          # echo "Port 222" | tee -a /etc/ssh/sshd_config > /dev/null
          service ssh restart

          # Setup variables
          echo "unset NCCL_NVLS_ENABLE TORCH_NCCL_USE_COMM_NONBLOCKING" | tee -a /root/.bashrc > /dev/null
          echo "export LD_LIBRARY_PATH=/usr/local/gib/lib64:${LD_LIBRARY_PATH}" | tee -a /root/.bashrc > /dev/null
          echo "source /usr/local/gib/scripts/set_nccl_env.sh" | tee -a /root/.bashrc > /dev/null

          #
          python -m pip install --upgrade pip
          # pip install -U deepspeed sentencepiece peft "huggingface_hub[cli]"
          # pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129
          # pip install -U --no-build-isolation transformer_engine[pytorch] flash_attn==2.8.1

          # Get helper variables to form all hostnames
          export POSTFIX=$(hostname | cut -d . -f 2-)
          export WORKERS_BASENAME=$(hostname | cut -d . -f 1 | rev | cut -d - -f 2- | rev )

          # For every worker, wait till online and add to hostfile
          for i in `seq 1 $(($N_NODES))`; do
            OTHER=${WORKERS_BASENAME}-${i}
            until ssh -o StrictHostKeyChecking=no $OTHER hostname; do
              echo Waiting for ${OTHER}...
              sleep 10
            done
            # echo ${OTHER} port=222 slots=8 | tee -a /tmp/hostfile;
            echo ${OTHER} slots=8 | tee -a /tmp/hostfile;
          done

          cd /workspace
          git clone https://github.com/LinHeLurking/megatron-bench.git
          #hf download Salesforce/wikitext --repo-type=dataset --local-dir="./wikitext"

          sleep infinity
  initContainers:
    - name: nccl-plugin-installer
      image: us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib-diagnostic:v1.1.0
      imagePullPolicy: Always
      args:
      - |
        set -ex
        /scripts/container_entry.sh install --install-nccl
        cp -R /var/lib/gib/lib64/. /target/usr/local/gib/lib64
        cp -R /var/lib/gib/. /target/usr/local/gib
        cp -R /scripts /target/dev/shm
        cp -R /diagnostic /target/dev/shm
        cp -R /third_party /target/dev/shm
      command:
      - /bin/sh
      - -c
      volumeMounts:
      - mountPath: /target/usr/local/gib
        name: gib
      - mountPath: /target/dev/shm
        name: shared-memory
---
apiVersion: v1
kind: Service
metadata:
  name: gpu-2
spec:
  selector:
    name: gpu-2
  clusterIP: None
---
apiVersion: v1
kind: Pod
metadata:
  name: gpu-2
  labels:
    name: gpu-2
  annotations:
    networking.gke.io/default-interface: 'eth0'
    networking.gke.io/interfaces: |
      [
        {"interfaceName":"eth0","network":"default"},
        {"interfaceName":"eth2","network":"rdma-0"},
        {"interfaceName":"eth3","network":"rdma-1"},
        {"interfaceName":"eth4","network":"rdma-2"},
        {"interfaceName":"eth5","network":"rdma-3"},
        {"interfaceName":"eth6","network":"rdma-4"},
        {"interfaceName":"eth7","network":"rdma-5"},
        {"interfaceName":"eth8","network":"rdma-6"},
        {"interfaceName":"eth9","network":"rdma-7"}
      ]
    kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
    gke-gcsfuse/volumes: "true"
    gke-gcsfuse/cpu-limit: "0"
    gke-gcsfuse/memory-limit: "0"
    gke-gcsfuse/ephemeral-storage-limit: "0"
spec:
  hostNetwork: false
  hostPID: false
  serviceAccount: workload-identity-k8s-sa
  nodeSelector:
    cloud.google.com/gke-nodepool: a4-highgpu-cos
  volumes:
    - name: local-ssd
      hostPath:
        path: /mnt/stateful_partition/kube-ephemeral-ssd
    - name: library-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: gib
      hostPath:
        path: /home/kubernetes/bin/gib
    - name: shared-memory
      emptyDir:
        medium: "Memory"
        sizeLimit: 250Gi
    # - name: data-volume
    #   persistentVolumeClaim:
    #     claimName: gcs-pvc
    # - name: cfs-volume
    #   persistentVolumeClaim:
    #     claimName: cfs-pvc
  containers:
    # - image: nvcr.io/nvidia/pytorch:25.01-py3
    # - image: nvcr.io/nvidia/pytorch:25.06-py3
    - image: us-central1-docker.pkg.dev/gpu-launchpad-playground/hzchen-poc/wxg-poc:0.2
    # - image: docker.io/pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel
      name: ngc-25-04
      resources:
        requests:
          # cpu: 32
          nvidia.com/gpu: "8"
        limits:
          # cpu: 32
          nvidia.com/gpu: "8"
      securityContext:
       privileged: true
      volumeMounts:
        - name: local-ssd
          mountPath: /scratch-data
        - name: library-dir-host
          mountPath: /usr/local/nvidia
        - name: gib
          mountPath: /usr/local/gib
        - name: shared-memory
          mountPath: /dev/shm
        # - name: data-volume
        #   mountPath: /data
        # - name: cfs-volume
        #   mountPath: /cfs
      env:
        # - name: LD_LIBRARY_PATH
        #   value: /usr/local/nvidia/lib64
        - name: N_NODES
          value: "2"
      command: ["/bin/bash", "-c"]
      args:
        - |
          cp -R /dev/shm/scripts /
          cp -R /dev/shm/diagnostic /
          cp -R /dev/shm/third_party /

          export N_NODES=${N_NODES}
          export LD_LIBRARY_PATH=/usr/local/nvidia/lib64:${LD_LIBRARY_PATH}

          # Load all the cuda libs
          /sbin/ldconfig

          # Install ping
          apt update -y
          apt install -y iputils-ping openssh-server iproute2 pciutils curl

          # Install gcloud sdk
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | \
            tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
            curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | \
            gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg && \
            apt-get update -y && \
            apt-get install google-cloud-cli -y

          # Enable persistence
          for i in $(seq 0 7)
          do
            nvidia-smi -i $i -pm ENABLED
          done

          # Setup ssh
          mkdir -p /root/.ssh/
          # Note: SSH keys need to be provided via ConfigMap or Secret
          # cp /data/id_rsa* /data/authorized_keys /root/.ssh/
          # chmod 400 /root/.ssh/id_rsa
          ssh-keygen -t rsa -N "" -f /root/.ssh/id_rsa
          cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
          chmod 400 /root/.ssh/id_rsa
          sed -i 's/^#PermitRootLogin prohibit-password/PermitRootLogin prohibit-password/' /etc/ssh/sshd_config
          # echo "Port 222" | tee -a /etc/ssh/sshd_config > /dev/null
          service ssh restart

          # Setup variables
          echo "unset NCCL_NVLS_ENABLE TORCH_NCCL_USE_COMM_NONBLOCKING" | tee -a /root/.bashrc > /dev/null
          echo "export LD_LIBRARY_PATH=/usr/local/gib/lib64:${LD_LIBRARY_PATH}" | tee -a /root/.bashrc > /dev/null
          echo "source /usr/local/gib/scripts/set_nccl_env.sh" | tee -a /root/.bashrc > /dev/null

          #
          python -m pip install --upgrade pip
          # pip install -U deepspeed sentencepiece peft "huggingface_hub[cli]"
          # pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129
          # pip install -U --no-build-isolation transformer_engine[pytorch] flash_attn==2.8.1

          # Get helper variables to form all hostnames
          export POSTFIX=$(hostname | cut -d . -f 2-)
          export WORKERS_BASENAME=$(hostname | cut -d . -f 1 | rev | cut -d - -f 2- | rev )

          # For every worker, wait till online and add to hostfile
          for i in `seq 1 $(($N_NODES))`; do
            OTHER=${WORKERS_BASENAME}-${i}
            until ssh -o StrictHostKeyChecking=no $OTHER hostname; do
              echo Waiting for ${OTHER}...
              sleep 10
            done
            # echo ${OTHER} port=222 slots=8 | tee -a /tmp/hostfile;
            echo ${OTHER} slots=8 | tee -a /tmp/hostfile;
          done

          cd /workspace
          git clone https://github.com/LinHeLurking/megatron-bench.git
          #hf download Salesforce/wikitext --repo-type=dataset --local-dir="./wikitext"

          sleep infinity
  initContainers:
    - name: nccl-plugin-installer
      image: us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib-diagnostic:v1.1.0
      imagePullPolicy: Always
      args:
      - |
        set -ex
        /scripts/container_entry.sh install --install-nccl
        cp -R /var/lib/gib/lib64/. /target/usr/local/gib/lib64
        cp -R /var/lib/gib/. /target/usr/local/gib
        cp -R /scripts /target/dev/shm
        cp -R /diagnostic /target/dev/shm
        cp -R /third_party /target/dev/shm
      command:
      - /bin/sh
      - -c
      volumeMounts:
      - mountPath: /target/usr/local/gib
        name: gib
      - mountPath: /target/dev/shm
        name: shared-memory