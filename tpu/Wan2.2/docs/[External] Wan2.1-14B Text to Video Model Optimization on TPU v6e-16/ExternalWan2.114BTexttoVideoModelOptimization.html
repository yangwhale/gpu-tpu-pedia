<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=DFQxm4rd7fRHgM9OTejWVYbTZVi_Y5F9JqoQGMBQEYTYnt9737GpWhMUMfToBGM3XGMkxXUZTA64h2imyzu79g);ul.lst-kix_e45mjayykp7f-7{list-style-type:none}ul.lst-kix_e45mjayykp7f-8{list-style-type:none}ol.lst-kix_9w5yoflww7o0-7.start{counter-reset:lst-ctn-kix_9w5yoflww7o0-7 0}.lst-kix_4jhw1c1a6bfe-3>li{counter-increment:lst-ctn-kix_4jhw1c1a6bfe-3}ul.lst-kix_1od86ppq6p3l-3{list-style-type:none}ul.lst-kix_e45mjayykp7f-0{list-style-type:none}ul.lst-kix_1od86ppq6p3l-4{list-style-type:none}ul.lst-kix_e45mjayykp7f-1{list-style-type:none}ul.lst-kix_1od86ppq6p3l-1{list-style-type:none}ul.lst-kix_e45mjayykp7f-2{list-style-type:none}ul.lst-kix_1od86ppq6p3l-2{list-style-type:none}ul.lst-kix_e45mjayykp7f-3{list-style-type:none}ul.lst-kix_1od86ppq6p3l-7{list-style-type:none}ul.lst-kix_e45mjayykp7f-4{list-style-type:none}ul.lst-kix_1od86ppq6p3l-8{list-style-type:none}.lst-kix_9w5yoflww7o0-6>li{counter-increment:lst-ctn-kix_9w5yoflww7o0-6}ul.lst-kix_e45mjayykp7f-5{list-style-type:none}ul.lst-kix_1od86ppq6p3l-5{list-style-type:none}ul.lst-kix_e45mjayykp7f-6{list-style-type:none}ul.lst-kix_1od86ppq6p3l-6{list-style-type:none}.lst-kix_9w5yoflww7o0-5>li{counter-increment:lst-ctn-kix_9w5yoflww7o0-5}.lst-kix_9w5yoflww7o0-8>li{counter-increment:lst-ctn-kix_9w5yoflww7o0-8}ol.lst-kix_4jhw1c1a6bfe-2.start{counter-reset:lst-ctn-kix_4jhw1c1a6bfe-2 0}.lst-kix_4jhw1c1a6bfe-4>li{counter-increment:lst-ctn-kix_4jhw1c1a6bfe-4}ul.lst-kix_1od86ppq6p3l-0{list-style-type:none}.lst-kix_4jhw1c1a6bfe-1>li{counter-increment:lst-ctn-kix_4jhw1c1a6bfe-1}.lst-kix_4jhw1c1a6bfe-6>li{counter-increment:lst-ctn-kix_4jhw1c1a6bfe-6}.lst-kix_4jhw1c1a6bfe-0>li{counter-increment:lst-ctn-kix_4jhw1c1a6bfe-0}.lst-kix_9w5yoflww7o0-4>li{counter-increment:lst-ctn-kix_9w5yoflww7o0-4}ul.lst-kix_9w5yoflww7o0-0{list-style-type:none}.lst-kix_9w5yoflww7o0-8>li:before{content:"" counter(lst-ctn-kix_9w5yoflww7o0-8,lower-roman) ". "}ol.lst-kix_9w5yoflww7o0-5.start{counter-reset:lst-ctn-kix_9w5yoflww7o0-5 0}.lst-kix_9w5yoflww7o0-5>li:before{content:"" counter(lst-ctn-kix_9w5yoflww7o0-5,lower-roman) ". "}.lst-kix_9w5yoflww7o0-7>li:before{content:"" counter(lst-ctn-kix_9w5yoflww7o0-7,lower-latin) ". "}ol.lst-kix_4jhw1c1a6bfe-5.start{counter-reset:lst-ctn-kix_4jhw1c1a6bfe-5 0}.lst-kix_9w5yoflww7o0-6>li:before{content:"" counter(lst-ctn-kix_9w5yoflww7o0-6,decimal) ". "}.lst-kix_e45mjayykp7f-1>li:before{content:"\0025cb   "}.lst-kix_e45mjayykp7f-0>li:before{content:"\0025cf   "}.lst-kix_e45mjayykp7f-3>li:before{content:"\0025cf   "}.lst-kix_e45mjayykp7f-2>li:before{content:"\0025a0   "}.lst-kix_e45mjayykp7f-7>li:before{content:"\0025cb   "}ol.lst-kix_4jhw1c1a6bfe-4.start{counter-reset:lst-ctn-kix_4jhw1c1a6bfe-4 0}.lst-kix_e45mjayykp7f-5>li:before{content:"\0025a0   "}.lst-kix_9w5yoflww7o0-7>li{counter-increment:lst-ctn-kix_9w5yoflww7o0-7}.lst-kix_e45mjayykp7f-4>li:before{content:"\0025cb   "}.lst-kix_e45mjayykp7f-8>li:before{content:"\0025a0   "}.lst-kix_e45mjayykp7f-6>li:before{content:"\0025cf   "}ol.lst-kix_9w5yoflww7o0-4.start{counter-reset:lst-ctn-kix_9w5yoflww7o0-4 0}ol.lst-kix_4jhw1c1a6bfe-6.start{counter-reset:lst-ctn-kix_4jhw1c1a6bfe-6 0}.lst-kix_9w5yoflww7o0-1>li{counter-increment:lst-ctn-kix_9w5yoflww7o0-1}ol.lst-kix_4jhw1c1a6bfe-3.start{counter-reset:lst-ctn-kix_4jhw1c1a6bfe-3 0}.lst-kix_1od86ppq6p3l-3>li:before{content:"\0025cf   "}.lst-kix_1od86ppq6p3l-5>li:before{content:"\0025a0   "}.lst-kix_1od86ppq6p3l-4>li:before{content:"\0025cb   "}.lst-kix_1od86ppq6p3l-8>li:before{content:"\0025a0   "}ol.lst-kix_9w5yoflww7o0-3.start{counter-reset:lst-ctn-kix_9w5yoflww7o0-3 0}.lst-kix_1od86ppq6p3l-7>li:before{content:"\0025cb   "}.lst-kix_1od86ppq6p3l-6>li:before{content:"\0025cf   "}.lst-kix_9w5yoflww7o0-1>li:before{content:"" counter(lst-ctn-kix_9w5yoflww7o0-1,lower-latin) ". "}.lst-kix_9w5yoflww7o0-3>li:before{content:"" counter(lst-ctn-kix_9w5yoflww7o0-3,decimal) ". "}.lst-kix_68407bki2o46-8>li:before{content:"\0025a0   "}ol.lst-kix_4jhw1c1a6bfe-0.start{counter-reset:lst-ctn-kix_4jhw1c1a6bfe-0 0}.lst-kix_1od86ppq6p3l-0>li:before{content:"\0025cf   "}.lst-kix_9w5yoflww7o0-0>li:before{content:"\0025cf   "}.lst-kix_9w5yoflww7o0-4>li:before{content:"" counter(lst-ctn-kix_9w5yoflww7o0-4,lower-latin) ". "}.lst-kix_68407bki2o46-6>li:before{content:"\0025cf   "}.lst-kix_68407bki2o46-5>li:before{content:"\0025a0   "}.lst-kix_68407bki2o46-7>li:before{content:"\0025cb   "}.lst-kix_1od86ppq6p3l-1>li:before{content:"\0025cb   "}ol.lst-kix_9w5yoflww7o0-6.start{counter-reset:lst-ctn-kix_9w5yoflww7o0-6 0}.lst-kix_1od86ppq6p3l-2>li:before{content:"\0025a0   "}.lst-kix_9w5yoflww7o0-2>li:before{content:"" counter(lst-ctn-kix_9w5yoflww7o0-2,lower-roman) ". "}.lst-kix_68407bki2o46-2>li:before{content:"\0025a0   "}ol.lst-kix_9w5yoflww7o0-5{list-style-type:none}ol.lst-kix_9w5yoflww7o0-4{list-style-type:none}.lst-kix_68407bki2o46-1>li:before{content:"\0025cb   "}.lst-kix_68407bki2o46-3>li:before{content:"\0025cf   "}ol.lst-kix_9w5yoflww7o0-3{list-style-type:none}ol.lst-kix_9w5yoflww7o0-2{list-style-type:none}.lst-kix_68407bki2o46-0>li:before{content:"\0025cf   "}.lst-kix_68407bki2o46-4>li:before{content:"\0025cb   "}.lst-kix_9w5yoflww7o0-3>li{counter-increment:lst-ctn-kix_9w5yoflww7o0-3}ol.lst-kix_9w5yoflww7o0-8{list-style-type:none}.lst-kix_4jhw1c1a6bfe-7>li{counter-increment:lst-ctn-kix_4jhw1c1a6bfe-7}ol.lst-kix_9w5yoflww7o0-7{list-style-type:none}ol.lst-kix_9w5yoflww7o0-6{list-style-type:none}ol.lst-kix_4jhw1c1a6bfe-1.start{counter-reset:lst-ctn-kix_4jhw1c1a6bfe-1 0}ol.lst-kix_9w5yoflww7o0-2.start{counter-reset:lst-ctn-kix_9w5yoflww7o0-2 0}ol.lst-kix_4jhw1c1a6bfe-8.start{counter-reset:lst-ctn-kix_4jhw1c1a6bfe-8 0}ol.lst-kix_4jhw1c1a6bfe-5{list-style-type:none}ol.lst-kix_9w5yoflww7o0-1{list-style-type:none}ol.lst-kix_4jhw1c1a6bfe-6{list-style-type:none}ol.lst-kix_4jhw1c1a6bfe-7{list-style-type:none}ol.lst-kix_4jhw1c1a6bfe-8{list-style-type:none}ol.lst-kix_4jhw1c1a6bfe-1{list-style-type:none}ol.lst-kix_4jhw1c1a6bfe-2{list-style-type:none}ol.lst-kix_4jhw1c1a6bfe-3{list-style-type:none}ol.lst-kix_4jhw1c1a6bfe-4{list-style-type:none}.lst-kix_4jhw1c1a6bfe-0>li:before{content:"" counter(lst-ctn-kix_4jhw1c1a6bfe-0,decimal) ". "}ol.lst-kix_4jhw1c1a6bfe-0{list-style-type:none}.lst-kix_4jhw1c1a6bfe-1>li:before{content:"" counter(lst-ctn-kix_4jhw1c1a6bfe-1,lower-latin) ". "}.lst-kix_4jhw1c1a6bfe-2>li:before{content:"" counter(lst-ctn-kix_4jhw1c1a6bfe-2,lower-roman) ". "}ol.lst-kix_9w5yoflww7o0-8.start{counter-reset:lst-ctn-kix_9w5yoflww7o0-8 0}ol.lst-kix_4jhw1c1a6bfe-7.start{counter-reset:lst-ctn-kix_4jhw1c1a6bfe-7 0}.lst-kix_4jhw1c1a6bfe-5>li:before{content:"" counter(lst-ctn-kix_4jhw1c1a6bfe-5,lower-roman) ". "}.lst-kix_4jhw1c1a6bfe-6>li:before{content:"" counter(lst-ctn-kix_4jhw1c1a6bfe-6,decimal) ". "}.lst-kix_9w5yoflww7o0-2>li{counter-increment:lst-ctn-kix_9w5yoflww7o0-2}.lst-kix_4jhw1c1a6bfe-3>li:before{content:"" counter(lst-ctn-kix_4jhw1c1a6bfe-3,decimal) ". "}.lst-kix_4jhw1c1a6bfe-4>li:before{content:"" counter(lst-ctn-kix_4jhw1c1a6bfe-4,lower-latin) ". "}.lst-kix_4jhw1c1a6bfe-7>li:before{content:"" counter(lst-ctn-kix_4jhw1c1a6bfe-7,lower-latin) ". "}.lst-kix_4jhw1c1a6bfe-8>li:before{content:"" counter(lst-ctn-kix_4jhw1c1a6bfe-8,lower-roman) ". "}ol.lst-kix_9w5yoflww7o0-1.start{counter-reset:lst-ctn-kix_9w5yoflww7o0-1 0}.lst-kix_4jhw1c1a6bfe-2>li{counter-increment:lst-ctn-kix_4jhw1c1a6bfe-2}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_4jhw1c1a6bfe-5>li{counter-increment:lst-ctn-kix_4jhw1c1a6bfe-5}ul.lst-kix_68407bki2o46-1{list-style-type:none}ul.lst-kix_68407bki2o46-2{list-style-type:none}ul.lst-kix_68407bki2o46-0{list-style-type:none}.lst-kix_4jhw1c1a6bfe-8>li{counter-increment:lst-ctn-kix_4jhw1c1a6bfe-8}ul.lst-kix_68407bki2o46-7{list-style-type:none}ul.lst-kix_68407bki2o46-8{list-style-type:none}ul.lst-kix_68407bki2o46-5{list-style-type:none}ul.lst-kix_68407bki2o46-6{list-style-type:none}ul.lst-kix_68407bki2o46-3{list-style-type:none}ul.lst-kix_68407bki2o46-4{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c21{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c28{padding-top:12pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c18{padding-top:14pt;padding-bottom:10pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Roboto Mono";font-style:normal}.c5{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c31{padding-top:24pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c32{padding-top:0pt;padding-bottom:10pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c11{padding-top:0pt;padding-bottom:10pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c22{color:#457ba7;font-weight:400;font-size:11pt;font-family:"Open Sans"}.c15{color:#000000;font-weight:400;font-size:11pt;font-family:"Arial"}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c24{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c26{color:#4175ca;font-weight:400;font-size:12pt;font-family:"Google Sans Text"}.c30{color:#1976d2;font-weight:400;font-size:18pt;font-family:"Google Sans"}.c19{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#0000ee;text-decoration:underline}.c0{font-size:9pt;font-family:"Roboto Mono";color:#188038;font-weight:400}.c7{color:#4a86e8;font-weight:400;font-size:14pt;font-family:"Google Sans"}.c9{color:#000000;font-weight:400;font-size:11pt;font-family:"Google Sans Text"}.c29{color:#4576c5;font-weight:400;font-size:11pt;font-family:"Open Sans"}.c14{color:#188038;font-weight:400;font-family:"Roboto Mono"}.c4{text-decoration:none;vertical-align:baseline;font-style:normal}.c33{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c2{font-size:9pt;font-weight:400;font-family:"Roboto Mono"}.c25{text-decoration:none;font-style:italic}.c27{border:1px solid black;margin:5px}.c17{margin-left:36pt;padding-left:0pt}.c10{color:#37474f}.c23{color:#6a9955}.c6{vertical-align:baseline}.c12{color:#b80672}.c13{color:#9334e6}.c8{height:11pt}.c20{color:#c5221f}.title{padding-top:0pt;color:#1976d2;font-size:28pt;padding-bottom:6pt;font-family:"Google Sans Text";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.subtitle{padding-top:0pt;color:#2196f3;font-size:12pt;padding-bottom:10pt;font-family:"Google Sans Text";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Google Sans Text"}p{margin:0;color:#000000;font-size:11pt;font-family:"Google Sans Text"}h1{padding-top:24pt;color:#1976d2;font-size:18pt;padding-bottom:6pt;font-family:"Google Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#4a86e8;font-size:14pt;padding-bottom:6pt;font-family:"Google Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#4175ca;font-size:12pt;padding-bottom:10pt;font-family:"Google Sans Text";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#4576c5;font-size:11pt;padding-bottom:4pt;font-family:"Open Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#457ba7;font-size:11pt;padding-bottom:4pt;font-family:"Open Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#457ba7;font-size:11pt;padding-bottom:4pt;font-family:"Open Sans";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c33 doc-content"><h1 class="c31" id="h.g30huzokkzwx"><span>[External] </span><span>Wan2.1-14B</span><sup><a href="#cmnt1" id="cmnt_ref1">[a]</a></sup><sup><a href="#cmnt2" id="cmnt_ref2">[b]</a></sup><sup><a href="#cmnt3" id="cmnt_ref3">[c]</a></sup><span class="c4 c30">&nbsp;Text to Video Model Optimization on TPU v6e-16</span></h1><p class="c11"><span>Contributors: </span><span class="c19"><a href="mailto:yuyanpeng@google.com">Yuyan Peng</a></span><span>, </span><span class="c19"><a href="mailto:haoluo@google.com">Hao Luo</a></span><span>, </span><span class="c19"><a href="mailto:hanq@google.com">Han Qi</a></span><span>, </span><span class="c19"><a href="mailto:shunwang@google.com">Shun Wang</a></span><span>,</span><span class="c19"><a href="mailto:wdhongtw@google.com">Weida Hong</a></span><span>, &lt;add your name&gt;<br>Last major revision: </span><span>Aug 28, 2025</span><span class="c9 c4"><br>Status: Reviewed</span></p><h2 class="c5" id="h.ocepw2fsspwy"><span class="c4 c7">Abstract</span></h2><p class="c11"><span class="c9 c4">This document details the optimization of the Wan2.1-14B text-to-video model on the TPU v6e-16 architecture, which aimed to significantly reduce the end-to-end video generation time for 720p, 81-frame outputs. Through a combination of software and hardware-aware optimizations, including adapting PyTorch models to the JAX backend via Torchax, strategic sharding (FSDP, CP, and SP), and kernel-level enhancements within the Diffusion Transformer (DiT) and Variational Autoencoder (VAE) components, the generation time was successfully reduced from 428 seconds to 124.9 seconds. This report outlines the methodologies employed and the technical insights that led to the substantial reduction in inference latency.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h2 class="c5" id="h.yxuyu1yxp9fa"><span class="c7 c4">Introduction</span></h2><p class="c11"><span class="c6">This document outlines the optimization efforts for the Wan2.1 14B text-to-video model, targeting the generation of 720p, 81-frame outputs on a v6e-16 TPU architecture.</span><span class="c6">&nbsp;</span><span class="c6">The foundational optimization commenced with the Diffuser library from Hugging Face, leveraging Torchax to facilitate the adaptation of PyTorch models to the JAX backend for the TPU environment, incorporating select components from Maxdiffusion.</span></p><p class="c11"><span class="c6">The Wan2.1 pipeline is structured into three primary components: text embedding for prompt processing, the Diffusion Transformer (DiT) for video diffusion steps, and a Variational Autoencoder (VAE) for decoding latent representations into video.</span><span class="c6">&nbsp;</span><span class="c6">Of these, the DiT consistently accounts for the predominant share of processing time, rendering it the central focus of the optimization initiatives.</span></p><p class="c11"><span class="c6">The optimization strategy encompassed several key methodologies:</span></p><ul class="lst-kix_68407bki2o46-0 start"><li class="c11 c17 li-bullet-0"><span class="c6">Text Embedding: This component was enhanced through the application of tensor parallelism.</span></li><li class="c11 c17 li-bullet-0"><span class="c6">VAE: The original implementation was superseded by a more efficient version derived from Maxdiffusion, accompanied by strategic sharding of its activations.</span></li></ul><ul class="lst-kix_9w5yoflww7o0-0 start"><li class="c11 c17 li-bullet-0"><span class="c6">Diffusion Transformer (DiT): The core of the optimization involved a synergistic combination of sharding strategies&mdash;including Fully Sharded Data Parallelism (FSDP), Context Parallelism (CP), and Sequence Parallelism (SP)&mdash;and sophisticated self-attention optimizations utilizing the splash attention kernel.</span><span class="c6">&nbsp;</span><span class="c6">These techniques were pivotal in mitigating communication overhead and accelerating computational throughput.</span></li></ul><p class="c11 c8"><span class="c9 c4"></span></p><h2 class="c5" id="h.tgkdyo7cgxvu"><span class="c7 c4">Background</span></h2><p class="c11"><span class="c9 c4">The document focuses on optimizing generating a 720p 81 frames text to video with Wan2.1 14B model, using v6e-16.</span></p><p class="c11"><span class="c9 c4">Torchax is a library that adapts pytorch models to the Jax backend for the TPU environment. Also, it could easily combine the implementation of pytorch with jax. </span></p><p class="c11"><span>Diffuser by the hugging face is the beginning of the optimization, with Torchax and some codes from </span><span class="c24"><a href="https://www.google.com/url?q=https://github.com/AI-Hypercomputer/maxdiffusion/tree/main&amp;sa=D&amp;source=editors&amp;ust=1765684106279378&amp;usg=AOvVaw0f52-uebc0fAYAwBV4YnVg">Maxdiffusion</a></span><span class="c9 c4">.</span></p><p class="c11"><span class="c9 c4">The Wan2.1 pipeline comprises three distinct components: text embedding for prompt processing, the Diffusion Transformer (DiT) for video diffusion steps, and a Variational Autoencoder (VAE) for decoding latent representations into video.</span><span class="c9 c4">&nbsp;</span><span class="c9 c4">Of these, the DiT consistently consumes the most processing time, while the VAE accounts for approximately 5% of the total, and text embedding contributes negligibly to the overall runtime.</span></p><p class="c11"><span>The code could be referenced from </span><span class="c24"><a href="https://www.google.com/url?q=https://github.com/shungcp/diffusers/tree/df1e2157b6bbd67b98d8f26ecdcc4273f19cbd09&amp;sa=D&amp;source=editors&amp;ust=1765684106280507&amp;usg=AOvVaw0QO0yS7Gg_Ck4V68RoybU4">github</a></span><span>.</span></p><p class="c11"><span class="c9 c4">Also see &ldquo;Wan2.1-14B Text to Video Model FLOPs Utilization Analysis on TPU v6e-16&rdquo; for roofline analysis of 141s version.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><p class="c11 c8"><span class="c9 c4"></span></p><h2 class="c5" id="h.5gatjze3ttdv"><span>Method</span></h2><p class="c11"><span class="c9 c4">Methods are introduced in this section. The overall architecture is built on the diffuser and some code from Maxdiffusion, which adapts PyTorch models to the Jax backend for TPU. The optimization focuses on the DiT part due to its dominant computation time. Other components are optimized by applying tensor parallelism for Text Embedding and replacing the original VAE implementation with a more efficient one from Maxdiffusion. For DiT, sharding strategies and self-attention optimizations using the splash attention kernel are key to reducing communication and accelerating computation.</span></p><p class="c11"><span>See the Appendix for more detail of how to find out the bottlenecks and implementation detail.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h3 class="c18" id="h.1xadmtt8299s"><span class="c26 c4">Device Mesh</span></h3><p class="c3"><span class="c9 c4">On v6e-16, there are 4x4 TPU device mesh. We define dp, sp, axis for data parallelism, sequence parallelism, and remaining devices in the axis.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; </span><span class="c0">mesh_devices</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">mesh_utils.create_device_mesh((dp_dim,</span><span class="c2">&nbsp;</span><span class="c0">sp_dim,</span><span class="c2">&nbsp;</span><span class="c0">tp_dim),</span><span class="c2">&nbsp;</span><span class="c0">allow_split_physical_axes=True)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">mesh</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">Mesh(mesh_devices,</span><span class="c2">&nbsp;</span><span class="c0">(&#39;dp&#39;,&#39;sp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">axis))</span></p><p class="c11"><span>&#60418;It needs to add </span><span class="c14">allow_split_physical_axes</span><span class="c9 c4">&nbsp;while the mesh axis is not a combination of 1x4x4.</span></p><p class="c11"><span>The axes could be used simultaneously, such as </span><span class="c14">(&lsquo;sp&rsquo;, &rsquo;axis&rsquo;)</span><span class="c9 c4">&nbsp;means using both axes devices for sharding.</span></p><p class="c11"><span class="c9 c4">Beware that the later axis has better communication performance. </span></p><p class="c11 c8"><span class="c9 c4"></span></p><h3 class="c18" id="h.tudtkq9g8uyt"><span class="c26 c4">Text Embedding</span></h3><p class="c11"><span>While text embedding consumes little time, it just simply does tensor parallelism (TP) along all the devices </span><span class="c14">(&lsquo;dp&rsquo;,&lsquo;sp&rsquo;,&lsquo;axis&rsquo;)</span><span class="c9 c4">. By putting the weights with sharding, the compiler could automatically do the TP in the inference.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h3 class="c18" id="h.4ybm9cwjm9fq"><span class="c26 c4">VAE</span></h3><p class="c11"><span class="c9 c4">The original pytorch implementation of VAE with torchax is replaced by the implementation in Maxdiffusion observing that the original implementation is slow and hard to parallelize computation.</span></p><p class="c11"><span>VAE shards the activations along conv_in and conv_out along all devices </span><span class="c14">(&lsquo;dp&rsquo;,&lsquo;sp&rsquo;,&lsquo;axis&rsquo;)</span><span class="c9 c4">.</span></p><p class="c11"><span class="c9 c4">Encountering HBM OOM problems, there are some modifications added to the Maxdiffusion VAE to prevent useless tensors occupying the HBM.</span></p><p class="c11"><span class="c9 c4">VAE weights are fully replicated along all devices.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h3 class="c18" id="h.v45h963gkasq"><span>Diffusion Transformer (DiT)</span></h3><p class="c11"><span class="c6">Activations in DiT are sharded along 2 devices for data parallelism (DP) since there is a positive and a negative prompt which can be batched.</span></p><p class="c11"><span class="c6">Given that the long sequence is 75600 length, the optimization focuses on reducing communications and self-attention optimization.</span></p><p class="c11"><span class="c6">For reducing the communication, Context parallelism (CP) is applied to self-attention, and Sequence parallelism (SP) is applied to cross-attention.</span><span class="c6">&nbsp;</span><span class="c6">They prevent communication as much as possible.</span></p><p class="c11"><span>Since the parallelism mechanisms are different among attention and other operations, the mesh axis </span><span class="c14">dp, sp, axis</span><span>&nbsp;name definition is for the mechanism used in self attention, which is </span><span class="c14">dp=2, sp=1, axis=8</span><span>&nbsp;with v6e-16. For cross attention and other linear layers, </span><span class="c14">axis</span><span>&nbsp;is for sequence sharding. We preserve the </span><span class="c14">sp</span><span>&nbsp;for the scenario head number could not divide by axis.</span></p><p class="c11"><span class="c9 c4">To simplify, let us assume the presence of 8 TPU chips (16 // 2) and a batch size of 1 for the subsequent section.</span></p><p class="c11"><span class="c6">For self-attention optimization, the splash attention kernel in the jax library is used and does further optimization from the kernel.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><a id="id.lf2y9aalkezy"></a><h4 class="c21" id="h.2tent3wjcpbm"><span class="c29 c4">Sharding Strategy</span></h4><p class="c11"><span class="c6">Model parameters are sharded across devices utilizing Fully Sharded Data Parallelism (FSDP).</span><span class="c6">&nbsp;</span><span class="c9 c4">Given the significant size of the model, this approach is necessary to ensure adequate working memory, as each v6e chip has only 32GB of HBM. By asynchronously gathering parameters before computation, communication and computational operations can be effectively overlapped.</span></p><p class="c3"><span>&#60419;</span><span class="c0 c4"># hidden_states=[batch, latent//4+1 * height//8//2 * width//8//2, dim]</span></p><p class="c3"><span class="c0">hidden_states</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">mark_sharding(hidden_states,</span><span class="c2">&nbsp;</span><span class="c0">P(&quot;dp&quot;,</span><span class="c2">&nbsp;</span><span class="c0">(&#39;axis&#39;,&#39;sp&#39;,),</span><span class="c2">&nbsp;</span><span class="c0">None))</span></p><p class="c11"><span>&#60418;</span><span>Activations in DiT are sharded along devices for sequence parallelism (SP) after patch embedding.</span><span>&nbsp;</span><span class="c14">None</span><span class="c9 c4">&nbsp;means replicate along all devices.</span></p><p class="c3"><span>&#60419;</span><span class="c0">q_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;sp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c3"><span class="c0">kv_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c11"><span>&#60418;In self attention, </span><span>Q, K, V are sharded along head_number on device</span><span>s</span><span>.</span></p><p class="c3"><span>&#60419;</span><span class="c0">q_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">(&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c3"><span class="c0">kv_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c11"><span>&#60418;In cross attention, </span><span>Q is sharded along the sequence dimension and K, V are fully replicated</span><span class="c9 c4">.</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 337.33px;"><img alt="" src="images/image7.png" style="width: 624.00px; height: 337.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span class="c9 c4">Graph for overall sharding in DiT transformer block.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h4 class="c21" id="h.jpmuqxze7eb4"><span class="c4 c29">Sequence Sharding</span></h4><p class="c11"><span class="c6">Sequence sharding partitions the matrix along the sequence dimension, generally preventing collective operations in most layers except the attention layers.</span><span class="c6">&nbsp;</span><span class="c6">Within attention layers, QK necessitates dot product calculations along the sequence dimension.</span></p><p class="c11"><span class="c6">Context parallelism (CP) shards along the head number dimension, requiring all-to-all (A2A) operations to re-shard activations from the sequence dimension to the head number dimension.</span><span class="c6">&nbsp;</span><span class="c6">Conversely, Sequence parallelism (SP) shards Q along the sequence dimension while keeping K and V fully replicated; only K and V then require all-gather operations from their sequence-sharded state.</span></p><p class="c11"><span class="c6">Self-attention layers employ Context Parallelism (CP), while cross-attention layers leverage Sequence Parallelism (SP). The selection of CP for self-attention is predicated on its ability to minimize communication data volume, alongside empirical evidence indicating superior performance of the splash attention kernel under CP. Conversely, for cross-attention, K and V maintain a constant value and length of 512 across all DiT transformer blocks, necessitating SP to preclude communication of Q.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h4 class="c21" id="h.vtm54kh9vohb"><span class="c29 c4">Self Attention</span></h4><p class="c11"><span>The splash attention kernel</span><span>&nbsp;is used for self attention. </span><span class="c6">The splash attention kernel, integrated into the Jax library, further optimizes flash attention on the TPU.</span><span class="c6">&nbsp;</span><span>Given Q, K, and V tensor shapes of [1, 40, 75600, 128], the resulting QK matrix of [1, 40, 75600, 75600] exceeds the capacity of the TPU. Flash attention addresses this by tiling QKV calculations, thereby reducing peak memory usage and HBM access.</span><span>&nbsp;</span><span class="c6">&nbsp;It is crucial to note that the efficacy of block sizes within splash attention is highly sensitive to input shape, necessitating careful adjustment.</span></p><p class="c11"><span class="c9 c4">Splash attention kernel contains optimization for TPUs although splash means sparse flash, which may not be needed in the dense attention here. Splash attention kernel implements the flash attention V2, which does the QK, softmax, QKV on the fly by tiling the Q, K, V to blocks and maintains internal states of the results. The kernel is implemented by the pallas kernel grid, which could overlap HBM to VMEM communication and computation by pipeline mechanism.</span></p><p class="c11"><span class="c9 c4">The kernel contains additional KV compute blocks to iterate through KV blocks as an additional inner loop. The KV compute blocks, which is another parameter to tune the factor among matmul FLOPs utilization, VMEM usage of internal states, overlapping softmax on VPU with MXU, and overhead of tiling. </span></p><p class="c11"><span class="c9 c4">Since the block sizes are sensitive to the performance, it needs to adjust the sizes case by case. It is worth sweeping the block sizes at an TPU chip with the kernel itself only, to find out the best block sizes combination, &nbsp;while the kernel is run on every TPU chip without communication, and could not overlap with HLO ops compiled by the XLA compiler without further manual optimization.</span></p><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0 c4">block_sizes = splash_attention.BlockSizes(</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; block_q=min(BQSIZE, padded_q_seq_len),</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; block_kv=min(BKVSIZE, padded_kv_seq_len),</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; block_kv_compute=min(BKVCOMPUTESIZE, padded_kv_seq_len),</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # The kernel is create on the local device here, therefore the shards=1</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; splash_kernel = splash_attention.make_splash_mha(</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mask=mask, block_sizes=block_sizes, head_shards=1, q_seq_shards=1</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; out = splash_kernel(q_3d_padded, k_3d_padded, v_3d_padded)</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c11"><span class="c9 c4">&#60418;Padding is needed if the sequence is not multiple of block sizes.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h5 class="c28" id="h.pyb0e32ofwd0"><span class="c4 c22">Q, KV Blocks</span></h5><p class="c11"><span class="c9 c4">The Q, KV blocks are the sizes loaded into the VMEM at one iteration of flash attention.</span></p><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">q_index_map(h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">*_):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">(h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">0)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">k_index_map(h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">*_):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">(h</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">q_heads_per_kv_head,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">0)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">v_index_map(h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">*_):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">(h</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">q_heads_per_kv_head,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">0)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">in_specs</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">[</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((None,</span><span class="c2">&nbsp;</span><span class="c0">bq,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_qk),</span><span class="c2">&nbsp;</span><span class="c0">q_index_map),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((None,</span><span class="c2">&nbsp;</span><span class="c0">bkv,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_qk),</span><span class="c2">&nbsp;</span><span class="c0">k_index_map),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((None,</span><span class="c2">&nbsp;</span><span class="c0">bkv,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_v),</span><span class="c2">&nbsp;</span><span class="c0">v_index_map),</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0 c4">]</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">grid_width</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">kv_seq_len</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">bkv</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">grid</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(num_q_heads,</span><span class="c2">&nbsp;</span><span class="c0">q_seq_len</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">bq,</span><span class="c2">&nbsp;</span><span class="c0 c4">grid_width)</span></p><p class="c11"><span class="c9 c4">&#60418;Pallas kernel calculates the blocks sequentially represented as a grid, defining how to sweep the arrays.</span></p><p class="c11"><span class="c9 c4">Usually, the larger size the better performance. For the larger size of blocks, the matmul could be more efficient and the iteration overhead could be reduced. Try as large as possible sizes until the VMEM or HBM resource is exhausted.</span></p><p class="c11"><span class="c9 c4">It&rsquo;s not a certain pattern which Q or KV size should be larger, but both sizes should be similar large for better performance empirically.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h5 class="c28" id="h.d9sq9dvxca4t"><span class="c22 c4">KV Compute Blocks</span></h5><p class="c11"><span class="c9 c4">The KV compute is an additional inner loop of the QK, softmax, QKV iteration. It breaks the KV blocks further to the KV compute blocks, like using smaller size KV blocks for flash attention, but loads from HBM into VMEM at first.</span></p><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp;</span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">body(kv_compute_index,</span><span class="c2">&nbsp;</span><span class="c0">_):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">jax.named_scope(&quot;qk&quot;):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">slice_k</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pl.ds(kv_compute_index</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">bkv_compute,</span><span class="c2">&nbsp;</span><span class="c0">bkv_compute)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_prev,</span><span class="c2">&nbsp;</span><span class="c0">l_prev</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">m_scratch_ref[...],</span><span class="c2">&nbsp;</span><span class="c0">l_scratch_ref[...]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">q</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">q_ref[...]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">k</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">k_ref[slice_k,</span><span class="c2">&nbsp;</span><span class="c0">:]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">qk</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">lax.dot_general(q,</span><span class="c2">&nbsp;</span><span class="c0">k,</span><span class="c2">&nbsp;</span><span class="c0">NT_DIM_NUMBERS,</span><span class="c2">&nbsp;</span><span class="c0">preferred_element_type=float32)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">jax.named_scope(&quot;softmax&quot;):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">qk.max(axis=-1)[:,</span><span class="c2">&nbsp;</span><span class="c0">None]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_next</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.maximum(m_prev,</span><span class="c2">&nbsp;</span><span class="c0">m_curr)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">bkv_repeats,</span><span class="c2">&nbsp;</span><span class="c0">rem</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">divmod(bkv_compute,</span><span class="c2">&nbsp;</span><span class="c0">NUM_LANES)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">rem</span><span class="c2">&nbsp;</span><span class="c0">!=</span><span class="c2">&nbsp;</span><span class="c0">0:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">raise</span><span class="c2">&nbsp;</span><span class="c0">NotImplementedError(f&quot;{bkv_compute=}</span><span class="c2">&nbsp;</span><span class="c0">should</span><span class="c2">&nbsp;</span><span class="c0">be</span><span class="c2">&nbsp;</span><span class="c0">a</span><span class="c2">&nbsp;</span><span class="c0">multiple</span><span class="c2">&nbsp;</span><span class="c0">of</span><span class="c2">&nbsp;</span><span class="c0">{NUM_LANES}&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">s_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.exp(qk</span><span class="c2">&nbsp;</span><span class="c0">-</span><span class="c2">&nbsp;</span><span class="c0">pltpu.repeat(m_next,</span><span class="c2">&nbsp;</span><span class="c0">bkv_repeats,</span><span class="c2">&nbsp;</span><span class="c0">axis=1))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">l_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.lax.broadcast_in_dim(s_curr.sum(axis=-1),</span><span class="c2">&nbsp;</span><span class="c0">l_prev.shape,</span><span class="c2">&nbsp;</span><span class="c0">(0,))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">alpha</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.exp(m_prev</span><span class="c2">&nbsp;</span><span class="c0">-</span><span class="c2">&nbsp;</span><span class="c0">m_next)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">l_next</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">l_curr</span><span class="c2">&nbsp;</span><span class="c0">+</span><span class="c2">&nbsp;</span><span class="c0">alpha</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">l_prev</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">jax.named_scope(&quot;qkv&quot;):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_scratch_ref[...],</span><span class="c2">&nbsp;</span><span class="c0">l_scratch_ref[...]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">m_next,</span><span class="c2">&nbsp;</span><span class="c0">l_next</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">v</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">v_ref[slice_k,</span><span class="c2">&nbsp;</span><span class="c0">:].astype(float32)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">o_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">lax.dot_general(s_curr,</span><span class="c2">&nbsp;</span><span class="c0">v,</span><span class="c2">&nbsp;</span><span class="c0">NN_DIM_NUMBERS)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">alpha_o</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pltpu.repeat(alpha,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_v_repeats,</span><span class="c2">&nbsp;</span><span class="c0">axis=1)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">o_scratch_ref[:]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">alpha_o</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">o_scratch_ref[:]</span><span class="c2">&nbsp;</span><span class="c0">+</span><span class="c2">&nbsp;</span><span class="c0">o_curr</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">lax.fori_loop(0,</span><span class="c2">&nbsp;</span><span class="c0">bkv</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">bkv_compute,</span><span class="c2">&nbsp;</span><span class="c0">body,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">unroll=True)</span></p><p class="c11"><span class="c9 c4">&#60418;Simplified version of splash attention kernel kv compute loop.</span></p><p class="c11"><span class="c9 c4">It is not a thumb rule of which size is the best among all cases. It affects among matmul FLOPs utilization, VMEM usage of internal states, overlapping softmax on VPU with MXU, and overhead of tiling. </span></p><p class="c11"><span class="c9 c4">For the matmul FLOPs utilization, the larger size often means larger utilization since we could do larger size arrays matmul.</span></p><p class="c11"><span class="c9 c4">For the VMEM usage of internal states, the less block size produces less size of softmax internal arrays, which means we could load larger Q, KV blocks into VMEM at first.</span></p><p class="c11"><span class="c9 c4">For overlapping softmax on VPU with MXU, it is possible for the TPU to overlap the VPU with MXU through multiple QK, softmax, QKV iterations, while cannot overlap through the pallas kernel grid call iteration. The softmax are element wise operations, which are computed on the VPU; however, it depends on the max value of QK matmul, which prevents the computation overlapping well. For the multiple iterations at once, it is more possible for the scheduler, which schedules the computation to the VPU and the MXU, to overlap the computations among the VPU and the MXU.</span></p><p class="c11"><span class="c9 c4">For the overhead of tiling, the larger size arrays loaded from HBM to VMEM are usually better, which reduces the count of iterations and the overhead from iterations.</span></p><p class="c11"><span class="c9 c4">Due to the complexity of the effect by the KV compute sizes, It needs to sweep from the small size to find the best size.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h5 class="c28" id="h.533muhgd26oi"><span class="c22 c4">Further Optimization</span></h5><p class="c11"><span class="c9 c4">From the original splash attention kernel, there is still much room for improvement, since the softmax VPU computation cannot fully overlap with the MXU. To accelerate the softmax computation bounded by VPU, there are two directions, elementwise operations optimization and VPU computation overlapping.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h6 class="c28" id="h.4h9cmeig1a60"><span class="c22 c6 c25">Elementwise Operations</span></h6><p class="c11"><span class="c9 c4">There are two optimizations, one for reduction, and another for exp.</span></p><p class="c11"><span>The original implementation, the reduction operations are conducted along the axis=-1. According to the </span><span class="c24"><a href="https://www.google.com/url?q=https://docs.jax.dev/en/latest/pallas/tpu/details.html%23elementwise-operations&amp;sa=D&amp;source=editors&amp;ust=1765684106321254&amp;usg=AOvVaw1X7i-2-VRfgKVd5l3RwKne">user guides</a></span><span class="c9 c4">, it is less efficient along the last dimensions.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; &nbsp;</span><span class="c0">m_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">qk.max(axis=-1)[:,</span><span class="c2">&nbsp;</span><span class="c0">None]</span></p><p class="c11"><span class="c9 c4">&#60418;Reduce max along last dimension in the original implementation.</span></p><p class="c11"><span class="c9 c4">To reduce along the second last dimension, the QK matrix should be the K dimension first. Fortunately, transportation in the MXU&rsquo;s input output is almost free. Change the output order, there are 2 benefits. 1. The max reduction and sum are along the first dimension which is more efficient. 2. The internal states could use less size and broadcast more efficiently.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; </span><span class="c0">qk</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">lax.dot_general(k,</span><span class="c2">&nbsp;</span><span class="c0">q,</span><span class="c2">&nbsp;</span><span class="c0">NT_DIM_NUMBERS,</span><span class="c2">&nbsp;</span><span class="c0">preferred_element_type=float32)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">assert</span><span class="c2">&nbsp;</span><span class="c0">qk.shape</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">(bkv_compute,</span><span class="c2">&nbsp;</span><span class="c0 c4">bq)</span></p><p class="c3"><span class="c0 c4">...</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; m_curr</span><span class="c2 c23">&nbsp;</span><span class="c0">=</span><span class="c2 c23">&nbsp;</span><span class="c0">qk.max(axis=0)[None,</span><span class="c2 c23">&nbsp;</span><span class="c0 c4">:]</span></p><p class="c11"><span class="c9 c4">&#60418;Transposes are free for dot_general.</span></p><p class="c11"><span class="c9 c4">According to the TPU memory layout, the arrays need to be multiple of &nbsp;8 x 128 size in v6e or it needs to be padded. For this layout, it is more efficient to reduce along the first dimension. And with KQ dimension order, we could save the internal states with size 8 x q_block instead of the q_block x 128.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_next</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.maximum(m_prev,</span><span class="c2">&nbsp;</span><span class="c0">m_curr)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">assert</span><span class="c2">&nbsp;</span><span class="c0">m_next.shape</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">(NUM_SUBLANES,</span><span class="c2">&nbsp;</span><span class="c0">bq)</span></p><p class="c11"><span class="c9 c4">&#60418;NUM_SUBLANES=8 for the second last dimension.</span></p><p class="c11"><span class="c9 c4">For the exp operation in TPU, it is combined with two operation log2(e) * exp2(x) in the implementation. To reduce the time taken in the softmax, the log2(e) could be merged into the previous Q / sqrt(dim) operations, which would be no cost since there are just two constants merged at compile time. Be aware that all operations of exp need to be modified to exp2. Also jnp.exp2 is slower and it needs to use jnp.power directly.</span></p><p class="c3"><span>&#60419;</span><span class="c0 c4">&nbsp; &nbsp; &nbsp;_LOG2_E = 1.44269504</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp;q = q * scale_factor * _LOG2_E</span></p><p class="c3"><span class="c0">...</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp;</span><span class="c0">s_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.power(2.0, qk</span><span class="c2">&nbsp;</span><span class="c0">-</span><span class="c2">&nbsp;</span><span class="c0">pltpu.repeat(m_next,</span><span class="c2">&nbsp;</span><span class="c0">bkv_repeats,</span><span class="c2">&nbsp;</span><span class="c0">axis=1))</span></p><p class="c11"><span>&#60418;The multiply part in exp could be separated and merged in the previous operation.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h6 class="c28" id="h.2va5oe1rcbtb"><span class="c22 c25 c6">VPU Computation Overlapping</span></h6><p class="c11"><span class="c9 c4">There are two optimizations, one for further softmax QKV inner loop, and another for linear programing LLO scheduler flags.</span></p><p class="c11"><span class="c9 c4">Observing that the softmax may depend on the max of QK, add a further inner loop of softmax QKV to reduce the synchronization. While preserving the QK matmul size for efficiency, slice the results of QK along the K dimension for a further inner loop of the softmax and QKV. It could start doing the computation in the VPU as early as possible while the QK is not fully completed. For the efficiency of the QKV, the further inner loop size needs to be larger than 256 due to MXU design.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; </span><span class="c0">step</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">bkv_compute_in</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">assert</span><span class="c2">&nbsp;</span><span class="c0">qk.shape[0]</span><span class="c2">&nbsp;</span><span class="c0">%</span><span class="c2">&nbsp;</span><span class="c0">step</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">0</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">i</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">range(0,</span><span class="c2">&nbsp;</span><span class="c0">qk.shape[0],</span><span class="c2">&nbsp;</span><span class="c0">step):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">qk[i:i+step].max(axis=0)[None,</span><span class="c2">&nbsp;</span><span class="c0">:]</span></p><p class="c11"><span class="c9 c4">&#60418;Further inner loop in kv compute.</span></p><p class="c11"><span>Another linear programing LLO scheduler flags is provided by the pallas call compiler. The LLO is the low level operations near to the machine code. The scheduler sees the MXU as the bottleneck part, and tries to overlap other resources like VPU as much as possible while MXU is computing. It is worth trying different strategies such as the flag to overlap better. Add </span><span class="c14">flags={&quot;XLA_TPU_FORCE_LP_LLO_SCHEDULER&quot;: True})</span><span class="c9 c4">&nbsp;in pallas_call CompilerParams.</span></p><p class="c3"><span>&#60419;</span><span class="c0">compiler_params=pltpu.CompilerParams(dimension_semantics=(&quot;parallel&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;arbitrary&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;arbitrary&quot;),</span><span class="c2">&nbsp;</span><span class="c0">flags={&quot;XLA_TPU_FORCE_LP_LLO_SCHEDULER&quot;:</span><span class="c2">&nbsp;</span><span class="c0">True}),</span></p><p class="c11"><span class="c9 c4">&#60418;Add the flag in compiler_params of pallas_call.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h2 class="c5" id="h.2uctmakbzmdc"><span>Results</span></h2><p class="c11"><span class="c9 c4">Test end to end Wan2.1-14B model text to video generating time on TPU v6e-16. Before 141s optimization, there were multiple optimizations implemented simultaneously. The end to end time difference could be varied from below a bit. At each optimization step, sweep the splash attention kernel block size to find the best combination since the sizes are sensitive to the implementation.</span></p><p class="c11"><span class="c9 c4">At the time flash attention was added to get the first result on v6e-16. With TP in DiT, using DP, CP for both self and cross attention.<br>428s</span></p><p class="c11"><span class="c9 c4">Substitute VAE with Maxdiffusion implementation.<br>428s -&gt; 358s</span></p><p class="c11"><span class="c9 c4">Add the splash attention kernel.<br>358s -&gt; 247s</span></p><p class="c11"><span class="c9 c4">Add sharding in VAE.<br>247s -&gt; 227s</span></p><p class="c11"><span class="c9 c4">Adjust to the final sharding, full sequence sharding with CP in self attention.<br>227s -&gt; 141s</span></p><p class="c11"><span class="c9 c4">Below optimizations are not merged into the POC.</span></p><p class="c11"><span class="c9 c4">Change the QK dimension to KQ dimension, accelerate the reduction operations.<br>141s -&gt; 138.4s</span></p><p class="c11"><span class="c9 c4">Extract splash attention kernel from the Jax library and eliminate unused sparse and segment id parts.<br>138.4s -&gt; 137.8s</span></p><p class="c11"><span class="c9 c4">Another further inner loop for softmax QKV.<br>137.8s -&gt; 135.2s</span></p><p class="c11"><span class="c9 c4">Add XLA_TPU_FORCE_LP_LLO_SCHEDULER flag.<br>135.2s -&gt; 130.1s</span></p><p class="c11"><span class="c9 c4">Further adjust the devices mesh order for better collective performance.<br>130.1s -&gt; 128.6s</span></p><p class="c11"><span class="c9 c4">Exp2 optimization.<br>128.6s -&gt; 125.2s</span></p><p class="c11"><span class="c9 c4">Replicate a few layers weights after the cross attention.<br>125.2s -&gt; 124.9s=</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 388.00px;"><img alt="" src="images/image5.png" style="width: 624.00px; height: 388.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Chart"></span></p><a id="id.8zps605nsh31"></a><p class="c32"><span class="c9 c4">Chart above is for all steps of optimization</span></p><p class="c11 c8"><span class="c9 c4"></span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 388.00px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 388.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Chart"></span></p><p class="c32"><span class="c9 c4">Chart above is for the steps after 141-second milestone (zoomed in)</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h2 class="c5" id="h.gv6st3bgqg3j"><span class="c7 c4">Remark</span></h2><p class="c11"><span class="c9 c4">Optimization of the Wan2.1-14B text-to-video model on TPU v6e-16 successfully reduced the end-to-end generation time from 428 seconds to 124.9 seconds. This significant improvement was achieved through a series of strategic optimizations, primarily focusing on the Diffusion Transformer (DiT) component due to its dominant processing time. Key methods included adapting to the Jax backend with Torchax, implementing efficient VAE, applying various sharding strategies (FSDP, CP, SP), and extensively optimizing self-attention using the splash attention kernel. Further fine-tuning of elementwise operations, VPU computation overlapping within the kernel, and precise adjustment of block sizes contributed to the final performance gains. This detailed analysis and iterative optimization process highlight the critical role of hardware-aware kernel optimization and effective parallelism strategies in achieving high performance on TPU architectures for large-scale generative models.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h2 class="c5" id="h.yfz6i1z9g3fc"><span class="c7 c4">Appendix</span></h2><p class="c11"><span class="c9 c4">The appendix goes through the process of profiling, finding the bottleneck and some code snippets.</span></p><p class="c11"><span>The code could be referenced from </span><span class="c24"><a href="https://www.google.com/url?q=https://github.com/shungcp/diffusers/tree/df1e2157b6bbd67b98d8f26ecdcc4273f19cbd09&amp;sa=D&amp;source=editors&amp;ust=1765684106342532&amp;usg=AOvVaw1u7xwa3rj47WoeboSBgGSe">github</a></span><span>. Find the context inside the repo. Some of the snippets could not be found which are used in the middle of debugging.</span></p><p class="c11"><span class="c9 c4">Jax jit could shard all the activations automatically between the layers given the input and weights sharding.</span></p><h3 class="c18" id="h.ou2m3bncvm31"><span class="c26 c4">Result Reproduction</span></h3><p class="c11"><span class="c4 c9">First of all, create a v6e-16 Cloud TPU VM. The following command is an example:</span></p><p class="c3"><span>&#60419;</span><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">PROJECT_ID=tpu-prod-env-multipod</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">ZONE=us-east5-a</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">TPU_NAME=chishuen-v6e-16</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">ACCELERATOR_TYPE=v6e-16</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">RUNTIME_VERSION=v2-alpha-tpuv6e</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c2 c10">gcloud</span><span class="c0">&nbsp;</span><span class="c2 c10">compute</span><span class="c0">&nbsp;</span><span class="c2 c10">tpus</span><span class="c0">&nbsp;</span><span class="c2 c10">tpu-vm</span><span class="c0">&nbsp;</span><span class="c2 c10">create</span><span class="c0">&nbsp;</span><span class="c2 c10">${TPU_NAME}</span><span class="c0">&nbsp; </span><span class="c2 c10">\</span></p><p class="c3"><span class="c0">&nbsp; </span><span class="c2 c10">--zone=${ZONE}</span><span class="c0">&nbsp;</span><span class="c2 c10">\</span></p><p class="c3"><span class="c0">&nbsp; </span><span class="c2 c10">--project=${PROJECT_ID}</span><span class="c0">&nbsp;</span><span class="c2 c10">\</span></p><p class="c3"><span class="c0">&nbsp; </span><span class="c2 c10">--accelerator-type=${ACCELERATOR_TYPE}</span><span class="c0">&nbsp; </span><span class="c2 c10">\</span></p><p class="c3"><span class="c0">&nbsp; </span><span class="c2 c10">--version=${RUNTIME_VERSION}</span></p><p class="c11"><span>&#60418;Replace </span><span class="c14">PROJECT_ID</span><span>, </span><span class="c14">ZONE</span><span>, </span><span class="c14">TPU_NAME</span><span>&nbsp;with your own values.</span></p><p class="c11"><span>Then run the following shell script from a terminal with </span><span class="c14">gcloud</span><span>&nbsp;installed:</span><span class="c9 c4">&nbsp;</span></p><p class="c3"><span>&#60419;</span><span class="c4 c2 c12">#!/bin/bash</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">PROJECT_ID=tpu-prod-env-multipod</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">ZONE=us-east5-a</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">TPU_NAME=chishuen-v6e-16</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c4 c2 c12"># Set up environment on every TPU VM</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2 c10">setup_cmd=</span><span class="c0 c4">&quot;set -x &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">sudo apt update &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">sudo apt install -y python3.10-venv &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">python -m venv venv &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">source venv/bin/activate &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">git clone -b wan2-1-141s https://github.com/yuyanpeng-google/diffusers.git || true &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">cd diffusers &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">git fetch origin &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">git reset --hard origin/wan2-1-141s &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">pip install -e . &amp;&amp; \</span></p><p class="c3"><span class="c0">sh -ex setup-dep.sh&quot;</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2 c10">gcloud</span><span class="c2">&nbsp;</span><span class="c2 c10">compute</span><span class="c2">&nbsp;</span><span class="c2 c10">tpus</span><span class="c2">&nbsp;</span><span class="c2 c10">tpu-vm</span><span class="c2">&nbsp;</span><span class="c2 c10">ssh</span><span class="c2">&nbsp;</span><span class="c2 c10">--zone</span><span class="c2">&nbsp;</span><span class="c2 c10">$ZONE</span><span class="c2">&nbsp;</span><span class="c2 c10">$TPU_NAME</span><span class="c2">&nbsp;</span><span class="c2 c10">--project</span><span class="c2">&nbsp;</span><span class="c2 c10">$PROJECT_ID</span><span class="c2">&nbsp;</span><span class="c2 c10">--worker=all</span><span class="c2">&nbsp;</span><span class="c2 c10">--command=</span><span class="c0">&quot;$setup_cmd&quot;</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c4 c2 c12"># Run model</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2 c10">run_cmd=</span><span class="c0 c4">&quot;set -x &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">source ~/venv/bin/activate &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">killall -9 python || true &amp;&amp; \</span></p><p class="c3"><span class="c0 c4">sleep 10 &amp;&amp; \</span></p><p class="c3"><span class="c0">export JAX_COMPILATION_CACHE_DIR=&quot;</span><span class="c2 c10">/dev/shm/jax_cache</span><span class="c0">&quot;</span><span class="c2">&nbsp;</span><span class="c2 c10">&amp;&amp;</span><span class="c2">&nbsp;</span><span class="c2 c10">\</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">JAX_PERSISTENT_CACHE_MIN_ENTRY_SIZE_BYTES=</span><span class="c2 c20">-1</span><span class="c2">&nbsp;</span><span class="c2 c10">&amp;&amp;</span><span class="c2">&nbsp;</span><span class="c2 c10">\</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">JAX_PERSISTENT_CACHE_MIN_COMPILE_TIME_SECS=</span><span class="c2 c20">0</span><span class="c2">&nbsp;</span><span class="c2 c10">&amp;&amp;</span><span class="c2">&nbsp;</span><span class="c2 c10">\</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">JAX_PERSISTENT_CACHE_ENABLE_XLA_CACHES=</span><span class="c0">&#39;xla_gpu_per_fusion_autotune_cache_dir&#39;</span><span class="c2">&nbsp;</span><span class="c2 c10">&amp;&amp;</span><span class="c2">&nbsp;</span><span class="c2 c10">\</span></p><p class="c3"><span class="c2 c13">export</span><span class="c2">&nbsp;</span><span class="c2 c10">HF_HUB_CACHE=/dev/shm/hf_cache</span><span class="c2">&nbsp;</span><span class="c2 c10">&amp;&amp;</span><span class="c2">&nbsp;</span><span class="c2 c10">\</span></p><p class="c3"><span class="c2 c13">cd</span><span class="c2">&nbsp;</span><span class="c2 c10">diffusers</span><span class="c2">&nbsp;</span><span class="c2 c10">&amp;&amp;</span><span class="c2">&nbsp;</span><span class="c2 c10">\</span></p><p class="c3"><span class="c2 c10">git</span><span class="c2">&nbsp;</span><span class="c2 c10">fetch</span><span class="c2">&nbsp;</span><span class="c2 c10">&amp;&amp;</span><span class="c2">&nbsp;</span><span class="c2 c10">git</span><span class="c2">&nbsp;</span><span class="c2 c10">reset</span><span class="c2">&nbsp;</span><span class="c2 c10">--hard</span><span class="c2">&nbsp;</span><span class="c2 c10">origin/wan2-1-125s</span><span class="c2">&nbsp;</span><span class="c2 c10">&amp;&amp;</span><span class="c2">&nbsp;</span><span class="c2 c10">\</span></p><p class="c3"><span class="c2 c10">python</span><span class="c2">&nbsp;</span><span class="c2 c10">wan_tx_splash_attn.py</span><span class="c0">&quot;</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2 c10">sleep</span><span class="c2">&nbsp;</span><span class="c2 c20">1</span></p><p class="c3"><span class="c2 c10">gcloud</span><span class="c2">&nbsp;</span><span class="c2 c10">compute</span><span class="c2">&nbsp;</span><span class="c2 c10">tpus</span><span class="c2">&nbsp;</span><span class="c2 c10">tpu-vm</span><span class="c2">&nbsp;</span><span class="c2 c10">ssh</span><span class="c2">&nbsp;</span><span class="c2 c10">--zone</span><span class="c2">&nbsp;</span><span class="c2 c10">$ZONE</span><span class="c2">&nbsp;</span><span class="c2 c10">$TPU_NAME</span><span class="c2">&nbsp;</span><span class="c2 c10">--project</span><span class="c2">&nbsp;</span><span class="c2 c10">$PROJECT_ID</span><span class="c2">&nbsp;</span><span class="c2 c10">--worker=all</span><span class="c2">&nbsp;</span><span class="c2 c10">--command=</span><span class="c0">&quot;$run_cmd&quot;</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c4 c2 c12"># Copy the generated video back to local</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2 c10">sleep</span><span class="c2">&nbsp;</span><span class="c2 c20">1</span></p><p class="c3"><span class="c2 c10">gcloud</span><span class="c2">&nbsp;</span><span class="c2 c10">compute</span><span class="c2">&nbsp;</span><span class="c2 c10">tpus</span><span class="c2">&nbsp;</span><span class="c2 c10">tpu-vm</span><span class="c2">&nbsp;</span><span class="c2 c10">scp</span><span class="c2">&nbsp;</span><span class="c2 c10">--zone</span><span class="c2">&nbsp;</span><span class="c2 c10">$ZONE</span><span class="c2">&nbsp;</span><span class="c2 c10">$TPU_NAME:~/diffusers/*.mp4</span><span class="c2">&nbsp;</span><span class="c2 c10">.</span><span class="c2">&nbsp;</span><span class="c2 c10">--project</span><span class="c2">&nbsp;</span><span class="c2 c10">$PROJECT_ID</span><span class="c2">&nbsp;</span><span class="c2 c10">--worker=</span><span class="c2 c20">0</span></p><p class="c11"><span class="c9 c4">&#60418;</span></p><p class="c11"><span class="c9 c4">You should be able to see the following output on your terminal, i.e. a 5-second long, 720P video is generated in 125 seconds:</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 220.00px;"><img alt="" src="images/image6.png" style="width: 624.00px; height: 220.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span class="c9 c4">The generated video (.mp4 file) should be available on your local machine in the local directory where the shell script was run:</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 394.67px;"><img alt="" src="images/image1.png" style="width: 624.00px; height: 394.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c18" id="h.wv2cdr1s2kb"><span class="c4 c26">Text Embedding</span></h3><p class="c3"><span>&#60419;</span><span class="c0">text_encoder_shardings</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">{</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">&#39;shared.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;dp&#39;,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([256384,</span><span class="c2">&nbsp;</span><span class="c0">4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">&#39;encoder.block.*.layer.*.SelfAttention.q.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;dp&#39;,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([4096,</span><span class="c2">&nbsp;</span><span class="c0">4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">&#39;encoder.block.*.layer.*.SelfAttention.k.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;dp&#39;,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([4096,</span><span class="c2">&nbsp;</span><span class="c0">4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">&#39;encoder.block.*.layer.*.SelfAttention.v.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;dp&#39;,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([4096,</span><span class="c2">&nbsp;</span><span class="c0">4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">&#39;encoder.block.*.layer.*.SelfAttention.o.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(None,</span><span class="c2">&nbsp;</span><span class="c0">(axis,&#39;dp&#39;,&#39;sp&#39;)),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([4096,</span><span class="c2">&nbsp;</span><span class="c0">4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;encoder.block.*.layer.*.SelfAttention.relative_attention_bias.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([32,</span><span class="c2">&nbsp;</span><span class="c0">64]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;encoder.block.*.layer.*.layer_norm.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">&#39;encoder.block.*.layer.*.DenseReluDense.wi_0.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;dp&#39;,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([10240,</span><span class="c2">&nbsp;</span><span class="c0">4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">&#39;encoder.block.*.layer.*.DenseReluDense.wi_1.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;dp&#39;,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([10240,</span><span class="c2">&nbsp;</span><span class="c0">4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">&#39;encoder.block.*.layer.*.DenseReluDense.wo.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(None,</span><span class="c2">&nbsp;</span><span class="c0">(axis,&#39;dp&#39;,&#39;sp&#39;)),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([4096,</span><span class="c2">&nbsp;</span><span class="c0">10240]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;encoder.final_layer_norm.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">}</span></p><p class="c11"><span class="c9 c4">&#60418;Define all the weights sharding.</span></p><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span></p><p class="c3"><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">_shard_weight_dict(weight_dict,</span><span class="c2">&nbsp;</span><span class="c0">sharding_dict,</span><span class="c2">&nbsp;</span><span class="c0">mesh):</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">result</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">{}</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">k,</span><span class="c2">&nbsp;</span><span class="c0">v</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">weight_dict.items():</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">target,</span><span class="c2">&nbsp;</span><span class="c0">sharding</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">sharding_dict.items():</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">re.fullmatch(target,</span><span class="c2">&nbsp;</span><span class="c0">k)</span><span class="c2">&nbsp;</span><span class="c0">is</span><span class="c2">&nbsp;</span><span class="c0">not</span><span class="c2">&nbsp;</span><span class="c0">None:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">v.apply_jax_(jax.device_put,</span><span class="c2">&nbsp;</span><span class="c0">NamedSharding(mesh,</span><span class="c2">&nbsp;</span><span class="c0">P(*sharding)))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">break</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">else:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">replicate</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">v.apply_jax_(jax.device_put,</span><span class="c2">&nbsp;</span><span class="c0">NamedSharding(mesh,</span><span class="c2">&nbsp;</span><span class="c0">P()))</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">result[k]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">v</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0 c4">result</span></p><p class="c3"><span class="c0 c4">...</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp;pipe.text_encoder = torchax.compile(pipe.text_encoder)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; pipe.text_encoder.params = _shard_weight_dict(pipe.text_encoder.params, text_encoder_shardings, mesh)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; pipe.text_encoder.buffers = _shard_weight_dict(pipe.text_encoder.buffers, text_encoder_shardings, mesh)</span></p><p class="c11"><span class="c9 c4">&#60418;Shard the weights. Afterward the compiler will know using TP.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h3 class="c18" id="h.ciwmn2xrrkcn"><span class="c26 c4">VAE</span></h3><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">load_wan_vae_fixed(pretrained_model_name_or_path:</span><span class="c2">&nbsp;</span><span class="c0">str,</span><span class="c2">&nbsp;</span><span class="c0">eval_shapes:</span><span class="c2">&nbsp;</span><span class="c0">dict,</span><span class="c2">&nbsp;</span><span class="c0">device:</span><span class="c2">&nbsp;</span><span class="c0">str,</span><span class="c2">&nbsp;</span><span class="c0">hf_download:</span><span class="c2">&nbsp;</span><span class="c0">bool</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">True):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">&quot;&quot;&quot;Fixed</span><span class="c2">&nbsp;</span><span class="c0">version</span><span class="c2">&nbsp;</span><span class="c0">of</span><span class="c2">&nbsp;</span><span class="c0">load_wan_vae</span><span class="c2">&nbsp;</span><span class="c0">that</span><span class="c2">&nbsp;</span><span class="c0">avoids</span><span class="c2">&nbsp;</span><span class="c0">torch2jax</span><span class="c2">&nbsp;</span><span class="c0">issues&quot;&quot;&quot;</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">torch</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">huggingface_hub</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">hf_hub_download</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">safetensors</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">safe_open</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">flax.traverse_util</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">unflatten_dict,</span><span class="c2">&nbsp;</span><span class="c0">flatten_dict</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; </span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">device_obj</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.local_devices(backend=device)[0]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">jax.default_device(device_obj):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">hf_download:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">ckpt_path</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">hf_hub_download(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">pretrained_model_name_or_path,</span><span class="c2">&nbsp;</span><span class="c0">subfolder=&quot;vae&quot;,</span><span class="c2">&nbsp;</span><span class="c0">filename=&quot;diffusion_pytorch_model.safetensors&quot;</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">print(f&quot;Load</span><span class="c2">&nbsp;</span><span class="c0">and</span><span class="c2">&nbsp;</span><span class="c0">port</span><span class="c2">&nbsp;</span><span class="c0">Wan</span><span class="c2">&nbsp;</span><span class="c0">2.1</span><span class="c2">&nbsp;</span><span class="c0">VAE</span><span class="c2">&nbsp;</span><span class="c0">on</span><span class="c2">&nbsp;</span><span class="c0">{device}&quot;)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">ckpt_path</span><span class="c2">&nbsp;</span><span class="c0">is</span><span class="c2">&nbsp;</span><span class="c0">not</span><span class="c2">&nbsp;</span><span class="c0">None:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">tensors</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">{}</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">Use</span><span class="c2">&nbsp;</span><span class="c0">safetensors</span><span class="c2">&nbsp;</span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">numpy</span><span class="c2">&nbsp;</span><span class="c0">framework</span><span class="c2">&nbsp;</span><span class="c0">to</span><span class="c2">&nbsp;</span><span class="c0">avoid</span><span class="c2">&nbsp;</span><span class="c0">torchax</span><span class="c2">&nbsp;</span><span class="c0">interference</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">safe_open(ckpt_path,</span><span class="c2">&nbsp;</span><span class="c0">framework=&quot;np&quot;)</span><span class="c2">&nbsp;</span><span class="c0">as</span><span class="c2">&nbsp;</span><span class="c0">f:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">k</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">f.keys():</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">Get</span><span class="c2">&nbsp;</span><span class="c0">numpy</span><span class="c2">&nbsp;</span><span class="c0">array</span><span class="c2">&nbsp;</span><span class="c0">directly</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">numpy_tensor</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">f.get_tensor(k)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">tensors[k]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.array(numpy_tensor)</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">flax_state_dict</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">{}</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">cpu</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.local_devices(backend=&quot;cpu&quot;)[0]</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">Import</span><span class="c2">&nbsp;</span><span class="c0">the</span><span class="c2">&nbsp;</span><span class="c0">utility</span><span class="c2">&nbsp;</span><span class="c0">functions</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">maxdiffusion.models.modeling_flax_pytorch_utils</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">rename_key,</span><span class="c2">&nbsp;</span><span class="c0">rename_key_and_reshape_tensor,</span><span class="c2">&nbsp;</span><span class="c0">validate_flax_state_dict</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">pt_key,</span><span class="c2">&nbsp;</span><span class="c0">tensor</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">tensors.items():</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">rename_key(pt_key)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">Order</span><span class="c2">&nbsp;</span><span class="c0">matters</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;up_blocks_&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;up_blocks.&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;mid_block_&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;mid_block.&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;down_blocks_&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;down_blocks.&quot;)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;conv_in.bias&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;conv_in.conv.bias&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;conv_in.weight&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;conv_in.conv.weight&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;conv_out.bias&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;conv_out.conv.bias&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;conv_out.weight&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;conv_out.conv.weight&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;attentions_&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;attentions.&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;resnets_&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;resnets.&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;upsamplers_&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;upsamplers.&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;resample_&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;resample.&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;conv1.bias&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;conv1.conv.bias&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;conv1.weight&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;conv1.conv.weight&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;conv2.bias&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;conv2.conv.bias&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;conv2.weight&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;conv2.conv.weight&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;time_conv.bias&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;time_conv.conv.bias&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;time_conv.weight&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;time_conv.conv.weight&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;quant_conv&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;quant_conv.conv&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;conv_shortcut&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;conv_shortcut.conv&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">&quot;decoder&quot;</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;resample.1.bias&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;resample.layers.1.bias&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;resample.1.weight&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;resample.layers.1.weight&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">&quot;encoder&quot;</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">renamed_pt_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">renamed_pt_key.replace(&quot;resample.1&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;resample.conv&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">pt_tuple_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">tuple(renamed_pt_key.split(&quot;.&quot;))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">flax_key,</span><span class="c2">&nbsp;</span><span class="c0">flax_tensor</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">rename_key_and_reshape_tensor(pt_tuple_key,</span><span class="c2">&nbsp;</span><span class="c0">tensor,</span><span class="c2">&nbsp;</span><span class="c0">eval_shapes)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">flax_key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">tuple(int(item)</span><span class="c2">&nbsp;</span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">isinstance(item,</span><span class="c2">&nbsp;</span><span class="c0">str)</span><span class="c2">&nbsp;</span><span class="c0">and</span><span class="c2">&nbsp;</span><span class="c0">item.isdigit()</span><span class="c2">&nbsp;</span><span class="c0">else</span><span class="c2">&nbsp;</span><span class="c0">item</span><span class="c2">&nbsp;</span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">item</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">flax_key)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">flax_state_dict[flax_key]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.device_put(jnp.asarray(flax_tensor),</span><span class="c2">&nbsp;</span><span class="c0">device=cpu)</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">validate_flax_state_dict(eval_shapes,</span><span class="c2">&nbsp;</span><span class="c0">flax_state_dict)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">flax_state_dict</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">unflatten_dict(flax_state_dict)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">del</span><span class="c2">&nbsp;</span><span class="c0">tensors</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">jax.clear_caches()</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">else:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">raise</span><span class="c2">&nbsp;</span><span class="c0">FileNotFoundError(f&quot;Path</span><span class="c2">&nbsp;</span><span class="c0">{ckpt_path}</span><span class="c2">&nbsp;</span><span class="c0">was</span><span class="c2">&nbsp;</span><span class="c0">not</span><span class="c2">&nbsp;</span><span class="c0">found&quot;)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">flax_state_dict</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">sharded_device_put(tensor,</span><span class="c2">&nbsp;</span><span class="c0">sharding):</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">isinstance(tensor,</span><span class="c2">&nbsp;</span><span class="c0">tuple):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">tuple(sharded_device_put(t,</span><span class="c2">&nbsp;</span><span class="c0">sharding)</span><span class="c2">&nbsp;</span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">t</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">tensor)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">num_global_devices</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.device_count()</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">num_local_devices</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.local_device_count()</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">num_global_devices</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">num_local_devices:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">jax.device_put(tensor,</span><span class="c2">&nbsp;</span><span class="c0">sharding)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">shape</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">tensor.shape</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">x_split</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">[</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">jax.device_put(tensor[i],</span><span class="c2">&nbsp;</span><span class="c0">device)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">device,</span><span class="c2">&nbsp;</span><span class="c0">i</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">sharding.addressable_devices_indices_map(shape).items()</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">]</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">jax.make_array_from_single_device_arrays(shape,</span><span class="c2">&nbsp;</span><span class="c0">sharding,</span><span class="c2">&nbsp;</span><span class="c0 c4">x_split)</span></p><p class="c3"><span class="c0 c4">...</span></p><p class="c3"><span class="c0 c4">&nbsp; # Load pretrained weights</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; graphdef, state = nnx.split(wan_vae)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; params = state.to_pure_dict()</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; params = load_wan_vae_fixed(model_id, params, &quot;tpu&quot;)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; sharding = NamedSharding(mesh, P())</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; params = jax.tree_util.tree_map(lambda x: sharded_device_put(x, sharding), params)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; params = jax.tree_util.tree_map(lambda x: x.astype(jnp.bfloat16), params)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; wan_vae = nnx.merge(graphdef, params)</span></p><p class="c11"><span class="c9 c4">&#60418;Load and rename the weights from the diffusers to match the layer definition in Maxdiffusion. </span></p><p class="c3"><span>&#60419;</span><span class="c0">ValueError:</span><span class="c2">&nbsp;</span><span class="c0">device_put&#39;s</span><span class="c2">&nbsp;</span><span class="c0">second</span><span class="c2">&nbsp;</span><span class="c0">argument</span><span class="c2">&nbsp;</span><span class="c0">must</span><span class="c2">&nbsp;</span><span class="c0">be</span><span class="c2">&nbsp;</span><span class="c0">a</span><span class="c2">&nbsp;</span><span class="c0">Device</span><span class="c2">&nbsp;</span><span class="c0">or</span><span class="c2">&nbsp;</span><span class="c0">a</span><span class="c2">&nbsp;</span><span class="c0">Sharding</span><span class="c2">&nbsp;</span><span class="c0">which</span><span class="c2">&nbsp;</span><span class="c0">represents</span><span class="c2">&nbsp;</span><span class="c0">addressable</span><span class="c2">&nbsp;</span><span class="c0">devices,</span><span class="c2">&nbsp;</span><span class="c0">but</span><span class="c2">&nbsp;</span><span class="c0">got</span><span class="c2">&nbsp;</span><span class="c0">NamedSharding(mesh=Mesh(&#39;axis&#39;:</span><span class="c2">&nbsp;</span><span class="c0">16,</span><span class="c2">&nbsp;</span><span class="c0">axis_types=(Auto,)),</span><span class="c2">&nbsp;</span><span class="c0">spec=PartitionSpec(),</span><span class="c2">&nbsp;</span><span class="c0">memory_kind=device).</span><span class="c2">&nbsp;</span><span class="c0">Please</span><span class="c2">&nbsp;</span><span class="c0">pass</span><span class="c2">&nbsp;</span><span class="c0">device</span><span class="c2">&nbsp;</span><span class="c0">or</span><span class="c2">&nbsp;</span><span class="c0">Sharding</span><span class="c2">&nbsp;</span><span class="c0">which</span><span class="c2">&nbsp;</span><span class="c0">represents</span><span class="c2">&nbsp;</span><span class="c0">addressable</span><span class="c2">&nbsp;</span><span class="c0">devices.</span></p><p class="c11"><span class="c9 c4">&#60418;Encountered error non-addressable devices in a multi-host environment, &nbsp;use sharded_device_put.</span></p><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span><span class="c0 c4">class ConfigWrapper:</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; def __init__(self, **kwargs):</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; self.__dict__.update(kwargs)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; def __getitem__(self, key):</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; return getattr(self, key)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; def __setitem__(self, key, value):</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; setattr(self, key, value)</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c0">class</span><span class="c2">&nbsp;</span><span class="c0">VAEProxy:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">__init__(self,</span><span class="c2">&nbsp;</span><span class="c0">vae,</span><span class="c2">&nbsp;</span><span class="c0">vae_cache,</span><span class="c2">&nbsp;</span><span class="c0">dtype,</span><span class="c2">&nbsp;</span><span class="c0">config):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">self._vae</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">vae</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">self.vae_cache</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">vae_cache</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">self.dtype</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">dtype</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">self.config</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">config</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">__getattr__(self,</span><span class="c2">&nbsp;</span><span class="c0">name):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">getattr(self._vae,</span><span class="c2">&nbsp;</span><span class="c0">name)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">decode(self,</span><span class="c2">&nbsp;</span><span class="c0">*args,</span><span class="c2">&nbsp;</span><span class="c0">**kwargs):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">&#39;feat_cache&#39;</span><span class="c2">&nbsp;</span><span class="c0">not</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">kwargs:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">kwargs[&#39;feat_cache&#39;]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">self.vae_cache</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">out</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">self._vae.decode(*args,</span><span class="c2">&nbsp;</span><span class="c0">**kwargs)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0 c4">to_torch_recursive(out)</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c0 c4">...</span></p><p class="c3"><span class="c0 c4">&nbsp; vae_config = ConfigWrapper(</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; latents_mean=np.array(wan_vae.latents_mean),</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; latents_std=np.array(wan_vae.latents_std),</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; z_dim=wan_vae.z_dim</span></p><p class="c3"><span class="c0 c4">&nbsp; )</span></p><p class="c3"><span class="c0 c4">&nbsp; pipe.vae.config = vae_config</span></p><p class="c3"><span class="c0 c4">&nbsp; pipe.vae = VAEProxy(wan_vae, vae_cache, torch.bfloat16, vae_config)</span></p><p class="c3"><span class="c0 c4">&nbsp; pipe.vae_cache = vae_cache</span></p><p class="c11"><span class="c9 c4">&#60418;Add VAEProxy to use jax code in the torch.</span></p><p class="c11"><span class="c9 c4">VAE in the maxdiffusion package at the time memory usage peak is high and OOM. Optimize the VAE.</span></p><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span><span class="c0 c4">diff --git a/src/maxdiffusion/models/wan/autoencoder_kl_wan.py b/src/maxdiffusion/models/wan/autoencoder_kl_wan.py</span></p><p class="c3"><span class="c0 c4">index 19244f72..1c7b3eee 100644</span></p><p class="c3"><span class="c0 c4">--- a/src/maxdiffusion/models/wan/autoencoder_kl_wan.py</span></p><p class="c3"><span class="c0 c4">+++ b/src/maxdiffusion/models/wan/autoencoder_kl_wan.py</span></p><p class="c3"><span class="c0 c4">@@ -116,8 +116,9 @@ def __call__(self, x: jax.Array, cache_x: Optional[jax.Array] = None, idx=-1) -&gt;</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp;x_padded = jnp.pad(x, padding_to_apply, mode=&quot;constant&quot;, constant_values=0.0)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp;else:</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp;x_padded = x</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp;out = self.conv(x_padded)</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp;return out</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp;del x</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp;# out = self.conv(x_padded)</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp;return self.conv(x_padded)</span></p><p class="c3"><span class="c0 c4">&nbsp;</span></p><p class="c3"><span class="c0 c4">&nbsp;</span></p><p class="c3"><span class="c0 c4">&nbsp;class WanRMS_norm(nnx.Module):</span></p><p class="c3"><span class="c0 c4">@@ -419,13 +420,23 @@ def __call__(self, x: jax.Array, feat_cache=None, feat_idx=[0]):</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp;x = self.nonlinearity(x)</span></p><p class="c3"><span class="c0 c4">&nbsp;</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp;if feat_cache is not None:</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# idx = feat_idx[0]</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# cache_x = jnp.copy(x[:, -CACHE_T:, :, :, :])</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# if cache_x.shape[1] &lt; 2 and feat_cache[idx] is not None:</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# &nbsp; cache_x = jnp.concatenate([jnp.expand_dims(feat_cache[idx][:, -1, :, :, :], axis=1), cache_x], axis=1)</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# x = self.conv1(x, feat_cache[idx], idx)</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# feat_cache[idx] = cache_x</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# feat_idx[0] += 1</span></p><p class="c3"><span class="c0 c4">+</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp;idx = feat_idx[0]</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp;cache_x = jnp.copy(x[:, -CACHE_T:, :, :, :])</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp;if cache_x.shape[1] &lt; 2 and feat_cache[idx] is not None:</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp; &nbsp;cache_x = jnp.concatenate([jnp.expand_dims(feat_cache[idx][:, -1, :, :, :], axis=1), cache_x], axis=1)</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp;x = self.conv1(x, feat_cache[idx], idx)</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp;feat_cache[idx] = cache_x</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;x2 = self.conv1(x, feat_cache[idx], idx)</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;if x.shape[1] &lt; 2 and feat_cache[idx] is not None:</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp; &nbsp;feat_cache[idx] = jnp.concatenate([jnp.expand_dims(feat_cache[idx][:, -1, :, :, :], axis=1), x[:, -CACHE_T:, :, :, :]], axis=1)</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;else:</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp; &nbsp;feat_cache[idx] = x[:, -CACHE_T:, :, :, :]</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp;feat_idx[0] += 1</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;x = x2</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;del x2</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp;else:</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp;x = self.conv1(x)</span></p><p class="c3"><span class="c0 c4">&nbsp;</span></p><p class="c3"><span class="c0 c4">@@ -434,13 +445,23 @@ def __call__(self, x: jax.Array, feat_cache=None, feat_idx=[0]):</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp;idx = feat_idx[0]</span></p><p class="c3"><span class="c0 c4">&nbsp;</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp;if feat_cache is not None:</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# idx = feat_idx[0]</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# cache_x = jnp.copy(x[:, -CACHE_T:, :, :, :])</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# if cache_x.shape[1] &lt; 2 and feat_cache[idx] is not None:</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# &nbsp; cache_x = jnp.concatenate([jnp.expand_dims(feat_cache[idx][:, -1, :, :, :], axis=1), cache_x], axis=1)</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# x = self.conv2(x, feat_cache[idx])</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# feat_cache[idx] = cache_x</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;# feat_idx[0] += 1</span></p><p class="c3"><span class="c0 c4">+</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp;idx = feat_idx[0]</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp;cache_x = jnp.copy(x[:, -CACHE_T:, :, :, :])</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp;if cache_x.shape[1] &lt; 2 and feat_cache[idx] is not None:</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp; &nbsp;cache_x = jnp.concatenate([jnp.expand_dims(feat_cache[idx][:, -1, :, :, :], axis=1), cache_x], axis=1)</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp;x = self.conv2(x, feat_cache[idx])</span></p><p class="c3"><span class="c0 c4">- &nbsp; &nbsp; &nbsp;feat_cache[idx] = cache_x</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;x2 = self.conv2(x, feat_cache[idx])</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;if x.shape[1] &lt; 2 and feat_cache[idx] is not None:</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp; &nbsp;feat_cache[idx] = jnp.concatenate([jnp.expand_dims(feat_cache[idx][:, -1, :, :, :], axis=1), x[:, -CACHE_T:, :, :, :]], axis=1)</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;else:</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp; &nbsp;feat_cache[idx] = x[:, -CACHE_T:, :, :, :]</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp;feat_idx[0] += 1</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;x = x2</span></p><p class="c3"><span class="c0 c4">+ &nbsp; &nbsp; &nbsp;del x2</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp;else:</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp;x = self.conv2(x)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp;x = x + h</span></p><p class="c11"><span class="c9 c4">&#60418;Since the VAE cannot easily modify the jit at the time, the memory management needs to be more careful and released as soon as possible.</span></p><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">shard</span><span class="c2">&nbsp;</span><span class="c0">vae</span></p><p class="c3"><span class="c0">LOGICAL_AXIS_RULES</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">(&#39;conv_out&#39;,</span><span class="c2">&nbsp;</span><span class="c0">(&#39;axis&#39;,&#39;dp&#39;,&#39;sp&#39;)),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">(&#39;conv_in&#39;,</span><span class="c2">&nbsp;</span><span class="c0">(&#39;axis&#39;,&#39;dp&#39;,&#39;sp&#39;))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0 c4">)</span></p><p class="c3"><span class="c0 c4">@nnx.jit(static_argnums=(1,), donate_argnums=(0,))</span></p><p class="c3"><span class="c0 c4">def create_sharded_logical_model(model, logical_axis_rules):</span></p><p class="c3"><span class="c0 c4">&nbsp; graphdef, state, rest_of_state = nnx.split(model, nnx.Param, ...)</span></p><p class="c3"><span class="c0 c4">&nbsp; p_add_sharding_rule = functools.partial(_add_sharding_rule, logical_axis_rules=logical_axis_rules)</span></p><p class="c3"><span class="c0 c4">&nbsp; state = jax.tree.map(p_add_sharding_rule, state, is_leaf=lambda x: isinstance(x, nnx.VariableState))</span></p><p class="c3"><span class="c0 c4">&nbsp; pspecs = nnx.get_partition_spec(state)</span></p><p class="c3"><span class="c0 c4">&nbsp; sharded_state = jax.lax.with_sharding_constraint(state, pspecs)</span></p><p class="c3"><span class="c0 c4">&nbsp; model = nnx.merge(graphdef, sharded_state, rest_of_state)</span></p><p class="c3"><span class="c0 c4">&nbsp; return model</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c0 c4">...</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; p_create_sharded_logical_model = functools.partial(create_sharded_logical_model, logical_axis_rules=LOGICAL_AXIS_RULES)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; wan_vae = p_create_sharded_logical_model(model=wan_vae)</span></p><p class="c3"><span class="c0 c4">...</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; with mesh, nn_partitioning.axis_rules(LOGICAL_AXIS_RULES):</span></p><p class="c3"><span class="c0 c4">...</span></p><p class="c11"><span class="c9 c4">&#60418;Create the sharding model and use them with the mesh context.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h3 class="c18" id="h.in8ai5h1fz0b"><span class="c26 c4">Dit</span></h3><p class="c11"><span class="c9 c4">At first, tensor parallelism (TP) strategy is used. All the weights are sharded along according to the TP, and then the activation sharding will automatically be decided.</span></p><p class="c11"><span class="c9 c4">The device mesh is (dp, sp, axis), which dp=2, sp=1, axis=8 on v6e-16. The sp is not used in v6e-16. It is for the v6e-32, in which the attention head is not divided by 16 (with dp=2, remaining 16), and self-attention needs to mix CP with SP.</span></p><p class="c3"><span>&#60419;</span><span class="c0 c4"># The tensor size is wan2.1 1.3B models, while the names and strategy are the same</span></p><p class="c3"><span class="c0 c4"># Comment out means full replicate the weight</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c0">transformer_shardings_tp</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">{</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;scale_shift_table&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1,</span><span class="c2">&nbsp;</span><span class="c0">2,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.float32)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;patch_embedding.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">16,</span><span class="c2">&nbsp;</span><span class="c0">1,</span><span class="c2">&nbsp;</span><span class="c0">2,</span><span class="c2">&nbsp;</span><span class="c0">2]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;patch_embedding.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;condition_embedder.time_embedder.linear_1.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">None),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">256]),</span><span class="c2">&nbsp;</span><span class="c0">torch.float32)</span></p><p class="c3"><span class="c0">r&#39;condition_embedder.time_embedder.linear_1.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.float32)</span></p><p class="c3"><span class="c0">r&#39;condition_embedder.time_embedder.linear_2.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(None,</span><span class="c2">&nbsp;</span><span class="c0">(axis,&#39;sp&#39;)),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.float32)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;condition_embedder.time_embedder.linear_2.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.float32)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;condition_embedder.time_proj.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([9216,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;condition_embedder.time_proj.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([9216]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;condition_embedder.text_embedder.linear_1.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">None),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">4096]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;condition_embedder.text_embedder.linear_1.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;condition_embedder.text_embedder.linear_2.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(None,</span><span class="c2">&nbsp;</span><span class="c0">(axis,&#39;sp&#39;)),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;condition_embedder.text_embedder.linear_2.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.scale_shift_table&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1,</span><span class="c2">&nbsp;</span><span class="c0">6,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.float32)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn1.norm_q.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn1.norm_k.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn1.to_q.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">None),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn1.to_q.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn1.to_k.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn1.to_k.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn1.to_v.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn1.to_v.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">to_out</span><span class="c2">&nbsp;</span><span class="c0">has</span><span class="c2">&nbsp;</span><span class="c0">2</span><span class="c2">&nbsp;</span><span class="c0">submodules,</span><span class="c2">&nbsp;</span><span class="c0">the</span><span class="c2">&nbsp;</span><span class="c0">first</span><span class="c2">&nbsp;</span><span class="c0">is</span><span class="c2">&nbsp;</span><span class="c0">the</span><span class="c2">&nbsp;</span><span class="c0">Linear</span><span class="c2">&nbsp;</span><span class="c0">and</span><span class="c2">&nbsp;</span><span class="c0">second</span><span class="c2">&nbsp;</span><span class="c0">is</span><span class="c2">&nbsp;</span><span class="c0">dropout</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn1.to_out.0.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(None,</span><span class="c2">&nbsp;</span><span class="c0">(axis,&#39;sp&#39;)),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn1.to_out.0.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn1.to_out.1.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn1.to_out.1.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn2.norm_q.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn2.norm_k.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn2.to_q.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn2.to_q.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn2.to_k.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn2.to_k.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn2.to_v.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn2.to_v.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.attn2.to_out.0.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(None,</span><span class="c2">&nbsp;</span><span class="c0">(axis,&#39;sp&#39;)),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn2.to_out.0.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn2.to_out.1.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.attn2.to_out.1.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.norm2.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.float32)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.norm2.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.float32)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.ffn.net.0.proj.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([8960,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.ffn.net.0.proj.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">((axis,&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([8960]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">r&#39;blocks.\d+.ffn.net.2.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(None,</span><span class="c2">&nbsp;</span><span class="c0">(axis,&#39;sp&#39;)),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536,</span><span class="c2">&nbsp;</span><span class="c0">8960]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;blocks.\d+.ffn.net.2.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;proj_out.weight&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([64,</span><span class="c2">&nbsp;</span><span class="c0">1536]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">&#39;proj_out.bias&#39;:</span><span class="c2">&nbsp;</span><span class="c0">(),</span><span class="c2">&nbsp;</span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">(torch.Size([64]),</span><span class="c2">&nbsp;</span><span class="c0">torch.bfloat16)</span></p><p class="c3"><span class="c0">}</span></p><p class="c11"><span class="c9 c4">&#60418;</span></p><p class="c11"><span class="c9 c4">Also found that HBM is OOM with large sequence self attention, using flash attention kernel and splash attention kernel for attention.</span></p><p class="c3"><span class="c9 c4">Add profiler for DiT 3 steps to prevent too large and truncated profiling.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; &nbsp;</span><span class="c0">jax.profiler.start_trace(PROFILE_OUT_PATH)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">output</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pipe(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">prompt=prompt,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">negative_prompt=negative_prompt,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">height=args.height,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">width=args.width,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">num_inference_steps=3,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">num_frames=args.frames,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">guidance_scale=5.0,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">output_type=&quot;latent&quot;,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">generator=generator,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">use_dp=args.use_dp,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">jax.effects_barrier()</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">jax.profiler.stop_trace()</span></p><p class="c11"><span class="c9 c4">&#60418;</span></p><p class="c11"><span class="c9 c4">Found that 66% time custom-call which is the attention kernel and 17.5% for all-reduce communication.</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 266.28px; height: 203.35px;"><img alt="" src="images/image8.png" style="width: 266.28px; height: 203.35px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 60.00px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 60.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span class="c9 c4">Found that all_reduce are due to TP. TP needs an all_reduce after each linear block, which is not efficient since the sequence 75600 is much larger than contract dimensions.</span></p><p class="c11"><span class="c9 c4">Try sequence parallelism which could prevent communications except attention.</span></p><p class="c3"><span class="c9 c4">Add mark_sharding after the patch_embedding, which constrains the sharding at the inner state, and the compiler would know to do the sequence sharding.</span></p><p class="c3"><span>&#60419;</span><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">torchax</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">interop</span></p><p class="c3"><span class="c0">mark_sharding</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0 c4">interop.torch_view(jax.lax.with_sharding_constraint)</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c0 c4">...</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; hidden_states = self.patch_embedding(hidden_states)</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; hidden_states = hidden_states.flatten(2).transpose(1, 2)</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; # hidden_states=[batch, latent//4+1 * height//8//2 * width//8//2, dim]</span></p><p class="c3"><span class="c0 c4">&nbsp; &nbsp; &nbsp; &nbsp; hidden_states = mark_sharding(hidden_states, P(&quot;dp&quot;, (&#39;axis&#39;,&#39;sp&#39;,), None))</span></p><p class="c11"><span class="c9 c4">&#60418;For self attention, try both CP and SP, and CP performs better. For cross attention, SP is better. Quick workaround condition by sequence length.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">Sharded</span><span class="c2">&nbsp;</span><span class="c0">case</span><span class="c2">&nbsp;</span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">Transformer.</span><span class="c2">&nbsp;</span><span class="c0">Split</span><span class="c2">&nbsp;</span><span class="c0">along</span><span class="c2">&nbsp;</span><span class="c0">the</span><span class="c2">&nbsp;</span><span class="c0">heads</span><span class="c2">&nbsp;</span><span class="c0">axis.</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">Attn1</span><span class="c2">&nbsp;</span><span class="c0">self</span><span class="c2">&nbsp;</span><span class="c0">attention,</span><span class="c2">&nbsp;</span><span class="c0">key</span><span class="c2">&nbsp;</span><span class="c0">length</span><span class="c2">&nbsp;</span><span class="c0">is</span><span class="c2">&nbsp;</span><span class="c0">long.</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">key.shape[2]</span><span class="c2">&nbsp;</span><span class="c0">&gt;</span><span class="c2">&nbsp;</span><span class="c0">10000:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">q_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;sp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">kv_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">else:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">Attn2</span><span class="c2">&nbsp;</span><span class="c0">which</span><span class="c2">&nbsp;</span><span class="c0">is</span><span class="c2">&nbsp;</span><span class="c0">cross</span><span class="c2">&nbsp;</span><span class="c0">attention,</span><span class="c2">&nbsp;</span><span class="c0">kv</span><span class="c2">&nbsp;</span><span class="c0">sequence</span><span class="c2">&nbsp;</span><span class="c0">is</span><span class="c2">&nbsp;</span><span class="c0">shorter.</span><span class="c2">&nbsp;</span><span class="c0">All</span><span class="c2">&nbsp;</span><span class="c0">gather</span><span class="c2">&nbsp;</span><span class="c0">the</span><span class="c2">&nbsp;</span><span class="c0">key</span><span class="c2">&nbsp;</span><span class="c0">value</span><span class="c2">&nbsp;</span><span class="c0">cost</span><span class="c2">&nbsp;</span><span class="c0">less.</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">q_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">(&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">kv_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c11"><span class="c9 c4">&#60418;Found a weird all-gather (async-collective) which should not exist with SP here.</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 412.00px;"><img alt="" src="images/image9.png" style="width: 624.00px; height: 412.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c9 c4">Add debug logs around the Q, K, V, and found that the automatically sharding is not as expected.</span></p><p class="c3"><span>&#60419;</span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">log_sharding_before(s,</span><span class="c2">&nbsp;</span><span class="c0">name):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">isinstance(s,</span><span class="c2">&nbsp;</span><span class="c0">jax.sharding.NamedSharding):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">print(&quot;[%s]</span><span class="c2">&nbsp;</span><span class="c0">SHARDING</span><span class="c2">&nbsp;</span><span class="c0">BEFORE:</span><span class="c2">&nbsp;</span><span class="c0">%s&quot;,</span><span class="c2">&nbsp;</span><span class="c0">name,</span><span class="c2">&nbsp;</span><span class="c0">s.spec)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">else:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">print(&quot;[%s]</span><span class="c2">&nbsp;</span><span class="c0">SHARDING</span><span class="c2">&nbsp;</span><span class="c0">BEFORE:</span><span class="c2">&nbsp;</span><span class="c0">%s&quot;,</span><span class="c2">&nbsp;</span><span class="c0">name,</span><span class="c2">&nbsp;</span><span class="c0">s.shape)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">jax.debug.inspect_array_sharding(query.jax(),</span><span class="c2">&nbsp;</span><span class="c0">callback=functools.partial(log_sharding_before,</span><span class="c2">&nbsp;</span><span class="c0 c4">name=&quot;query&quot;))</span></p><p class="c3"><span class="c9 c4">&#60418;</span></p><p class="c3"><span>&#60419;</span><span class="c0">[hidden_states]:</span><span class="c2">&nbsp;</span><span class="c0">PartitionSpec(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;)</span></p><p class="c3"><span class="c0">[query]:</span><span class="c2">&nbsp;</span><span class="c0">PartitionSpec(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;)</span></p><p class="c3"><span class="c0">[key]:</span><span class="c2">&nbsp;</span><span class="c0">PartitionSpec(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;)</span></p><p class="c3"><span class="c0">[value]:</span><span class="c2">&nbsp;</span><span class="c0">PartitionSpec(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;)</span></p><p class="c3"><span class="c9 c4">&#60418;Add mark_sharding to guide the compiler.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">query</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">mark_sharding(query,</span><span class="c2">&nbsp;</span><span class="c0">P(&quot;dp&quot;,</span><span class="c2">&nbsp;</span><span class="c0">(&quot;axis&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;sp&quot;),</span><span class="c2">&nbsp;</span><span class="c0">None))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">key</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">mark_sharding(key,</span><span class="c2">&nbsp;</span><span class="c0">P(&quot;dp&quot;,</span><span class="c2">&nbsp;</span><span class="c0">(&quot;axis&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;sp&quot;),</span><span class="c2">&nbsp;</span><span class="c0">None))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">value</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">mark_sharding(value,</span><span class="c2">&nbsp;</span><span class="c0">P(&quot;dp&quot;,</span><span class="c2">&nbsp;</span><span class="c0">(&quot;axis&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;sp&quot;),</span><span class="c2">&nbsp;</span><span class="c0">None))</span></p><p class="c11"><span class="c9 c4">&#60418;After sweeping the block size of the splash attention kernel, we got 141s here.</span></p><p class="c11"><span class="c9 c4">The remaining obvious bottleneck is self attention.</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 419.00px; height: 324.00px;"><img alt="" src="images/image3.png" style="width: 419.00px; height: 324.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span class="c9 c4">To further optimize the splash attention kernel, simplify and extract them from the library for easier modification.</span></p><p class="c3"><span>With env variables </span><span class="c14">--xla_enable_transpose_trace</span><span class="c9 c4">&nbsp;in LIBTPU_INIT_ARGS, we could see the named_scope in the pallas kernel. Be aware that named_scope would affect the performance, remember to delete them after profiling.</span></p><p class="c3"><span>&#60419;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">os</span></p><p class="c3"><span class="c0">os.environ[&quot;LIBTPU_INIT_ARGS&quot;]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">&quot;--xla_enable_transpose_trace&quot;</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">functools</span></p><p class="c3"><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">jax.experimental.shard_map</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">shard_map</span></p><p class="c3"><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">jax.sharding</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">NamedSharding,</span><span class="c2">&nbsp;</span><span class="c0">PartitionSpec</span><span class="c2">&nbsp;</span><span class="c0">as</span><span class="c2">&nbsp;</span><span class="c0">P</span></p><p class="c3"><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">jax</span></p><p class="c3"><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">math</span></p><p class="c3"><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">jax.numpy</span><span class="c2">&nbsp;</span><span class="c0">as</span><span class="c2">&nbsp;</span><span class="c0">jnp</span></p><p class="c3"><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">numpy</span><span class="c2">&nbsp;</span><span class="c0">as</span><span class="c2">&nbsp;</span><span class="c0">np</span></p><p class="c3"><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">dataclasses</span></p><p class="c3"><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">enum</span></p><p class="c3"><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">typing</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">Any</span></p><p class="c3"><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">jax.experimental</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">pallas</span><span class="c2">&nbsp;</span><span class="c0">as</span><span class="c2">&nbsp;</span><span class="c0">pl</span></p><p class="c3"><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">jax.experimental.pallas</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">tpu</span><span class="c2">&nbsp;</span><span class="c0">as</span><span class="c2">&nbsp;</span><span class="c0">pltpu</span></p><p class="c3"><span class="c0">from</span><span class="c2">&nbsp;</span><span class="c0">jax</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">lax</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">partial</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">functools.partial</span></p><p class="c3"><span class="c0">DEFAULT_MASK_VALUE</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">-0.7</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">float(np.finfo(np.dtype(&quot;float32&quot;)).max)</span></p><p class="c3"><span class="c0">NUM_LANES</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">128</span></p><p class="c3"><span class="c0">NN_DIM_NUMBERS</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(((1,),</span><span class="c2">&nbsp;</span><span class="c0">(0,)),</span><span class="c2">&nbsp;</span><span class="c0">((),</span><span class="c2">&nbsp;</span><span class="c0">()))</span></p><p class="c3"><span class="c0">NT_DIM_NUMBERS</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(((1,),</span><span class="c2">&nbsp;</span><span class="c0">(1,)),</span><span class="c2">&nbsp;</span><span class="c0">((),</span><span class="c2">&nbsp;</span><span class="c0">()))</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">class</span><span class="c2">&nbsp;</span><span class="c0">_QKVLayout(enum.IntEnum):</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">HEAD_DIM_MINOR</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">enum.auto()</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">SEQ_MINOR</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">enum.auto()</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">_from_head_minor(vals:</span><span class="c2">&nbsp;</span><span class="c0">tuple[Any,</span><span class="c2">&nbsp;</span><span class="c0">...],</span><span class="c2">&nbsp;</span><span class="c0">layout:</span><span class="c2">&nbsp;</span><span class="c0">_QKVLayout):</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">layout</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">_QKVLayout.HEAD_DIM_MINOR:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">vals</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">(*vals[:-2],</span><span class="c2">&nbsp;</span><span class="c0">vals[-1],</span><span class="c2">&nbsp;</span><span class="c0">vals[-2])</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">@dataclasses.dataclass(frozen=True,</span><span class="c2">&nbsp;</span><span class="c0">slots=True)</span></p><p class="c3"><span class="c0">class</span><span class="c2">&nbsp;</span><span class="c0">_BlockSizes:</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">block_q:</span><span class="c2">&nbsp;</span><span class="c0">int</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">block_kv:</span><span class="c2">&nbsp;</span><span class="c0">int</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">block_kv_compute:</span><span class="c2">&nbsp;</span><span class="c0">int</span><span class="c2">&nbsp;</span><span class="c0">|</span><span class="c2">&nbsp;</span><span class="c0">None</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">None</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">q_layout:</span><span class="c2">&nbsp;</span><span class="c0">_QKVLayout</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">_QKVLayout.HEAD_DIM_MINOR</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">k_layout:</span><span class="c2">&nbsp;</span><span class="c0">_QKVLayout</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">_QKVLayout.HEAD_DIM_MINOR</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">v_layout:</span><span class="c2">&nbsp;</span><span class="c0">_QKVLayout</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">_QKVLayout.HEAD_DIM_MINOR</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">__post_init__(self):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">self.block_kv_compute</span><span class="c2">&nbsp;</span><span class="c0">is</span><span class="c2">&nbsp;</span><span class="c0">None:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">object.__setattr__(self,</span><span class="c2">&nbsp;</span><span class="c0">&quot;block_kv_compute&quot;,</span><span class="c2">&nbsp;</span><span class="c0">self.block_kv)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">_flash_attention_kernel(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">q_ref,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">k_ref,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">v_ref,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">m_scratch_ref,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">l_scratch_ref,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">o_scratch_ref,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">o_ref,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">*,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">mask_value:</span><span class="c2">&nbsp;</span><span class="c0">float,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">grid_width:</span><span class="c2">&nbsp;</span><span class="c0">int,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">bq:</span><span class="c2">&nbsp;</span><span class="c0">int,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">bkv:</span><span class="c2">&nbsp;</span><span class="c0">int,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">bkv_compute:</span><span class="c2">&nbsp;</span><span class="c0">int,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">head_dim_v:</span><span class="c2">&nbsp;</span><span class="c0">int,</span></p><p class="c3"><span class="c0">):</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">float32</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.float32</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">head_dim_v_repeats,</span><span class="c2">&nbsp;</span><span class="c0">rem</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">divmod(head_dim_v,</span><span class="c2">&nbsp;</span><span class="c0">NUM_LANES)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">rem</span><span class="c2">&nbsp;</span><span class="c0">!=</span><span class="c2">&nbsp;</span><span class="c0">0:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">raise</span><span class="c2">&nbsp;</span><span class="c0">NotImplementedError(f&quot;{head_dim_v=}</span><span class="c2">&nbsp;</span><span class="c0">should</span><span class="c2">&nbsp;</span><span class="c0">be</span><span class="c2">&nbsp;</span><span class="c0">a</span><span class="c2">&nbsp;</span><span class="c0">multiple</span><span class="c2">&nbsp;</span><span class="c0">of</span><span class="c2">&nbsp;</span><span class="c0">{NUM_LANES}&quot;)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">j</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pl.program_id(0),</span><span class="c2">&nbsp;</span><span class="c0">pl.program_id(1),</span><span class="c2">&nbsp;</span><span class="c0">pl.program_id(2)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">@pl.when(j</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">0)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">init():</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">o_scratch_ref[...]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.zeros_like(o_scratch_ref)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">m_scratch_ref[...]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.full_like(m_scratch_ref,</span><span class="c2">&nbsp;</span><span class="c0">mask_value)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">l_scratch_ref[...]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.zeros_like(l_scratch_ref)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">body(kv_compute_index,</span><span class="c2">&nbsp;</span><span class="c0">_):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">jax.named_scope(&quot;qk&quot;):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">slice_k</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pl.ds(kv_compute_index</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">bkv_compute,</span><span class="c2">&nbsp;</span><span class="c0">bkv_compute)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_prev,</span><span class="c2">&nbsp;</span><span class="c0">l_prev</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">m_scratch_ref[...],</span><span class="c2">&nbsp;</span><span class="c0">l_scratch_ref[...]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">q</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">q_ref[...]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">k</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">k_ref[slice_k,</span><span class="c2">&nbsp;</span><span class="c0">:]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">qk</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">lax.dot_general(q,</span><span class="c2">&nbsp;</span><span class="c0">k,</span><span class="c2">&nbsp;</span><span class="c0">NT_DIM_NUMBERS,</span><span class="c2">&nbsp;</span><span class="c0">preferred_element_type=float32)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">jax.named_scope(&quot;softmax&quot;):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">qk.max(axis=-1)[:,</span><span class="c2">&nbsp;</span><span class="c0">None]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_next</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.maximum(m_prev,</span><span class="c2">&nbsp;</span><span class="c0">m_curr)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">bkv_repeats,</span><span class="c2">&nbsp;</span><span class="c0">rem</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">divmod(bkv_compute,</span><span class="c2">&nbsp;</span><span class="c0">NUM_LANES)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">rem</span><span class="c2">&nbsp;</span><span class="c0">!=</span><span class="c2">&nbsp;</span><span class="c0">0:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">raise</span><span class="c2">&nbsp;</span><span class="c0">NotImplementedError(f&quot;{bkv_compute=}</span><span class="c2">&nbsp;</span><span class="c0">should</span><span class="c2">&nbsp;</span><span class="c0">be</span><span class="c2">&nbsp;</span><span class="c0">a</span><span class="c2">&nbsp;</span><span class="c0">multiple</span><span class="c2">&nbsp;</span><span class="c0">of</span><span class="c2">&nbsp;</span><span class="c0">{NUM_LANES}&quot;)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">s_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.exp(qk</span><span class="c2">&nbsp;</span><span class="c0">-</span><span class="c2">&nbsp;</span><span class="c0">pltpu.repeat(m_next,</span><span class="c2">&nbsp;</span><span class="c0">bkv_repeats,</span><span class="c2">&nbsp;</span><span class="c0">axis=1))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">l_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.lax.broadcast_in_dim(s_curr.sum(axis=-1),</span><span class="c2">&nbsp;</span><span class="c0">l_prev.shape,</span><span class="c2">&nbsp;</span><span class="c0">(0,))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">alpha</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.exp(m_prev</span><span class="c2">&nbsp;</span><span class="c0">-</span><span class="c2">&nbsp;</span><span class="c0">m_next)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">l_next</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">l_curr</span><span class="c2">&nbsp;</span><span class="c0">+</span><span class="c2">&nbsp;</span><span class="c0">alpha</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">l_prev</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">jax.named_scope(&quot;qkv&quot;):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_scratch_ref[...],</span><span class="c2">&nbsp;</span><span class="c0">l_scratch_ref[...]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">m_next,</span><span class="c2">&nbsp;</span><span class="c0">l_next</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">v</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">v_ref[slice_k,</span><span class="c2">&nbsp;</span><span class="c0">:].astype(float32)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">o_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">lax.dot_general(s_curr,</span><span class="c2">&nbsp;</span><span class="c0">v,</span><span class="c2">&nbsp;</span><span class="c0">NN_DIM_NUMBERS)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">alpha_o</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pltpu.repeat(alpha,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_v_repeats,</span><span class="c2">&nbsp;</span><span class="c0">axis=1)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">o_scratch_ref[:]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">alpha_o</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">o_scratch_ref[:]</span><span class="c2">&nbsp;</span><span class="c0">+</span><span class="c2">&nbsp;</span><span class="c0">o_curr</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">lax.fori_loop(0,</span><span class="c2">&nbsp;</span><span class="c0">bkv</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">bkv_compute,</span><span class="c2">&nbsp;</span><span class="c0">body,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">unroll=True)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">@pl.when(j</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">grid_width</span><span class="c2">&nbsp;</span><span class="c0">-</span><span class="c2">&nbsp;</span><span class="c0">1)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">end():</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">l</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">l_scratch_ref[...]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">l_inv</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pltpu.repeat(1.0</span><span class="c2">&nbsp;</span><span class="c0">/</span><span class="c2">&nbsp;</span><span class="c0">l,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_v_repeats,</span><span class="c2">&nbsp;</span><span class="c0">axis=1)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">o_ref[...]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(o_scratch_ref[...]</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">l_inv).astype(o_ref.dtype)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">__splash_attention_forward(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">q:</span><span class="c2">&nbsp;</span><span class="c0">jax.Array,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">k:</span><span class="c2">&nbsp;</span><span class="c0">jax.Array,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">v:</span><span class="c2">&nbsp;</span><span class="c0">jax.Array,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">block_sizes:</span><span class="c2">&nbsp;</span><span class="c0">_BlockSizes,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">interpret:</span><span class="c2">&nbsp;</span><span class="c0">bool</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">False,</span></p><p class="c3"><span class="c0">):</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">num_q_heads,</span><span class="c2">&nbsp;</span><span class="c0">q_seq_len,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_qk</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">q.shape</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">head_dim_v</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">v.shape[-1]</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">bq,</span><span class="c2">&nbsp;</span><span class="c0">bkv</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">block_sizes.block_q,</span><span class="c2">&nbsp;</span><span class="c0">block_sizes.block_kv</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">bkv_compute</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">block_sizes.block_kv_compute</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">num_kv_heads</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">k.shape[0]</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">kv_seq_len</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">k.shape[1]</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">q_heads_per_kv_head</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">num_q_heads</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">num_kv_heads</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">q_index_map(h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">*_):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">(h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">0)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">out_index_map(h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">*_):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">0</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">k_index_map(h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">*_):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">(h</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">q_heads_per_kv_head,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">0)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">v_index_map(h,</span><span class="c2">&nbsp;</span><span class="c0">i,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">*_):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">(h</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">q_heads_per_kv_head,</span><span class="c2">&nbsp;</span><span class="c0">j,</span><span class="c2">&nbsp;</span><span class="c0">0)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">in_specs</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">[</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((None,</span><span class="c2">&nbsp;</span><span class="c0">bq,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_qk),</span><span class="c2">&nbsp;</span><span class="c0">q_index_map),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((None,</span><span class="c2">&nbsp;</span><span class="c0">bkv,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_qk),</span><span class="c2">&nbsp;</span><span class="c0">k_index_map),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((None,</span><span class="c2">&nbsp;</span><span class="c0">bkv,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_v),</span><span class="c2">&nbsp;</span><span class="c0">v_index_map),</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">]</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">out_shapes</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">[</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">jax.ShapeDtypeStruct((bq,</span><span class="c2">&nbsp;</span><span class="c0">NUM_LANES),</span><span class="c2">&nbsp;</span><span class="c0">jnp.float32),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">jax.ShapeDtypeStruct((bq,</span><span class="c2">&nbsp;</span><span class="c0">NUM_LANES),</span><span class="c2">&nbsp;</span><span class="c0">jnp.float32),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">jax.ShapeDtypeStruct((bq,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_v),</span><span class="c2">&nbsp;</span><span class="c0">jnp.float32),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">jax.ShapeDtypeStruct((num_q_heads,</span><span class="c2">&nbsp;</span><span class="c0">q_seq_len,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_v),</span><span class="c2">&nbsp;</span><span class="c0">q.dtype),</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">]</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">out_specs</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">[</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((bq,</span><span class="c2">&nbsp;</span><span class="c0">NUM_LANES),</span><span class="c2">&nbsp;</span><span class="c0">lambda</span><span class="c2">&nbsp;</span><span class="c0">*_:</span><span class="c2">&nbsp;</span><span class="c0">(0,</span><span class="c2">&nbsp;</span><span class="c0">0)),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((bq,</span><span class="c2">&nbsp;</span><span class="c0">NUM_LANES),</span><span class="c2">&nbsp;</span><span class="c0">lambda</span><span class="c2">&nbsp;</span><span class="c0">*_:</span><span class="c2">&nbsp;</span><span class="c0">(0,</span><span class="c2">&nbsp;</span><span class="c0">0)),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((bq,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_v),</span><span class="c2">&nbsp;</span><span class="c0">lambda</span><span class="c2">&nbsp;</span><span class="c0">*_:</span><span class="c2">&nbsp;</span><span class="c0">(0,</span><span class="c2">&nbsp;</span><span class="c0">0)),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">pl.BlockSpec((None,</span><span class="c2">&nbsp;</span><span class="c0">bq,</span><span class="c2">&nbsp;</span><span class="c0">head_dim_v),</span><span class="c2">&nbsp;</span><span class="c0">out_index_map),</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">]</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">grid_width</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">kv_seq_len</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">bkv</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">grid</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(num_q_heads,</span><span class="c2">&nbsp;</span><span class="c0">q_seq_len</span><span class="c2">&nbsp;</span><span class="c0">//</span><span class="c2">&nbsp;</span><span class="c0">bq,</span><span class="c2">&nbsp;</span><span class="c0">grid_width)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">all_out</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pl.pallas_call(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">partial(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">_flash_attention_kernel,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">mask_value=DEFAULT_MASK_VALUE,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">grid_width=grid_width,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">bq=bq,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">bkv=bkv,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">bkv_compute=bkv_compute,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">head_dim_v=head_dim_v,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">grid_spec=pltpu.PrefetchScalarGridSpec(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">num_scalar_prefetch=0,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">in_specs=in_specs,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">out_specs=out_specs,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">grid=grid,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">compiler_params=pltpu.CompilerParams(dimension_semantics=(&quot;parallel&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;arbitrary&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;arbitrary&quot;)),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">out_shape=out_shapes,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">interpret=interpret,</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">)(q,</span><span class="c2">&nbsp;</span><span class="c0">k,</span><span class="c2">&nbsp;</span><span class="c0">v)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">all_out[-1]</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">_make_splash_mha(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">block_sizes:</span><span class="c2">&nbsp;</span><span class="c0">_BlockSizes,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">interpret:</span><span class="c2">&nbsp;</span><span class="c0">bool</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">False,</span></p><p class="c3"><span class="c0">):</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">_splash_attention(q:</span><span class="c2">&nbsp;</span><span class="c0">jax.Array,</span><span class="c2">&nbsp;</span><span class="c0">k:</span><span class="c2">&nbsp;</span><span class="c0">jax.Array,</span><span class="c2">&nbsp;</span><span class="c0">v:</span><span class="c2">&nbsp;</span><span class="c0">jax.Array):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">__splash_attention_forward(q,</span><span class="c2">&nbsp;</span><span class="c0">k,</span><span class="c2">&nbsp;</span><span class="c0">v,</span><span class="c2">&nbsp;</span><span class="c0">block_sizes,</span><span class="c2">&nbsp;</span><span class="c0">interpret)</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">_splash_attention</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">BQSIZE</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">BKVSIZE</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">BKVCOMPUTESIZE</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">1024</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">sharded_fn</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">None</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">_tpu_splash_attention(query,</span><span class="c2">&nbsp;</span><span class="c0">key,</span><span class="c2">&nbsp;</span><span class="c0">value,</span><span class="c2">&nbsp;</span><span class="c0">mesh):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">global</span><span class="c2">&nbsp;</span><span class="c0">sharded_fn</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">num_heads</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">query.shape[1]</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">_attention_on_slices(q,</span><span class="c2">&nbsp;</span><span class="c0">k,</span><span class="c2">&nbsp;</span><span class="c0">v):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">scale_factor</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">1.0</span><span class="c2">&nbsp;</span><span class="c0">/</span><span class="c2">&nbsp;</span><span class="c0">math.sqrt(q.shape[-1])</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">q</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">q</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">scale_factor</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">pad_to_multiple(x,</span><span class="c2">&nbsp;</span><span class="c0">multiple,</span><span class="c2">&nbsp;</span><span class="c0">axis):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">seq_len</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">x.shape[axis]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">pad_len</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(multiple</span><span class="c2">&nbsp;</span><span class="c0">-</span><span class="c2">&nbsp;</span><span class="c0">seq_len</span><span class="c2">&nbsp;</span><span class="c0">%</span><span class="c2">&nbsp;</span><span class="c0">multiple)</span><span class="c2">&nbsp;</span><span class="c0">%</span><span class="c2">&nbsp;</span><span class="c0">multiple</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">pad_len</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">0:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">x,</span><span class="c2">&nbsp;</span><span class="c0">seq_len</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">pad_width</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">[(0,</span><span class="c2">&nbsp;</span><span class="c0">0)]</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">x.ndim</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">pad_width[axis]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(0,</span><span class="c2">&nbsp;</span><span class="c0">pad_len)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">jnp.pad(x,</span><span class="c2">&nbsp;</span><span class="c0">pad_width),</span><span class="c2">&nbsp;</span><span class="c0">seq_len</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">def</span><span class="c2">&nbsp;</span><span class="c0">kernel_3d(q_3d,</span><span class="c2">&nbsp;</span><span class="c0">k_3d,</span><span class="c2">&nbsp;</span><span class="c0">v_3d):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">q_3d_padded,</span><span class="c2">&nbsp;</span><span class="c0">q_orig_len</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pad_to_multiple(q_3d,</span><span class="c2">&nbsp;</span><span class="c0">BQSIZE,</span><span class="c2">&nbsp;</span><span class="c0">axis=1)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">k_3d_padded,</span><span class="c2">&nbsp;</span><span class="c0">k_orig_len</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pad_to_multiple(k_3d,</span><span class="c2">&nbsp;</span><span class="c0">BKVSIZE,</span><span class="c2">&nbsp;</span><span class="c0">axis=1)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">v_3d_padded,</span><span class="c2">&nbsp;</span><span class="c0">v_orig_len</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">pad_to_multiple(v_3d,</span><span class="c2">&nbsp;</span><span class="c0">BKVSIZE,</span><span class="c2">&nbsp;</span><span class="c0">axis=1)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">padded_q_seq_len</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">q_3d_padded.shape[1]</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">padded_kv_seq_len</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">k_3d_padded.shape[1]</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">block_sizes</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">_BlockSizes(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">block_q=min(BQSIZE,</span><span class="c2">&nbsp;</span><span class="c0">padded_q_seq_len),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">block_kv=min(BKVSIZE,</span><span class="c2">&nbsp;</span><span class="c0">padded_kv_seq_len),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">block_kv_compute=min(BKVCOMPUTESIZE,</span><span class="c2">&nbsp;</span><span class="c0">padded_kv_seq_len),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">splash_kernel</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">_make_splash_mha(block_sizes=block_sizes)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">out</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">splash_kernel(q_3d_padded,</span><span class="c2">&nbsp;</span><span class="c0">k_3d_padded,</span><span class="c2">&nbsp;</span><span class="c0">v_3d_padded)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">out[:,</span><span class="c2">&nbsp;</span><span class="c0">:q_orig_len,</span><span class="c2">&nbsp;</span><span class="c0">...]</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">vmapped_kernel</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.vmap(kernel_3d,</span><span class="c2">&nbsp;</span><span class="c0">in_axes=(0,</span><span class="c2">&nbsp;</span><span class="c0">0,</span><span class="c2">&nbsp;</span><span class="c0">0),</span><span class="c2">&nbsp;</span><span class="c0">out_axes=0)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">vmapped_kernel(q,</span><span class="c2">&nbsp;</span><span class="c0">k,</span><span class="c2">&nbsp;</span><span class="c0">v)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">if</span><span class="c2">&nbsp;</span><span class="c0">sharded_fn</span><span class="c2">&nbsp;</span><span class="c0">is</span><span class="c2">&nbsp;</span><span class="c0">None:</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">q_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">kv_partition_spec</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">None)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">sharded_fn</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.jit(shard_map(</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">_attention_on_slices,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">mesh=mesh,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">in_specs=(q_partition_spec,</span><span class="c2">&nbsp;</span><span class="c0">kv_partition_spec,</span><span class="c2">&nbsp;</span><span class="c0">kv_partition_spec),</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">out_specs=q_partition_spec,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">check_rep=False,</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">))</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">out</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">sharded_fn(query,</span><span class="c2">&nbsp;</span><span class="c0">key,</span><span class="c2">&nbsp;</span><span class="c0">value)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">return</span><span class="c2">&nbsp;</span><span class="c0">jax.lax.with_sharding_constraint(out,</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None))</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">shape</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(1,</span><span class="c2">&nbsp;</span><span class="c0">40,</span><span class="c2">&nbsp;</span><span class="c0">75600,</span><span class="c2">&nbsp;</span><span class="c0">128)</span></p><p class="c3"><span class="c0">q</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.arange(np.prod(shape),</span><span class="c2">&nbsp;</span><span class="c0">dtype=jnp.bfloat16).reshape(*shape)</span></p><p class="c3"><span class="c0">k</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.arange(np.prod(shape),</span><span class="c2">&nbsp;</span><span class="c0">dtype=jnp.bfloat16).reshape(*shape)</span></p><p class="c3"><span class="c0">v</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jnp.arange(np.prod(shape),</span><span class="c2">&nbsp;</span><span class="c0">dtype=jnp.bfloat16).reshape(*shape)</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">mesh</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.make_mesh((len(jax.devices()),</span><span class="c2">&nbsp;</span><span class="c0">1,</span><span class="c2">&nbsp;</span><span class="c0">1),</span><span class="c2">&nbsp;</span><span class="c0">(&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;sp&#39;))</span></p><p class="c3"><span class="c0">q</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.device_put(q,</span><span class="c2">&nbsp;</span><span class="c0">NamedSharding(mesh,</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">(&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">None)))</span></p><p class="c3"><span class="c0">k</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.device_put(k,</span><span class="c2">&nbsp;</span><span class="c0">NamedSharding(mesh,</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">(&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">None)))</span></p><p class="c3"><span class="c0">v</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">jax.device_put(v,</span><span class="c2">&nbsp;</span><span class="c0">NamedSharding(mesh,</span><span class="c2">&nbsp;</span><span class="c0">P(&#39;dp&#39;,</span><span class="c2">&nbsp;</span><span class="c0">None,</span><span class="c2">&nbsp;</span><span class="c0">(&#39;axis&#39;,</span><span class="c2">&nbsp;</span><span class="c0">&#39;sp&#39;),</span><span class="c2">&nbsp;</span><span class="c0">None)))</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">mesh:</span></p><p class="c3"><span class="c2">&nbsp; </span><span class="c0">output2</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">_tpu_splash_attention(q,k,v,mesh)</span></p><p class="c3"><span class="c0">output2.block_until_ready()</span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3 c8"><span class="c1"></span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">time</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp;</span><span class="c0">with</span><span class="c2">&nbsp;</span><span class="c0">mesh:</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp; &nbsp; &nbsp;</span><span class="c0">num_time</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">50</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp; &nbsp; &nbsp;</span><span class="c0">start_time</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">time.time()</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp; &nbsp; &nbsp;</span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">_</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">range(num_time):</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c0">output</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">_tpu_splash_attention(q,k,v,mesh)</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp; &nbsp; &nbsp;</span><span class="c0">output.block_until_ready()</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp; &nbsp; &nbsp;</span><span class="c0">end_time</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">time.time()</span></p><p class="c3"><span class="c0">#</span><span class="c2">&nbsp; &nbsp; &nbsp;</span><span class="c0">print(f&quot;{(end_time-start_time)/num_time}&quot;)</span></p><p class="c11"><span class="c9 c4">&#60418;We could run the pallas kernel only for further experiments, which would be faster.</span></p><p class="c11"><span class="c9 c4">Found that the softmax takes non-negligible time. Beware that named_scope would prevent optimization across different scope, try different name_scope boundaries.</span></p><p class="c11"><span class="c9 c4">Also try to replace some ops with constant to find the bottleneck.</span></p><p class="c3"><span>&#60419;</span><span class="c0 c4"># Try differnt ops and see the end to end time</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c0">m_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">qk.max(axis=-1)[:,</span><span class="c2">&nbsp;</span><span class="c0 c4">None]</span></p><p class="c3"><span class="c0"># m_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0 c4">qk.max(axis=0)[None, :]</span></p><p class="c3"><span class="c0"># m_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0 c4">jnp.zeros((bq, bkv_compute))</span></p><p class="c11"><span class="c9 c4">&#60418;Found that max could be slow. Change the dimension order to KQ to reduce along axis=0 and additionally reduce some memory of internal states due to padding.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; </span><span class="c0"># It&#39;s free to transpose in dot_general</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">qk</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">lax.dot_general(k,</span><span class="c2">&nbsp;</span><span class="c0">q,</span><span class="c2">&nbsp;</span><span class="c0">NT_DIM_NUMBERS,</span><span class="c2">&nbsp;</span><span class="c0">preferred_element_type=float32)</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">assert</span><span class="c2">&nbsp;</span><span class="c0">qk.shape</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">(bkv_compute,</span><span class="c2">&nbsp;</span><span class="c0 c4">bq)</span></p><p class="c3"><span class="c0 c4">...</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; m_curr</span><span class="c2 c23">&nbsp;</span><span class="c0">=</span><span class="c2 c23">&nbsp;</span><span class="c0">qk.max(axis=0)[None,</span><span class="c2 c23">&nbsp;</span><span class="c0">:]</span></p><p class="c3 c8"><span class="c0 c4"></span></p><p class="c3"><span class="c9 c4">&#60418;Max should not be that slow, try using a slice of the qk.</span></p><p class="c3"><span>&#60419;</span><span class="c0">m_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">qk.max(axis=-1)[:,</span><span class="c2">&nbsp;</span><span class="c0 c4">None]</span></p><p class="c3"><span class="c0 c4"># m_curr = qk[-1:, :]</span></p><p class="c3"><span class="c0 c4"># m_curr = qk[0:1, :]</span></p><p class="c11"><span class="c9 c4">&#60418;Found that the max would be bound to the qk results. It costs similar time with -1 slice.</span></p><p class="c3"><span class="c9 c4">Add an additional further inner loop in kv_compute, as bkv_compute_in. Try to do the remaining computation as soon as possible.</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; </span><span class="c0">step</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">bkv_compute_in</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">assert</span><span class="c2">&nbsp;</span><span class="c0">qk.shape[0]</span><span class="c2">&nbsp;</span><span class="c0">%</span><span class="c2">&nbsp;</span><span class="c0">step</span><span class="c2">&nbsp;</span><span class="c0">==</span><span class="c2">&nbsp;</span><span class="c0">0</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; </span><span class="c0">for</span><span class="c2">&nbsp;</span><span class="c0">i</span><span class="c2">&nbsp;</span><span class="c0">in</span><span class="c2">&nbsp;</span><span class="c0">range(0,</span><span class="c2">&nbsp;</span><span class="c0">qk.shape[0],</span><span class="c2">&nbsp;</span><span class="c0">step):</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; </span><span class="c0">m_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">qk[i:i+step].max(axis=0)[None,</span><span class="c2">&nbsp;</span><span class="c0 c4">:]</span></p><p class="c11"><span class="c9 c4">&#60418;We faced bad overlapping between VPU and MXU due to the max depending on the qk results.</span></p><p class="c11"><span class="c9 c4">Found a magic flag using a different strategy for scheduler overlapping the VPU and MXU.</span></p><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span><span class="c0">compiler_params=pltpu.CompilerParams(dimension_semantics=(&quot;parallel&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;arbitrary&quot;,</span><span class="c2">&nbsp;</span><span class="c0">&quot;arbitrary&quot;),</span><span class="c2">&nbsp;</span><span class="c0">flags={&quot;XLA_TPU_FORCE_LP_LLO_SCHEDULER&quot;:</span><span class="c2">&nbsp;</span><span class="c0">True}),</span></p><p class="c11"><span class="c9 c4">&#60418;Beware that the flag is not XLA_FLAGS nor LIBTPU_INIT_ARGS. It&rsquo;s for pallas kernel.</span></p><p class="c11"><span class="c9 c4">Dump the LLO for further optimization.</span></p><p class="c3"><span>&#60419;</span><span class="c0">import</span><span class="c2">&nbsp;</span><span class="c0">os</span></p><p class="c3"><span class="c0">os.environ[&quot;LIBTPU_INIT_ARGS&quot;]</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">&quot;--xla_jf_dump_to=llo/&quot;</span></p><p class="c11"><span class="c9 c4">&#60418;post-DCE-2.txt - after initial DCE/CSE passes, but before any scheduling<br>DGO-vliw-packed-bundles.txt - just after scheduling, but before register allocation<br>packed-bundles-post-ra.txt - essentially the final bundles</span></p><p class="c11"><span class="c9 c4">See the shmap_body.2 for the kernel. Found that for jnp.exp, it always be vmul constant than vpow.pop. Overlap the vmul with previous q / sqrt(dim). Beware that jnp.exp2 is slow, use jnp.power.</span></p><p class="c3 c8"><span class="c9 c4"></span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">%v5969</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">vmul.f32</span><span class="c2">&nbsp;</span><span class="c0">1.442695,</span><span class="c2">&nbsp;</span><span class="c0">%v4945</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">%v5970</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">vpow.pop</span><span class="c2">&nbsp;</span><span class="c0">%v5969</span></p><p class="c3"><span class="c9 c4">&#60418;</span></p><p class="c3"><span>&#60419;</span><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c0">_LOG2_E</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">1.44269504</span></p><p class="c3"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">q</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">q</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0">scale_factor</span><span class="c2">&nbsp;</span><span class="c0">*</span><span class="c2">&nbsp;</span><span class="c0 c4">_LOG2_E</span></p><p class="c3"><span class="c9 c4">&#60418;</span></p><p class="c3"><span>&#60419;</span><span class="c0">s_curr</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">(exp2(qk[i:i+step]</span><span class="c2">&nbsp;</span><span class="c0">-</span><span class="c2">&nbsp;</span><span class="c0">m_next[0:1]))</span></p><p class="c11"><span class="c9 c4">&#60418;</span></p><p class="c11"><span class="c9 c4">Also fix the device mesh order. The later the better for communication.</span></p><p class="c3"><span>&#60419;</span><span class="c0"># tp here is not really used for tp. It&#39;s for remaining all devices.</span></p><p class="c3"><span class="c0">mesh_devices</span><span class="c2">&nbsp;</span><span class="c0">=</span><span class="c2">&nbsp;</span><span class="c0">mesh_utils.create_device_mesh((dp_dim,</span><span class="c2">&nbsp;</span><span class="c0">sp_dim,</span><span class="c2">&nbsp;</span><span class="c0">tp_dim),</span><span class="c2">&nbsp;</span><span class="c0">allow_split_physical_axes=True)</span></p><p class="c11"><span class="c9 c4">&#60418;Found there are some all_gather weights after cross_attention, try replicating the weights of a few layers after cross_attention.</span></p><p class="c11"><span class="c9 c4">We got 124.9s here.</span></p><p class="c11 c8"><span class="c9 c4"></span></p><h2 class="c5" id="h.yvt8n283df9p"><span class="c7 c4">Reference</span></h2><ol class="lst-kix_4jhw1c1a6bfe-0 start" start="1"><li class="c11 c17 li-bullet-0"><span class="c19"><a href="https://www.google.com/url?q=https://docs.google.com/document/d/1cJw93j8gQ8r6pouA-gS4ynpbjNlkqggJ4Zs76G3Daqs/edit?resourcekey%3D0-rnX-srj_Hqsg_KsHqqYrqg%26tab%3Dt.0%23heading%3Dh.d7id8slf0vtu&amp;sa=D&amp;source=editors&amp;ust=1765684106538156&amp;usg=AOvVaw1Hw6rstLCSYupW3a0etktp">Wan2.1 Optimization Report</a></span></li><li class="c11 c17 li-bullet-0"><span class="c19"><a href="https://www.google.com/url?q=https://docs.google.com/presentation/d/1ZOjyEgtLpKHz1Lc0svp2KQKcNcjukSO4FYlmN_HqeMU/edit?slide%3Did.p%23slide%3Did.p&amp;sa=D&amp;source=editors&amp;ust=1765684106538418&amp;usg=AOvVaw3LAkNY0E9Gl11ZYz95PV95">Wan2.1 T2V Optimization Experience Share</a></span></li><li class="c11 c17 li-bullet-0"><span class="c19"><a href="https://www.google.com/url?q=https://docs.google.com/document/d/11kY_8-LclOQvibzTxPqIaBqlQCAqLYcLNk1cWP7NHP0/edit?tab%3Dt.0&amp;sa=D&amp;source=editors&amp;ust=1765684106538618&amp;usg=AOvVaw1qC_L5E8dI6K9eslQZG1fg">[External] Wan2.1-14B Model FLOPs Utilization Analysis on TPU v6e-16</a></span></li><li class="c11 c17 li-bullet-0"><span class="c19"><a href="https://www.google.com/url?q=https://docs.google.com/document/d/1HyRXHelT6en_r3vauQHHWeBD20njQCFO4OlKAmU3sAM/edit?tab%3Dt.0%23heading%3Dh.alqmnqqklvy7&amp;sa=D&amp;source=editors&amp;ust=1765684106538821&amp;usg=AOvVaw1jzzzxBuE2rFg1EG9y0n0x">Kling Wan2.1 TPU v6e Optimization</a></span></li><li class="c11 c17 li-bullet-0"><span class="c19"><a href="https://www.google.com/url?q=https://docs.google.com/presentation/d/1tq55PhfbJR7opuTAb7dBHG3U6ItVBATwP9y6La0vnXU/edit?slide%3Did.p%23slide%3Did.p&amp;sa=D&amp;source=editors&amp;ust=1765684106539023&amp;usg=AOvVaw0gY0lx2McoAz4c9RQbGPsS">Understanding and Tuning Splash Attention Kernel</a></span></li><li class="c11 c17 li-bullet-0"><span class="c24"><a href="https://www.google.com/url?q=https://docs.jax.dev/en/latest/pallas/tpu/details.html%23elementwise-operations&amp;sa=D&amp;source=editors&amp;ust=1765684106539369&amp;usg=AOvVaw3sY3Sz6UxARKSoUD2fM3CB">https://docs.jax.dev/en/latest/pallas/tpu/details.html#elementwise-operations</a></span></li><li class="c11 c17 li-bullet-0"><span class="c19"><a href="https://www.google.com/url?q=https://b.corp.google.com/issues/425282001&amp;sa=D&amp;source=editors&amp;ust=1765684106539542&amp;usg=AOvVaw3S2MLvoCLuDaPu4n_U5Y4U">Wan2.1 optimization on tpu v6e</a></span></li><li class="c11 c17 li-bullet-0"><span class="c24"><a href="https://www.google.com/url?q=https://github.com/shungcp/diffusers/tree/df1e2157b6bbd67b98d8f26ecdcc4273f19cbd09&amp;sa=D&amp;source=editors&amp;ust=1765684106539879&amp;usg=AOvVaw32sNC_C1y7XnQUirPAnhLO">https://github.com/shungcp/diffusers/tree/df1e2157b6bbd67b98d8f26ecdcc4273f19cbd09</a></span></li><li class="c11 c17 li-bullet-0"><span class="c19"><a href="https://www.google.com/url?q=https://docs.google.com/presentation/d/1jYgOkoERRTo24ikfVVKb2S73urmcLd-qXg9srsKU24E/edit?slide%3Did.g278921a57a4_0_553%23slide%3Did.g278921a57a4_0_553&amp;sa=D&amp;source=editors&amp;ust=1765684106540120&amp;usg=AOvVaw26Vdt0vh702nu87mf3oWz5">LLO Performance</a></span></li><li class="c11 c17 li-bullet-0"><span class="c24"><a href="https://www.google.com/url?q=https://github.com/AI-Hypercomputer/maxdiffusion/tree/main&amp;sa=D&amp;source=editors&amp;ust=1765684106540427&amp;usg=AOvVaw2-LKJa_jzHehJWesXr9KDo">https://github.com/AI-Hypercomputer/maxdiffusion/tree/main</a></span><span class="c9 c4">&nbsp;</span></li><li class="c11 c17 li-bullet-0"><span class="c19"><a href="https://www.google.com/url?q=https://docs.google.com/document/d/1HyRXHelT6en_r3vauQHHWeBD20njQCFO4OlKAmU3sAM/edit?tab%3Dt.0%23heading%3Dh.alqmnqqklvy7&amp;sa=D&amp;source=editors&amp;ust=1765684106540652&amp;usg=AOvVaw2Y5SdK9jw2uLh_LCnGA_03">Kling Wan2.1 TPU v6e Optimization</a></span></li><li class="c11 c17 li-bullet-0"><span class="c19"><a href="https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1-EXIDGd0iDDUVExP1BAE21EOUPQ28XMON25i-qLdkxU/edit?resourcekey%3D0-0h2NZY7Pq1em3u39DDNluQ%26gid%3D1022669690%23gid%3D1022669690&amp;sa=D&amp;source=editors&amp;ust=1765684106540894&amp;usg=AOvVaw2V5fC-EVhxr9-0LkL50Qpy">Latency Reduction Through Optimization</a></span></li></ol><div class="c27"><p class="c16"><a href="#cmnt_ref1" id="cmnt1">[a]</a><span class="c15 c4">Hi @kennyzheng@google.com , could&nbsp;you please review the document? Thanks.</span></p><p class="c16"><span class="c4 c15">_Assigned to kennyzheng@google.com_</span></p></div><div class="c27"><p class="c16"><a href="#cmnt_ref2" id="cmnt2">[b]</a><span class="c15 c4">Suggest the TL;DR part to cover the final result on TPU v6e, performance comparison&nbsp;vs GPU, and the summary of the main optimization parameters and technology.</span></p></div><div class="c27"><p class="c16"><a href="#cmnt_ref3" id="cmnt3">[c]</a><span class="c15 c4">Thanks for the advice. Add an abstract paragraph including the method and latency results.</span></p></div></body></html>