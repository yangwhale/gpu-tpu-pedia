# GPU vs TPU ä»£ç å¯¹æ¯”åˆ†ææ–‡æ¡£

æœ¬æ–‡æ¡£å¯¹æ¯” `stage2_transformer.py` (GPU) å’Œ `stage2_transformer_flax.py` (TPU) çš„ä»£ç å·®å¼‚ï¼Œ
é‡ç‚¹åˆ†æå¯èƒ½å¯¼è‡´ TPU ç‰ˆæœ¬ç”Ÿæˆé”™è¯¯è§†é¢‘çš„æ•°å€¼é—®é¢˜ã€‚

## æ–‡ä»¶æ¦‚è§ˆ

| æ–‡ä»¶ | ç‰ˆæœ¬ | è¡Œæ•° | ä¸»è¦æŠ€æœ¯ |
|------|------|------|----------|
| `stage2_transformer.py` | GPU | 714 | PyTorch + Flash Attention |
| `stage2_transformer_flax.py` | TPU | 998 | torchax + Splash Attention |

---

## æ•´ä½“æµç¨‹å¯¹æ¯”

```mermaid
flowchart TB
    subgraph GPU["GPU ç‰ˆæœ¬ (stage2_transformer.py)"]
        G1[åˆå§‹åŒ–å¹¶è¡ŒçŠ¶æ€<br/>initialize_parallel_state] --> G2[åŠ è½½ Transformer<br/>from_pretrained]
        G2 --> G3[åŠ è½½ Scheduler<br/>FlowMatchDiscreteScheduler]
        G3 --> G4[å‡†å¤‡ Embeddings<br/>CFG åˆå¹¶]
        G4 --> G5[å‡†å¤‡ Latents<br/>éšæœºå™ªå£°]
        G5 --> G6[Denoising Loop<br/>50 steps]
        G6 --> G7[ä¿å­˜ Latents]
    end
    
    subgraph TPU["TPU ç‰ˆæœ¬ (stage2_transformer_flax.py)"]
        T1[Mock å¹¶è¡ŒçŠ¶æ€<br/>sp_enabled=False] --> T2[åŠ è½½ Transformer<br/>from_pretrained]
        T2 --> T3[æƒé‡åˆ†ç‰‡<br/>shard_weights_transformer]
        T3 --> T4[é¢„è®¡ç®— Rotary Embeddings]
        T4 --> T5[ç¼–è¯‘ Transformer<br/>torchax.compile]
        T5 --> T6[å‡†å¤‡ Embeddings<br/>CFG åˆå¹¶]
        T6 --> T7[å‡†å¤‡ Latents<br/>éšæœºå™ªå£°]
        T7 --> T8[Denoising Loop<br/>50 steps]
        T8 --> T9[ä¿å­˜ Latents]
    end
    
    style G6 fill:#ff9999
    style T8 fill:#99ff99
```

---

## ğŸš¨ å…³é”®å·®å¼‚åˆ†æ

### å·®å¼‚ 1ï¼šAttention å®ç° (âš ï¸ é«˜é£é™©)

è¿™æ˜¯æœ€å¯èƒ½å¯¼è‡´æ•°å€¼é”™è¯¯çš„å·®å¼‚ï¼

```mermaid
flowchart LR
    subgraph GPU["GPU: parallel_attention"]
        GA1[åˆå¹¶ Q/K/V] --> GA2[æ„é€  attention mask]
        GA2 --> GA3[åº”ç”¨ mask<br/>å±è”½ padding]
        GA3 --> GA4[Flash Attention]
        GA4 --> GA5[è¿”å›ç»“æœ]
    end
    
    subgraph TPU["TPU: _parallel_attention_tpu"]
        TA1[åˆå¹¶ Q/K/V] --> TA2[å¼ºåˆ¶ attn_mask=None]
        TA2 --> TA3[Splash Attention<br/>æ—  mask]
        TA3 --> TA4[è¿”å›ç»“æœ]
    end
    
    style GA3 fill:#99ff99
    style TA2 fill:#ff9999
```

#### GPU ç‰ˆæœ¬ ([`stage2_transformer.py`](stage2_transformer.py:645-660))

ä½¿ç”¨åŸå§‹çš„ `parallel_attention`ï¼Œæ”¯æŒ attention maskï¼š

```python
# è°ƒç”¨ transformer forwardï¼Œå†…éƒ¨ä½¿ç”¨ parallel_attention
output = transformer(
    latent_model_input,
    t_expand,
    prompt_embeds,
    ...
)
```

åŸå§‹ `parallel_attention` ä¼šï¼š
1. æ ¹æ® `text_mask` æ„é€  attention mask
2. å±è”½ padding tokenï¼Œé˜²æ­¢å…¶å‚ä¸æ³¨æ„åŠ›è®¡ç®—

#### TPU ç‰ˆæœ¬ ([`stage2_transformer_flax.py`](stage2_transformer_flax.py:82-132))

Monkey-patch ä¸ºç®€åŒ–ç‰ˆæœ¬ï¼š

```python
def _parallel_attention_tpu(q, k, v, img_q_len, img_kv_len,
                             attn_mode=None, text_mask=None,
                             attn_param=None, block_idx=None):
    """
    TPU å…¼å®¹ç‰ˆæœ¬çš„ parallel_attention
    - å¼ºåˆ¶ä½¿ç”¨ Splash Attentionï¼ˆä¸ä½¿ç”¨ maskï¼Œé¿å… OOMï¼‰
    - ç§»é™¤æ–­è¨€æ£€æŸ¥ï¼ˆé¿å… JIT concretization é—®é¢˜ï¼‰
    """
    # ... åˆå¹¶ Q/K/V ...
    
    # âš ï¸ å¼ºåˆ¶ä¸ä½¿ç”¨ maskï¼
    attn_mask = None
    
    # è°ƒç”¨ SDPAï¼ˆè¢« Splash Attention æ‹¦æˆªï¼‰
    hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attn_mask)
```

**é—®é¢˜åˆ†æï¼š**
- **text_mask è¢«å®Œå…¨å¿½ç•¥**
- Padding token ä¼šå‚ä¸æ³¨æ„åŠ›è®¡ç®—
- è¿™å¯èƒ½å¯¼è‡´æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒé”™è¯¯

---

### å·®å¼‚ 2ï¼šByT5 Embeddings æ•°æ®ç±»å‹ (âš ï¸ ä¸­é£é™©)

```mermaid
flowchart LR
    subgraph GPU["GPU"]
        G1[ByT5 Embeddings] --> G2[è½¬æ¢ä¸º float32]
        G2 --> G3[Transformer forward]
    end
    
    subgraph TPU["TPU"]
        T1[ByT5 Embeddings] --> T2[è½¬æ¢ä¸º bf16]
        T2 --> T3[Transformer forward]
    end
    
    style G2 fill:#99ff99
    style T2 fill:#ffff99
```

#### GPU ç‰ˆæœ¬ ([`stage2_transformer.py`](stage2_transformer.py:518-532))

```python
if prompt_embeds_2 is not None:
    prompt_embeds_2 = prompt_embeds_2.to(device=device, dtype=torch.float32)  # âœ… float32
    prompt_embeds_mask_2 = prompt_embeds_mask_2.to(device=device)
    if do_classifier_free_guidance:
        negative_prompt_embeds_2 = negative_prompt_embeds_2.to(device=device, dtype=torch.float32)
        ...
    extra_kwargs = {
        "byt5_text_states": byt5_text_states,  # float32
        "byt5_text_mask": byt5_text_mask,
    }
```

#### TPU ç‰ˆæœ¬ ([`stage2_transformer_flax.py`](stage2_transformer_flax.py:812-829))

```python
# TPU æ˜¯ bf16 å‹å¥½çš„èŠ¯ç‰‡ï¼Œæ‰€æœ‰ tensor éƒ½ä½¿ç”¨ bfloat16
extra_kwargs = {}
if prompt_embeds_2 is not None:
    prompt_embeds_2 = prompt_embeds_2.to(dtype=target_dtype).to('jax')  # âš ï¸ bf16
    prompt_embeds_mask_2 = prompt_embeds_mask_2.to('jax')
    if do_classifier_free_guidance:
        negative_prompt_embeds_2 = negative_prompt_embeds_2.to(dtype=target_dtype).to('jax')  # bf16
```

**é—®é¢˜åˆ†æï¼š**
- GPU ä½¿ç”¨ float32 ä¿æŒç²¾åº¦
- TPU ä½¿ç”¨ bf16 å¯èƒ½å¯¼è‡´ç²¾åº¦æŸå¤±
- ByT5 embeddings ç”¨äºæ–‡æœ¬æ¡ä»¶æ§åˆ¶ï¼Œç²¾åº¦æŸå¤±å¯èƒ½å½±å“ç”Ÿæˆè´¨é‡

---

### å·®å¼‚ 3ï¼švision_states å¤„ç† (âš ï¸ ä¸­é£é™©)

```mermaid
flowchart LR
    subgraph GPU["GPU: t2v æ¨¡å¼"]
        G1[åˆ›å»ºé›¶å‘é‡<br/>shape: [1, 729, 1152]] --> G2[CFG å¤åˆ¶<br/>shape: [2, 729, 1152]]
        G2 --> G3[ä¼ å…¥ Transformer]
    end
    
    subgraph TPU["TPU: t2v æ¨¡å¼"]
        T1[vision_states = None] --> T2[ä¼ å…¥ Transformer]
    end
    
    style G1 fill:#99ff99
    style T1 fill:#ffff99
```

#### GPU ç‰ˆæœ¬ ([`stage2_transformer.py`](stage2_transformer.py:559-574))

```python
# å‡†å¤‡ vision_statesï¼ˆt2v æ¨¡å¼ä½¿ç”¨é›¶å‘é‡ï¼‰
vision_num_tokens = 729
vision_dim = 1152

vision_states = torch.zeros(
    latents.shape[0],
    vision_num_tokens,
    vision_dim
).to(device=device, dtype=target_dtype)

if do_classifier_free_guidance:
    vision_states = vision_states.repeat(2, 1, 1)
```

#### TPU ç‰ˆæœ¬ ([`stage2_transformer_flax.py`](stage2_transformer_flax.py:858-881))

```python
# å‡†å¤‡ vision_states
# t2v æ¨¡å¼ï¼šè®¾ä¸º None ä»¥è·³è¿‡ vision_in å¤„ç†
# è¿™ä¹Ÿé¿å…äº† torch.all(vision_states == 0) åœ¨ JIT ä¸­çš„ concretization é—®é¢˜
if task_type == 't2v':
    vision_states = None  # âš ï¸ ä¸ GPU ä¸åŒ
else:
    # i2v æˆ–å…¶ä»–æ¨¡å¼éœ€è¦å®é™…çš„ vision_states
    vision_states = torch.zeros(...)
```

**é—®é¢˜åˆ†æï¼š**
- Transformer å†…éƒ¨å¯¹ `None` å’Œé›¶å‘é‡å¯èƒ½æœ‰ä¸åŒå¤„ç†é€»è¾‘
- è™½ç„¶ä»£ç æ³¨é‡Šè¯´è¿™æ ·å¯ä»¥"è·³è¿‡ vision_in å¤„ç†"ï¼Œä½†å¯èƒ½å¯¼è‡´è¡Œä¸ºä¸ä¸€è‡´
- éœ€è¦æ£€æŸ¥ `HunyuanVideo_1_5_DiffusionTransformer.forward()` å¯¹ `vision_states=None` çš„å¤„ç†

---

### å·®å¼‚ 4ï¼šreorder_txt_token ç®€åŒ– (âš ï¸ ä¸­é£é™©)

```mermaid
flowchart TB
    subgraph GPU["GPU: åŸå§‹ reorder_txt_token"]
        G1[è¾“å…¥ byt5_txt, txt,<br/>byt5_mask, text_mask] --> G2{is_reorder?}
        G2 -->|True| G3[å¤æ‚é‡æ’åºé€»è¾‘<br/>ä½¿ç”¨å¸ƒå°”ç´¢å¼•]
        G2 -->|False| G4[ç®€å•æ‹¼æ¥]
        G3 --> G5[è¿”å› reorder_txt, mask]
        G4 --> G5
    end
    
    subgraph TPU["TPU: ç®€åŒ–ç‰ˆæœ¬"]
        T1[è¾“å…¥ byt5_txt, txt,<br/>byt5_mask, text_mask] --> T2[å¿½ç•¥ is_reorder<br/>ç›´æ¥æ‹¼æ¥]
        T2 --> T3[è¿”å› reorder_txt, mask]
    end
    
    style G3 fill:#99ff99
    style T2 fill:#ffff99
```

#### GPU ç‰ˆæœ¬

ä½¿ç”¨åŸå§‹çš„ `reorder_txt_token` æ–¹æ³•ï¼Œæ”¯æŒ `is_reorder=True` çš„å¤æ‚é€»è¾‘ã€‚

#### TPU ç‰ˆæœ¬ ([`stage2_transformer_flax.py`](stage2_transformer_flax.py:67-79))

```python
def _reorder_txt_token_tpu_compatible(self, byt5_txt, txt, byt5_text_mask, text_mask, zero_feat=False, is_reorder=True):
    """
    TPU å…¼å®¹ç‰ˆæœ¬çš„ reorder_txt_tokenï¼Œç¦ç”¨ is_reorder ä»¥é¿å…å¸ƒå°”ç´¢å¼•æ“ä½œ
    åŸå§‹æ–¹æ³•ä½¿ç”¨ tensor[~mask] è¿™æ ·çš„å¸ƒå°”ç´¢å¼•ï¼Œtorchax ä¸æ”¯æŒ
    """
    # å¼ºåˆ¶ä½¿ç”¨ç®€åŒ–é€»è¾‘ï¼ˆä¸ä½¿ç”¨å¸ƒå°”ç´¢å¼•ï¼‰
    reorder_txt = torch.concat([byt5_txt, txt], dim=1)  # âš ï¸ ç®€åŒ–
    reorder_mask = torch.concat([byt5_text_mask, text_mask], dim=1).to(dtype=torch.int64)
    return reorder_txt, reorder_mask
```

**é—®é¢˜åˆ†æï¼š**
- åŸå§‹å®ç°å¯èƒ½æ ¹æ® mask å¯¹ token è¿›è¡Œé‡æ–°æ’åˆ—
- ç®€åŒ–ç‰ˆæœ¬å¿½ç•¥äº†è¿™ä¸ªé€»è¾‘ï¼Œå¯èƒ½å¯¼è‡´ token é¡ºåºä¸æ­£ç¡®
- è¿™ä¼šå½±å“æ³¨æ„åŠ›è®¡ç®—ä¸­ text token çš„ä½ç½®ä¿¡æ¯

---

### å·®å¼‚ 5ï¼šScheduler Step åçš„ dtype å¤„ç† (âš ï¸ ä½é£é™©)

```mermaid
flowchart LR
    subgraph GPU["GPU"]
        G1[scheduler.step] --> G2[è¿”å› latents<br/>dtype ä¸å˜]
    end
    
    subgraph TPU["TPU"]
        T1[scheduler.step] --> T2[å†…éƒ¨è½¬ float32]
        T2 --> T3[æ˜¾å¼è½¬å› bf16]
    end
    
    style G2 fill:#99ff99
    style T3 fill:#ffff99
```

#### GPU ç‰ˆæœ¬ ([`stage2_transformer.py`](stage2_transformer.py:667-668))

```python
# Scheduler step
latents = scheduler.step(noise_pred, t, latents, generator=generator, return_dict=False)[0]
# æ²¡æœ‰æ˜¾å¼ dtype è½¬æ¢ï¼Œä¿æŒåŸå§‹ dtype
```

#### TPU ç‰ˆæœ¬ ([`stage2_transformer_flax.py`](stage2_transformer_flax.py:949-953))

```python
# Scheduler step
# æ³¨æ„ï¼šscheduler.step å†…éƒ¨ä¼šè½¬æˆ float32 åšç´¯åŠ ï¼ˆdiffusers çš„æ ‡å‡†åšæ³•ï¼‰
# ä½†å¯¹äº TPUï¼Œbf16 åŸç”Ÿæ”¯æŒï¼Œéœ€è¦è½¬å› bf16
latents = scheduler.step(noise_pred, t, latents, generator=generator, return_dict=False)[0]
latents = latents.to(target_dtype)  # è½¬å› bf16
```

**é—®é¢˜åˆ†æï¼š**
- è¿™ä¸ªå·®å¼‚æœ¬èº«å¯èƒ½ä¸æ˜¯ä¸»è¦é—®é¢˜
- ä½†æ˜¾å¼è½¬æ¢å¯èƒ½å¼•å…¥é¢å¤–çš„ç²¾åº¦æŸå¤±

---

### å·®å¼‚ 6ï¼šRotary Position Embeddings è®¡ç®— (âš ï¸ ä½é£é™©)

```mermaid
flowchart TB
    subgraph GPU["GPU"]
        G1[æ¯æ¬¡ forward] --> G2[è®¡ç®— rotary embeddings]
        G2 --> G3[åº”ç”¨åˆ° Q/K]
    end
    
    subgraph TPU["TPU"]
        T1[é¢„è®¡ç®—ä¸€æ¬¡] --> T2[ç¼“å­˜åˆ° _cached_freqs_cos/sin]
        T2 --> T3[æ¯æ¬¡ forward ä½¿ç”¨ç¼“å­˜]
    end
    
    style G2 fill:#99ff99
    style T2 fill:#ffff99
```

#### TPU ç‰ˆæœ¬ ([`stage2_transformer_flax.py`](stage2_transformer_flax.py:716-743))

```python
# é¢„è®¡ç®— rotary embeddingsï¼ˆåœ¨ CPU ä¸Šè®¡ç®—ï¼Œé¿å… torchax é—®é¢˜ï¼‰
with torch.no_grad():
    freqs_cos, freqs_sin = transformer.get_rotary_pos_embed((latent_target_length_temp, latent_height_temp, latent_width_temp))
    # è½¬æ¢åˆ° XLA è®¾å¤‡å¹¶ç¼“å­˜
    with env:
        transformer._cached_freqs_cos = freqs_cos.to('jax')
        transformer._cached_freqs_sin = freqs_sin.to('jax')

# Monkey-patch get_rotary_pos_embed ä½¿ç”¨ç¼“å­˜
def cached_get_rotary_pos_embed(self, latent_size):
    if hasattr(self, '_cached_freqs_cos') and hasattr(self, '_cached_freqs_sin'):
        return self._cached_freqs_cos, self._cached_freqs_sin
    return original_get_rotary_pos_embed(latent_size)
```

**é—®é¢˜åˆ†æï¼š**
- é¢„è®¡ç®—ä¸å®æ—¶è®¡ç®—åº”è¯¥äº§ç”Ÿç›¸åŒçš„ç»“æœ
- ä¸»è¦é£é™©æ˜¯ç¼“å­˜çš„å°ºå¯¸ä¸å®é™…ä½¿ç”¨çš„å°ºå¯¸ä¸åŒ¹é…
- ä»£ç ä¸­ä½¿ç”¨ç›¸åŒçš„å‚æ•°è®¡ç®—ï¼Œåº”è¯¥æ²¡é—®é¢˜

---

## ğŸ” é—®é¢˜æ’æŸ¥ä¼˜å…ˆçº§

æ ¹æ®ä¸Šè¿°åˆ†æï¼Œå»ºè®®æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§æ’æŸ¥ï¼š

| ä¼˜å…ˆçº§ | å·®å¼‚ç‚¹ | é£é™©ç­‰çº§ | ä¿®å¤å»ºè®® |
|--------|--------|----------|----------|
| **1** | Attention Mask è¢«å¿½ç•¥ | ğŸ”´ é«˜ | å°è¯•å®ç°æ”¯æŒ mask çš„ Splash Attention |
| **2** | ByT5 ä½¿ç”¨ bf16 | ğŸŸ¡ ä¸­ | æ”¹ä¸º float32 |
| **3** | vision_states = None | ğŸŸ¡ ä¸­ | æ”¹ä¸ºé›¶å‘é‡ |
| **4** | reorder_txt_token ç®€åŒ– | ğŸŸ¡ ä¸­ | å°è¯•å®ç°æ­£ç¡®çš„ token é‡æ’ |
| **5** | Scheduler dtype | ğŸŸ¢ ä½ | æ£€æŸ¥æ˜¯å¦éœ€è¦ float32 ç´¯åŠ  |
| **6** | Rotary Embeddings | ğŸŸ¢ ä½ | éªŒè¯ç¼“å­˜æ­£ç¡®æ€§ |

---

## ğŸ› ï¸ å»ºè®®ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä¿®å¤ Attention Maskï¼ˆæœ€é‡è¦ï¼‰

```python
def _parallel_attention_tpu_with_mask(q, k, v, img_q_len, img_kv_len,
                                       attn_mode=None, text_mask=None,
                                       attn_param=None, block_idx=None):
    """
    æ”¯æŒ mask çš„ TPU ç‰ˆæœ¬
    """
    query, encoder_query = q
    key, encoder_key = k
    value, encoder_value = v
    
    # åˆå¹¶ image å’Œ text tokens
    query = torch.cat([query, encoder_query], dim=1)
    key = torch.cat([key, encoder_key], dim=1)
    value = torch.cat([value, encoder_value], dim=1)
    
    # æ„é€  attention mask
    if text_mask is not None:
        seq_len = query.shape[1]
        text_len = text_mask.shape[1]
        img_len = seq_len - text_len
        
        # åˆ›å»º full attention çŸ©é˜µ
        # Image tokens å¯ä»¥çœ‹åˆ°æ‰€æœ‰ tokens
        # Text tokens åªèƒ½çœ‹åˆ°é padding ä½ç½®
        attn_mask = torch.ones(seq_len, seq_len, device=query.device, dtype=torch.bool)
        
        # å±è”½ text padding
        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, seq_len)
        attn_mask[img_len:, :] = text_mask_expanded[:, :, 0]
    else:
        attn_mask = None
    
    query = query.transpose(1, 2)
    key = key.transpose(1, 2)
    value = value.transpose(1, 2)
    
    # ä½¿ç”¨å‚è€ƒå®ç°è€Œé Splash Attentionï¼ˆæ”¯æŒ maskï¼‰
    hidden_states = _sdpa_reference(query, key, value, attn_mask=attn_mask)
    
    hidden_states = hidden_states.transpose(1, 2)
    b, s, a, d = hidden_states.shape
    hidden_states = hidden_states.reshape(b, s, -1)
    
    return hidden_states
```

### æ–¹æ¡ˆ 2ï¼šä¿®å¤ ByT5 dtype

```python
# æ”¹ä¸ºä½¿ç”¨ float32
if prompt_embeds_2 is not None:
    prompt_embeds_2 = prompt_embeds_2.to(dtype=torch.float32).to('jax')  # æ”¹ä¸º float32
    ...
    extra_kwargs = {
        "byt5_text_states": byt5_text_states.to(torch.float32),
        "byt5_text_mask": byt5_text_mask,
    }
```

### æ–¹æ¡ˆ 3ï¼šä¿®å¤ vision_states

```python
# ç»Ÿä¸€ä½¿ç”¨é›¶å‘é‡
vision_num_tokens = 729
vision_dim = 1152

vision_states = torch.zeros(
    latents.shape[0],
    vision_num_tokens,
    vision_dim,
    device='jax',
    dtype=target_dtype,
)

if do_classifier_free_guidance:
    vision_states = vision_states.repeat(2, 1, 1)
```

---

## ğŸ“Š éªŒè¯æ–¹æ³•

### 1. ä¸­é—´å€¼å¯¹æ¯”

åœ¨ GPU å’Œ TPU ç‰ˆæœ¬ä¸­æ·»åŠ ä»¥ä¸‹æ£€æŸ¥ç‚¹ï¼š

```python
# æ£€æŸ¥ç‚¹ 1ï¼šEmbeddings
print(f"prompt_embeds mean: {prompt_embeds.mean().item()}")
print(f"prompt_embeds std: {prompt_embeds.std().item()}")

# æ£€æŸ¥ç‚¹ 2ï¼šç¬¬ä¸€æ­¥ noise_pred
print(f"noise_pred[0] mean: {noise_pred.mean().item()}")
print(f"noise_pred[0] std: {noise_pred.std().item()}")

# æ£€æŸ¥ç‚¹ 3ï¼šAttention è¾“å‡º
# åœ¨ attention å‡½æ•°ä¸­æ·»åŠ 
print(f"attention output mean: {hidden_states.mean().item()}")
```

### 2. å•æ­¥å¯¹æ¯”

```python
# åªè¿è¡Œ 1 æ­¥ï¼Œå¯¹æ¯”ç»“æœ
args.num_inference_steps = 1

# ä¿å­˜ä¸­é—´ç»“æœ
torch.save({
    'noise_pred': noise_pred.cpu(),
    'latents_after_step': latents.cpu(),
}, 'debug_step1.pt')
```

### 3. é€æ¨¡å—å¯¹æ¯”

```python
# åœ¨ transformer forward ä¸­æ·»åŠ  hooks
def hook_fn(name):
    def fn(module, input, output):
        print(f"{name}: output mean={output.mean().item():.6f}, std={output.std().item():.6f}")
    return fn

for name, module in transformer.named_modules():
    if isinstance(module, torch.nn.Linear):
        module.register_forward_hook(hook_fn(name))
```

---

## ğŸ¯ æ€»ç»“

TPU ç‰ˆæœ¬ç”Ÿæˆé”™è¯¯è§†é¢‘æœ€å¯èƒ½çš„åŸå› æ˜¯ï¼š

1. **Attention Mask è¢«å®Œå…¨å¿½ç•¥** - å¯¼è‡´ padding token å‚ä¸æ³¨æ„åŠ›è®¡ç®—ï¼Œç ´åäº†ç”Ÿæˆè´¨é‡
2. **ByT5 Embeddings ç²¾åº¦é™ä½** - ä» float32 é™åˆ° bf16 å¯èƒ½å½±å“æ–‡æœ¬æ¡ä»¶
3. **Token é‡æ’åºé€»è¾‘è¢«ç®€åŒ–** - å¯èƒ½å¯¼è‡´ token é¡ºåºé”™è¯¯

å»ºè®®ä¼˜å…ˆä¿®å¤ Attention Mask é—®é¢˜ï¼Œè¿™æ˜¯å½±å“æœ€å¤§çš„å·®å¼‚ã€‚