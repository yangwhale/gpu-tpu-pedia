# CogVideoX TPU åŠ é€Ÿé¡¹ç›®

> **æœ€åæ›´æ–°**ï¼š2025-12-25
>
> ğŸ‰ **æœ€æ–°è¿›å±•**ï¼šgenerate_torchax.py é›†æˆ TorchAx VAEï¼720P 81å¸§ 50æ­¥ **106.47s (2.13s/step)**ï¼Œå†æå‡ **10%**ï¼

æœ¬é¡¹ç›®å®ç°äº† CogVideoX è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ Google Cloud TPU ä¸Šçš„é«˜æ€§èƒ½æ¨ç†ï¼Œé€šè¿‡ JAX + TorchAx/Flax å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œå†…å­˜ä¼˜åŒ–ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

CogVideoX æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæœ¬é¡¹ç›®å°†å…¶è¿ç§»åˆ° TPU å¹³å°ï¼Œåˆ©ç”¨ä»¥ä¸‹æŠ€æœ¯å®ç°é«˜æ•ˆæ¨ç†ï¼š

- **ä¸‰é˜¶æ®µæµæ°´çº¿**ï¼šText Encoder â†’ Transformer â†’ VAE Decoderï¼Œæ”¯æŒåˆ†é˜¶æ®µè°ƒè¯•å’Œä¼˜åŒ–
- **Splash Attention**ï¼šTPU åŸç”Ÿçš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒé•¿åºåˆ—å¤„ç†
- **åŒ VAE å®ç°**ï¼šTorchAx VAEï¼ˆğŸ† æ€§èƒ½æœ€ä¼˜ï¼Œæ¨èï¼‰å’Œ Flax VAEï¼ˆçº¯ JAX å®ç°ï¼‰
- **æ™ºèƒ½åˆ†ç‰‡**ï¼šTensor Parallel + Data Parallelï¼Œæ”¯æŒå¤š TPU å¹¶è¡Œ
- **BFloat16 ä¼˜åŒ–**ï¼šå…¨æµç¨‹ BF16 è®¡ç®—ï¼Œå‡å°‘å†…å­˜å ç”¨å¹¶æå‡æ€§èƒ½

```mermaid
flowchart LR
    subgraph "Stage 1"
        A[Text Prompt] --> B[T5 Encoder]
        B --> C[Embeddings]
    end
    
    subgraph "Stage 2"
        C --> D[Transformer<br/>+ Splash Attention]
        D --> E[Latents]
    end
    
    subgraph "Stage 3"
        E --> F{VAE Decoder}
        F -->|TorchAx ğŸ†| G1[0.86s]
        F -->|Flax| G2[1.30s]
    end
    
    G1 --> H[Video]
    G2 --> H
```

---

## ğŸš€ Quick Start

### 1. ç¯å¢ƒå®‰è£…

```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install huggingface-hub
pip install -U transformers datasets evaluate accelerate timm flax numpy
pip install torchax
pip install jax[tpu]
pip install tensorflow-cpu

# å®‰è£…è¾…åŠ©å·¥å…·
pip install sentencepiece
sudo apt install ffmpeg -y
pip install imageio[ffmpeg]
pip install tpu-info
pip install matplotlib
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

```bash
# è®¾ç½® Hugging Face ç¼“å­˜ç›®å½•ï¼ˆä½¿ç”¨å…±äº«å†…å­˜åŠ é€Ÿï¼‰
export HF_HOME=/dev/shm

# è®¾ç½® Hugging Face Token
export HF_TOKEN=<your HF_TOKEN>

# JAX ç¼–è¯‘ç¼“å­˜ï¼ˆåŠ é€Ÿé‡å¤è¿è¡Œï¼‰
export JAX_COMPILATION_CACHE_DIR=/dev/shm/jax_cache
```

### 3. å…‹éš†å¹¶å®‰è£…é¡¹ç›®

```bash
# å…‹éš† diffusers-tpu é¡¹ç›®ï¼ˆåŒ…å« TorchAx/Flax VAE å®ç°ï¼‰
git clone https://github.com/yangwhale/diffusers-tpu.git
cd diffusers-tpu
pip install -e .

# å…‹éš†æœ¬é¡¹ç›®
git clone https://github.com/yangwhale/gpu-tpu-pedia.git
cd gpu-tpu-pedia/tpu/CogVideoX/
```

### 4. è¿è¡Œè§†é¢‘ç”Ÿæˆ

**æ¨èï¼šä½¿ç”¨ä¸‰é˜¶æ®µæµæ°´çº¿**

```bash
cd generate_diffusers_torchax_staged

# é˜¶æ®µ1ï¼šæ–‡æœ¬ç¼–ç 
python stage1_text_encoder.py --prompt "A panda playing guitar"

# é˜¶æ®µ2ï¼šTransformer æ¨ç†
python stage2_transformer.py --num_inference_steps 50

# é˜¶æ®µ3ï¼šVAE è§£ç ï¼ˆğŸ† æ¨è TorchAx ç‰ˆæœ¬ï¼‰
python stage3_vae_decoder.py
```

**æˆ–è€…ï¼šä¸€ä½“åŒ–è„šæœ¬**

```bash
python generate_torchax.py
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
CogVideoX/
â”œâ”€â”€ README.md                           # æœ¬æ–‡æ¡£
â”œâ”€â”€ generate_torchax.py                 # ä¸€ä½“åŒ–ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ custom_splash_attention.py          # TPU Splash Attention å®ç°
â”‚
â””â”€â”€ generate_diffusers_torchax_staged/  # â­ ä¸‰é˜¶æ®µæµæ°´çº¿ï¼ˆæ¨èï¼‰
    â”œâ”€â”€ README.md                       # è¯¦ç»†ä½¿ç”¨è¯´æ˜
    â”œâ”€â”€ utils.py                        # å…±äº«å·¥å…·æ¨¡å—
    â”œâ”€â”€ stage1_text_encoder.py          # é˜¶æ®µ1ï¼šæ–‡æœ¬ç¼–ç 
    â”œâ”€â”€ stage2_transformer.py           # é˜¶æ®µ2ï¼šTransformer æ¨ç†
    â”œâ”€â”€ stage3_vae_decoder.py           # é˜¶æ®µ3ï¼šVAE (TorchAx) ğŸ†
    â”œâ”€â”€ stage3_vae_decoder_flax.py      # é˜¶æ®µ3ï¼šVAE (Flax)
    â”œâ”€â”€ TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md  # VAE æ·±åº¦å¯¹æ¯”æ–‡æ¡£
    â””â”€â”€ stage_outputs/                  # ä¸­é—´æ–‡ä»¶å­˜å‚¨
        â”œâ”€â”€ generation_config.json
        â”œâ”€â”€ stage1_embeddings.safetensors
        â”œâ”€â”€ stage2_latents.safetensors
        â””â”€â”€ output_video.mp4
```

---

## ğŸ¯ æ€§èƒ½åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ

- **ç¡¬ä»¶**ï¼šTPU v6e-8 (8 chips)
- **æ¨¡å‹**ï¼šCogVideoX1.5-5B
- **æ¨ç†æ­¥æ•°**ï¼š50 æ­¥
- **Mesh é…ç½®**ï¼š`dp=2, sp=1, tp=4`ï¼ˆæ¨èï¼‰

### ä¸€ä½“åŒ–è„šæœ¬æ€§èƒ½ï¼ˆgenerate_torchax.pyï¼‰

| åˆ†è¾¨ç‡ | å¸§æ•° | æ¨ç†æ­¥æ•° | åŸºå‡†æ—¶é—´ | å•æ­¥æ—¶é—´ | å¤‡æ³¨ |
|--------|------|----------|----------|----------|------|
| **720Ã—1280** ğŸ† | 81å¸§ 16fps | 50 æ­¥ | **106.47s** | **2.13s/step** | åŒ…å« Text Encoder + Transformer + TorchAx VAE |

> ğŸ“Š **æ€§èƒ½æ¼”è¿›**ï¼š
> - 2025-12-25 ä¹‹å‰ï¼š118.4s (2.37s/step) - ä½¿ç”¨æ ‡å‡† diffusers VAE
> - 2025-12-25 ä¹‹åï¼š**106.47s (2.13s/step)** - é›†æˆ TorchAx VAEï¼Œ**æå‡ 10%**

### æ€§èƒ½å¯¹æ¯”ï¼š720P vs 768Pï¼ˆä¸‰é˜¶æ®µæµæ°´çº¿ï¼‰

| åˆ†è¾¨ç‡ | å¸§æ•° | Stage 2 (åç»­) | VAE TorchAx (åç»­) | æ€»è®¡ (åç»­) |
|--------|------|----------------|-------------------|-------------|
| **720Ã—1280** ğŸ† | 81å¸§ 16fps | **103.8s** | **0.86s** | **~107s** |
| 768Ã—1360 | 81å¸§ 8fps | 56.6s | 0.65s | ~60s |

> ğŸ’¡ 720P æ›´æ…¢çš„åŸå› ï¼š720Ã—1280 = 921,600 åƒç´  vs 768Ã—1360 = 1,044,480 åƒç´ 
> è™½ç„¶ 720P åƒç´ æ›´å°‘ï¼Œä½†æµ‹è¯•æ—¶é—´å·®å¼‚å¯èƒ½æ¥è‡ªä¸åŒçš„è¿è¡Œç¯å¢ƒæ¡ä»¶

### 720Ã—1280Ã—81å¸§ 16fpsï¼ˆæ¨èé…ç½®ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    720P 81å¸§ æ€§èƒ½æ€»è§ˆ                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       é˜¶æ®µ        â”‚  é¦–æ¬¡è¿è¡Œ  â”‚  åç»­è¿è¡Œ   â”‚  JIT ç¼–è¯‘  â”‚    å¤‡æ³¨      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stage 1: T5       â”‚    ~2s     â”‚    ~2s     â”‚     -      â”‚ CPU è¿è¡Œ     â”‚
â”‚ Stage 2: Trans    â”‚   ~234s    â”‚  ~103.8s   â”‚   ~130s    â”‚ 50æ­¥ DP=2    â”‚
â”‚ Stage 3: TorchAx  â”‚    ~81s    â”‚   0.86s ğŸ† â”‚    ~80s    â”‚ VAE è§£ç      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ€»è®¡ (TorchAx)    â”‚   ~317s    â”‚  ~107s     â”‚     -      â”‚ æ¨èï¼       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 768Ã—1360Ã—81å¸§ 8fpsï¼ˆå†å²é…ç½®ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    768P 81å¸§ æ€§èƒ½æ€»è§ˆ                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       é˜¶æ®µ        â”‚  é¦–æ¬¡è¿è¡Œ  â”‚  åç»­è¿è¡Œ   â”‚  JIT ç¼–è¯‘  â”‚    å¤‡æ³¨      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stage 1: T5       â”‚    ~2s     â”‚    ~2s     â”‚     -      â”‚ CPU è¿è¡Œ     â”‚
â”‚ Stage 2: Trans    â”‚   ~194s    â”‚   ~57s     â”‚   ~137s    â”‚ 50æ­¥ DP=2 ğŸ† â”‚
â”‚ Stage 3: TorchAx  â”‚   ~129s    â”‚   0.65s    â”‚   ~126s    â”‚ VAE è§£ç      â”‚
â”‚ Stage 3: Flax     â”‚   ~245s    â”‚   1.30s    â”‚   ~244s    â”‚ VAE è§£ç      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ€»è®¡ (TorchAx)    â”‚   ~325s    â”‚   ~60s ğŸ†  â”‚     -      â”‚ æœ€å¿«ï¼       â”‚
â”‚ æ€»è®¡ (Flax)       â”‚   ~441s    â”‚   ~60s     â”‚     -      â”‚ å¤‡é€‰         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage 2 Transformer é…ç½®å¯¹æ¯”ï¼ˆ50 æ­¥æ¨ç†ï¼Œ768Pï¼‰

| é…ç½® | Mesh | æ¯æ­¥æ—¶é—´ | 50æ­¥æ€»æ—¶é—´ | åŠ é€Ÿæ¯” |
|------|------|----------|-----------|--------|
| DP=1 | `dp=1, tp=8` | 1.79s | 89.67s | 1.0x |
| **DP=2** ğŸ† | `dp=2, tp=4` | **1.13s** | **56.63s** | **1.58x** |

> ğŸ’¡ **DP=2 æ›´å¿«çš„åŸå› **ï¼šCFG æ¨¡å¼ä¸‹æ­£å‘+è´Ÿå‘ prompt å¯å¹¶è¡Œå¤„ç†

### VAE è§£ç å™¨å¯¹æ¯”ï¼ˆ768P 81å¸§ï¼‰

| å®ç° | è§£ç æ—¶é—´ | ç‰¹ç‚¹ |
|------|----------|------|
| **TorchAx VAE** ğŸ† | **0.65s** | PyTorch å…¼å®¹ï¼Œç§»æ¤å®¹æ˜“ï¼Œç¼“å­˜åæœ€å¿« |
| Flax VAE | 1.30s | çº¯ JAXï¼Œé¦–æ¬¡ç¼–è¯‘åç¨³å®š |
| Flax VAE (æœªä¼˜åŒ–) | ~12s | æ—  JIT/åˆ†ç‰‡ |

> ğŸ“– è¯¦ç»†å¯¹æ¯”åˆ†æè¯·å‚é˜…ï¼š[`TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md`](generate_diffusers_torchax_staged/TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md)

### Stage 2 å†å²ä¼˜åŒ–è®°å½•ï¼ˆ10 æ­¥åŸºå‡†æµ‹è¯•ï¼‰

| # | ä¼˜åŒ–é¡¹ | DP | Block Size | æ¯æ­¥æ—¶é—´ | 10æ­¥æ€»æ—¶é—´ | ç›¸å¯¹åŸºçº¿ |
|---|--------|-----|------------|----------|-----------|---------|
| 1 | åŸºçº¿ç‰ˆæœ¬ | âœ— | åŸå§‹ | 4.04s | 40.54s | - |
| 2 | +sharding constraint | âœ— | åŸå§‹ | 3.93s | 39.43s | +2.7% |
| 3 | +DP | âœ“ | åŸå§‹ | 3.08s | 33.90s | +16.4% |
| 4 | +mesh é¡ºåºä¼˜åŒ– | âœ“ | åŸå§‹ | 2.75s | 30.26s | +25.4% |
| 5 | **æœ€ä¼˜é…ç½®** | âœ“ | Wan2.1 | **2.31s** | **25.36s** | **+37.4%** |

---

## ğŸ”§ ä¸‰é˜¶æ®µæµæ°´çº¿è¯¦è§£

### é˜¶æ®µ1ï¼šæ–‡æœ¬ç¼–ç 

```bash
python stage1_text_encoder.py \
  --prompt "A panda playing guitar in a bamboo forest" \
  --output_dir ./stage_outputs
```

**å‚æ•°**ï¼š
- `--prompt`: æ­£é¢æç¤ºè¯
- `--negative_prompt`: è´Ÿé¢æç¤ºè¯ï¼ˆå¯é€‰ï¼‰
- `--model_id`: æ¨¡å‹ IDï¼ˆé»˜è®¤ `zai-org/CogVideoX1.5-5B`ï¼‰

**è¾“å‡º**ï¼š
- `stage1_embeddings.safetensors`: prompt embeddings
- `generation_config.json`: ç”Ÿæˆé…ç½®

### é˜¶æ®µ2ï¼šTransformer æ¨ç†

```bash
python stage2_transformer.py \
  --input_dir ./stage_outputs \
  --num_inference_steps 50 \
  --height 720 \
  --width 1280 \
  --frames 81
```

**å‚æ•°**ï¼š
- `--num_inference_steps`: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 50ï¼‰
- `--height/width`: è§†é¢‘å°ºå¯¸ï¼ˆé»˜è®¤ 720Ã—1280ï¼‰
- `--frames`: è§†é¢‘å¸§æ•°ï¼ˆé»˜è®¤ 81ï¼‰
- `--guidance_scale`: CFG å¼•å¯¼å°ºåº¦ï¼ˆé»˜è®¤ 6.0ï¼‰

**è¾“å‡º**ï¼š
- `stage2_latents.safetensors`: ç”Ÿæˆçš„ latents

> âš ï¸ **å¸§æ•°è¦æ±‚**ï¼šCogVideoX çš„ VAE è¦æ±‚ latent å¸§æ•°ä¸ºå¥‡æ•°ï¼Œå¦åˆ™è§£ç æ—¶ä¼šå¤šå‡ºå¸§ã€‚
> æœ‰æ•ˆå¸§æ•° = 4n+1 ä¸” (å¸§æ•°-1)/4+1 ä¸ºå¥‡æ•°ï¼Œå¦‚ï¼š41, 49, 57, 65, 73, **81**, 89, 97...

### é˜¶æ®µ3ï¼šVAE è§£ç 

**ğŸ† æ¨èï¼šTorchAx VAEï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰**

```bash
python stage3_vae_decoder.py \
  --input_dir ./stage_outputs \
  --output_video ./stage_outputs/output_video.mp4
```

**å¤‡é€‰ï¼šFlax VAEï¼ˆçº¯ JAX å®ç°ï¼‰**

```bash
python stage3_vae_decoder_flax.py \
  --input_dir ./stage_outputs \
  --output_video ./stage_outputs/output_video.mp4 \
  --dp 1
```

**è¾“å‡º**ï¼š
- `output_video.mp4`: æœ€ç»ˆè§†é¢‘

---

## âš™ï¸ æ ¸å¿ƒæŠ€æœ¯

### 1. Splash Attention

TPU ä¸“ç”¨çš„é«˜æ•ˆæ³¨æ„åŠ›å®ç°ï¼Œæ”¯æŒé•¿åºåˆ—å¤„ç†ï¼š

```python
# é…ç½®å‚æ•°ï¼ˆWan2.1 ä¼˜åŒ–é…ç½®ï¼‰
BQSIZE = 3328           # Query å—å¤§å°
BKVSIZE = 2816          # Key/Value å—å¤§å°
BKVCOMPUTESIZE = 256    # KV è®¡ç®—å—å¤§å°
USE_K_SMOOTH = True     # K-smooth ä¼˜åŒ–
```

**ç‰¹æ€§**ï¼š
- å—çŠ¶è®¡ç®—ï¼Œé¿å… VMEM æº¢å‡º
- æ”¯æŒå±€éƒ¨çª—å£æ³¨æ„åŠ›
- K-smooth æŠ€æœ¯æå‡æ•°å€¼ç¨³å®šæ€§

### 2. æ™ºèƒ½æƒé‡åˆ†ç‰‡

æ”¯æŒå¤šç§å¹¶è¡Œæ¨¡å¼ï¼š

```mermaid
graph TB
    subgraph "åˆ†ç‰‡ç­–ç•¥"
        A[8 TPU Chips] --> B{åˆ†ç‰‡æ¨¡å¼}
        B --> C["TP only<br/>(tp=8, dp=1)"]
        B --> D["TP + DP<br/>(tp=4, dp=2)"]
        B --> E["DP only<br/>(tp=1, dp=8)"]
    end
```

```python
# Mesh é…ç½®
mesh = Mesh(devices, ("dp", "sp", "tp"))

# æƒé‡åˆ†ç‰‡
P(None, None, None, ("dp", "tp"), None)  # Width ç»´åº¦åˆ†ç‰‡
```

### 3. VAE è§£ç ä¼˜åŒ–

#### TorchAx VAEï¼ˆğŸ† æ¨èï¼‰

é€šè¿‡ `torchax` ç›´æ¥è¿è¡Œ PyTorch ä»£ç åœ¨ TPU ä¸Šï¼š

```python
# ä½¿ç”¨ torchax ç¯å¢ƒ
import torchax
import torchax.interop

# JAX Mesh é…ç½®
mesh = jax.sharding.Mesh(...)

# PyTorch VAE åœ¨ JAX ç¯å¢ƒè¿è¡Œ
with mesh:
    video = vae.decode(latents)
```

**ä¼˜åŠ¿**ï¼š
- ä»£ç å…¼å®¹æ€§å¥½ï¼Œç›´æ¥å¤ç”¨ PyTorch å®ç°
- JIT ç¼–è¯‘åæ€§èƒ½æœ€ä¼˜ï¼ˆ0.86sï¼‰
- ç»´æŠ¤æˆæœ¬ä½

#### Flax VAE

çº¯ JAX/Flax å®ç°çš„ VAE è§£ç å™¨ï¼š

1. **JIT ç¼–è¯‘**ï¼š`nnx.jit(flax_vae.decoder)`
2. **TPU åˆ†ç‰‡**ï¼š`jax.lax.with_sharding_constraint()`
3. **Mesh ä¸Šä¸‹æ–‡**ï¼š`with mesh: decode(...)`

```python
# åˆ†ç‰‡çº¦æŸä»£ç 
def _apply_sharding_constraint(inputs, is_nthwc=True):
    """åœ¨ Width ç»´åº¦åˆ†ç‰‡åˆ°å¤š TPU"""
    if is_nthwc:
        # Flax: (B, T, H, W, C) - W at index 3
        spec = P(None, None, None, ("dp", "tp"), None)
    else:
        # TorchAx: (B, C, T, H, W) - W at index 4
        spec = P(None, None, None, None, ("dp", "tp"))
    return jax.lax.with_sharding_constraint(inputs, spec)
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### 1. VMEM æº¢å‡º

**ç—‡çŠ¶**ï¼š`RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem`

**è§£å†³**ï¼šå‡å° Splash Attention å—å¤§å°
```python
BQSIZE = 1024        # ä» 3328 å‡å°
BKVSIZE = 512        # ä» 2816 å‡å°
```

### 2. HBM OOM

**ç—‡çŠ¶**ï¼š`Attempting to allocate X.XXG. That was not possible.`

**è§£å†³**ï¼š
1. æ£€æŸ¥æ˜¯å¦æ·»åŠ äº† TPU åˆ†ç‰‡çº¦æŸ
2. å‡å°‘è§†é¢‘å¸§æ•°æˆ–åˆ†è¾¨ç‡
3. ä½¿ç”¨ Data Parallel æ¨¡å¼åˆ†æ•£å†…å­˜

### 3. JIT ç¼–è¯‘æ…¢

**è§£å†³**ï¼šå¯ç”¨æŒä¹…åŒ–ç¼–è¯‘ç¼“å­˜
```python
jax.config.update("jax_compilation_cache_dir", "/dev/shm/jax_cache")
jax.config.update("jax_persistent_cache_min_entry_size_bytes", -1)
jax.config.update("jax_persistent_cache_min_compile_time_secs", 0)
```

### 4. ç»´åº¦ä¸åŒ¹é…

**æ³¨æ„**ï¼šTorchAx å’Œ Flax ä½¿ç”¨ä¸åŒçš„ç»´åº¦é¡ºåºï¼š
- TorchAx: `NCTHW` (Width @ index 4)
- Flax: `NTHWC` (Width @ index 3)

åˆ†ç‰‡ PartitionSpec å¿…é¡»ç›¸åº”è°ƒæ•´ï¼

---

## ğŸ“Š æ•°æ®æ ¼å¼è¯´æ˜

### Latents ç»´åº¦æ ¼å¼

| é˜¶æ®µ | æ ¼å¼ | è¯´æ˜ |
|------|------|------|
| Pipeline è¾“å‡º | `[B, T, C, H, W]` | Diffusers åŸå§‹æ ¼å¼ |
| Stage2 ä¿å­˜ | `[B, C, T, H, W]` | PyTorch æ ‡å‡†æ ¼å¼ |
| TorchAx VAE | `[B, C, T, H, W]` | NCTHW |
| Flax VAE | `[B, T, H, W, C]` | NTHWC |

### è§†é¢‘è¾“å‡ºæ ¼å¼

- `List[np.ndarray]`ï¼šæ¯å¸§ä¸º `float32`ï¼ŒèŒƒå›´ `[0, 1]`
- ä½¿ç”¨ `diffusers.utils.export_to_video` ä¿å­˜

---

##  ç›¸å…³èµ„æº

- **æ·±åº¦å¯¹æ¯”æ–‡æ¡£**ï¼š[`TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md`](generate_diffusers_torchax_staged/TORCHAX_VS_FLAX_VAE_OPTIMIZATION.md)
- **ä¸‰é˜¶æ®µè¯¦ç»†è¯´æ˜**ï¼š[`generate_diffusers_torchax_staged/README.md`](generate_diffusers_torchax_staged/README.md)
- [CogVideoX å®˜æ–¹ä»“åº“](https://github.com/THUDM/CogVideo)
- [Diffusers TPU ç‰ˆæœ¬](https://github.com/yangwhale/diffusers-tpu)
- [JAX æ–‡æ¡£](https://jax.readthedocs.io/)
- [Flax æ–‡æ¡£](https://flax.readthedocs.io/)
- [TPU å¼€å‘æŒ‡å—](https://cloud.google.com/tpu/docs)

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªåŸå§‹é¡¹ç›®çš„è®¸å¯è¯ã€‚

## ğŸ™ è‡´è°¢

- CogVideoX / THUDM å›¢é˜Ÿ
- Hugging Face Diffusers å›¢é˜Ÿ
- Google JAX/Flax å›¢é˜Ÿ
- TPU ç¤¾åŒº

---

## ğŸ“ˆ æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | æ›´æ–°å†…å®¹ |
|------|----------|
| 2025-12-25 | ğŸš€ **generate_torchax.py é›†æˆ TorchAx VAE**ï¼š2.37s/step â†’ 2.13s/stepï¼ˆ-10%ï¼‰ï¼Œå®Œæ•´æµç¨‹ 106.47s |
| 2025-12-25 | ğŸ”§ generate_torchax.py æ€§èƒ½ä¼˜åŒ–ï¼š2.92s/step â†’ 2.37s/stepï¼ˆ-19%ï¼‰ï¼Œä¼˜åŒ– attention sharding ç­–ç•¥ |
| 2025-12-24 | ğŸ† TorchAx VAE æ€§èƒ½è¶…è¶Š Flaxï¼ˆ0.86s vs 1.30sï¼‰ï¼›æ›´æ–°é»˜è®¤é…ç½®ï¼š720Ã—1280Ã—81å¸§ 16fps |
| 2025-12-20 | Flax VAE ä¼˜åŒ–å®Œæˆï¼Œè§£ç æ—¶é—´ 12s â†’ 1.30s |
| 2025-12-18 | æ–°å¢ TorchAx VAE å®ç°ï¼Œè§£ç æ—¶é—´ 90s â†’ 2.4s |
| 2025-12-12 | Stage 2 ä¼˜åŒ–å®Œæˆï¼Œæ¨ç†æ—¶é—´é™ä½ 37.4% |
| 2025-11-04 | åˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒåŸºç¡€ TPU æ¨ç† |
